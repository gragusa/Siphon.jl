var documenterSearchIndex = {"docs":
[{"location":"tutorials/visualization.html#Visualization","page":"Visualization","title":"Visualization","text":"Filthy.jl provides plotting recipes via RecipesBase.jl for visualizing Kalman filter and smoother outputs with confidence bands. These recipes work with any Plots.jl backend.","category":"section"},{"location":"tutorials/visualization.html#Quick-Start-(Recommended:-KalmanWorkspace)","page":"Visualization","title":"Quick Start (Recommended: KalmanWorkspace)","text":"The recommended approach uses KalmanWorkspace for zero-allocation, in-place computation:\n\nusing Plots\nusing Filthy\n\n# Create workspace and run filter + smoother\nws = KalmanWorkspace(Z, H, T, R, Q, a1, P1, n)\nkalman_filter!(ws, y)\nkalman_smoother!(ws)\n\n# Plot smoothed states with 95% confidence bands\nplot(ws)","category":"section"},{"location":"tutorials/visualization.html#Plotting-with-KalmanWorkspace","page":"Visualization","title":"Plotting with KalmanWorkspace","text":"The KalmanWorkspace recipe is the primary plotting interface. It supports filtered, predicted, and smoothed states:\n\n# Create and run\nws = KalmanWorkspace(Z, H, T, R, Q, a1, P1, n)\nkalman_filter!(ws, y)\nkalman_smoother!(ws)\n\n# Plot smoothed states (default)\nplot(ws)\nplot(ws, what=:smoothed)\n\n# Plot filtered or predicted states\nplot(ws, what=:filtered)\nplot(ws, what=:predicted)\n\n# Customize\nplot(ws, vars=1, level=0.90)     # Single state with 90% CI\nplot(ws, vars=[1,3])             # Multiple states\nplot(ws, band=false)             # No confidence bands\nplot(ws, time=dates)             # Custom time axis","category":"section"},{"location":"tutorials/visualization.html#Keywords","page":"Visualization","title":"Keywords","text":"Keyword Default Description\nvars :all Variables to plot: Int, Vector, Range, or :all\nlevel 0.95 Confidence level for bands (0 < level < 1)\nwhat :smoothed State type: :smoothed, :filtered, or :predicted\nband true Whether to show confidence bands\ntime nothing Custom time axis (default: 1:n)","category":"section"},{"location":"tutorials/visualization.html#Functional-API-(Alternative)","page":"Visualization","title":"Functional API (Alternative)","text":"For simpler cases or when AD compatibility is needed, you can use the functional API:\n\n# Define model and run filter\np = KFParms(Z, H, T, R, Q)\nresult = kalman_filter_full(p, y, a1, P1)\n\n# Plot filtered states with 95% confidence bands\nplot(result)\nplot(result, vars=1)             # State 1 only\nplot(result, vars=[1,3])         # States 1 and 3\nplot(result, level=0.90)         # 90% CI\nplot(result, band=false)         # No confidence bands\nplot(result, filtered=false)     # Predicted states instead of filtered","category":"section"},{"location":"tutorials/visualization.html#Plotting-Smoothed-States","page":"Visualization","title":"Plotting Smoothed States","text":"The SmootherResult wrapper enables plotting smoothed states from any source:\n\n# Method 1: From KalmanWorkspace (recommended)\nws = KalmanWorkspace(Z, H, T, R, Q, a1, P1, n)\nkalman_filter!(ws, y)\nkalman_smoother!(ws)\nsmooth = SmootherResult(ws)\nplot(smooth)\n\n# Method 2: Convenience constructor (runs filter + smoother internally)\np = KFParms(Z, H, T, R, Q)\nsmooth = SmootherResult(p, y, a1, P1)\nplot(smooth, vars=1, level=0.90)\n\n# Method 3: Wrap existing smoother NamedTuple output\nfilt = kalman_filter_full(p, y, a1, P1)\nnt = kalman_smoother(p.Z, p.T, filt.at, filt.Pt, filt.vt, filt.Ft)\nsmooth = SmootherResult(nt)\nplot(smooth)","category":"section"},{"location":"tutorials/visualization.html#Keywords-2","page":"Visualization","title":"Keywords","text":"Keyword Default Description\nvars :all Variables to plot\nlevel 0.95 Confidence level for bands\nband true Whether to show confidence bands","category":"section"},{"location":"tutorials/visualization.html#Plotting-Forecasts","page":"Visualization","title":"Plotting Forecasts","text":"Wrap forecast output in ForecastResult:\n\n# Run forecast\nfc_nt = forecast(spec, θ, y, 24)  # 24-step ahead forecast\nfc = ForecastResult(fc_nt)\n\n# Plot forecasted observations (default)\nplot(fc)\nplot(fc, what=:observations)\n\n# Plot forecasted states\nplot(fc, what=:states)\nplot(fc, what=:states, vars=1)","category":"section"},{"location":"tutorials/visualization.html#Keywords-3","page":"Visualization","title":"Keywords","text":"Keyword Default Description\nvars :all Variables to plot\nlevel 0.95 Confidence level for bands\nwhat :observations Plot :observations or :states\nband true Whether to show confidence bands","category":"section"},{"location":"tutorials/visualization.html#Comparing-Filtered-vs-Smoothed","page":"Visualization","title":"Comparing Filtered vs Smoothed","text":"Plot both filtered and smoothed states together:\n\nresult = kalman_filter_full(p, y, a1, P1)\nsmooth = SmootherResult(p, y, a1, P1)\n\n# Compare on same plot\nplot((result, smooth), vars=1)\nplot((result, smooth), vars=[1,2], band=true)\n\nThe comparison shows filtered states as dashed blue lines and smoothed states as solid green lines, with optional confidence bands around the smoother.","category":"section"},{"location":"tutorials/visualization.html#Plotting-Observable-Predictions","page":"Visualization","title":"Plotting Observable Predictions","text":"Plot one-step-ahead predictions of observables (ŷₜ = Z * aₜ):\n\nresult = kalman_filter_full(p, y, a1, P1)\n\n# One-step-ahead predictions\nplot(result, ObservablePlot)\n\n# With actual observations overlay\nplot(result, ObservablePlot, actual=y)\n\n# Select specific observables\nplot(result, ObservablePlot, vars=1, level=0.90)","category":"section"},{"location":"tutorials/visualization.html#Complete-Example:-Nile-River-Flow","page":"Visualization","title":"Complete Example: Nile River Flow","text":"using Plots\nusing Filthy\nusing DelimitedFiles\n\n# Load Nile data\nnile = readdlm(\"Nile.csv\", ',', Float64)\ny = reshape(nile[:, 1], 1, :)\nn = size(y, 2)\n\n# Local level model parameters (MLE estimates)\nZ = [1.0;;]       # Observation matrix\nH = [15099.0;;]   # Observation variance\nT = [1.0;;]       # Transition matrix\nR = [1.0;;]       # Selection matrix\nQ = [1469.0;;]    # State variance\na1 = [0.0]        # Initial state mean\nP1 = [1e7;;]      # Initial state variance (diffuse)\n\n# Create workspace and run filter + smoother\nws = KalmanWorkspace(Z, H, T, R, Q, a1, P1, n)\nkalman_filter!(ws, y)\nkalman_smoother!(ws)\n\n# Plot smoothed states (recommended)\np1 = plot(ws, what=:smoothed, title=\"Smoothed Level\",\n          xlabel=\"Year\", ylabel=\"Flow\")\n\n# Plot filtered states\np2 = plot(ws, what=:filtered, title=\"Filtered Level\",\n          xlabel=\"Year\", ylabel=\"Flow\")\n\n# Plot predicted states\np3 = plot(ws, what=:predicted, title=\"Predicted Level\",\n          xlabel=\"Year\", ylabel=\"Flow\")\n\n# For comparison with functional API\nresult = kalman_filter_full(KFParms(Z, H, T, R, Q), y, a1, P1)\nsmooth = SmootherResult(ws)  # Create from workspace\n\n# Compare filtered vs smoothed\np4 = plot((result, smooth), title=\"Filtered vs Smoothed\",\n          xlabel=\"Year\", ylabel=\"Flow\")\n\n# Observable predictions with actual data\np5 = plot(result, ObservablePlot, actual=y,\n          title=\"One-Step-Ahead Predictions\",\n          xlabel=\"Year\", ylabel=\"Flow\")\n\n# Combine into single figure\nplot(p1, p2, p4, p5, layout=(2,2), size=(1000, 800))","category":"section"},{"location":"tutorials/visualization.html#Customizing-Plots","page":"Visualization","title":"Customizing Plots","text":"Since recipes use RecipesBase.jl, you can combine them with standard Plots.jl attributes:\n\nplot(result,\n    vars=1,\n    level=0.95,\n    title=\"Custom Title\",\n    xlabel=\"Time\",\n    ylabel=\"Value\",\n    linewidth=3,\n    legend=:bottomright,\n    size=(800, 400)\n)","category":"section"},{"location":"tutorials/visualization.html#Color-Scheme","page":"Visualization","title":"Color Scheme","text":"The default color scheme uses consistent colors across plot types:\n\nFiltered states: Steel blue (line) with light blue (bands)\nSmoothed states: Dark green (line) with light green (bands)\nForecasts: Dark orange (dashed line) with light orange (bands)\nPredicted states: Purple (line) with light purple (bands)","category":"section"},{"location":"tutorials/visualization.html#API-Reference","page":"Visualization","title":"API Reference","text":"","category":"section"},{"location":"tutorials/visualization.html#Core-Types","page":"Visualization","title":"Core Types","text":"KalmanWorkspace: In-place filter/smoother workspace (recommended)\nKalmanFilterResult: Functional API filter output\nSmootherResult: Wrapper for smoother output\nForecastResult: Wrapper for forecast output\nObservablePlot: Marker type for observable prediction plots","category":"section"},{"location":"tutorials/visualization.html#In-place-Functions","page":"Visualization","title":"In-place Functions","text":"kalman_filter!: Run filter in-place\nkalman_smoother!: Run smoother in-place\nfilter_and_smooth!: Run both in-place","category":"section"},{"location":"tutorials/visualization.html#Accessor-Functions","page":"Visualization","title":"Accessor Functions","text":"smoothed_states: Get smoothed state means\nfiltered_states: Get filtered state means\npredicted_states: Get predicted state means\nvariances_smoothed_states: Get smoothed state covariances\nloglikelihood: Get log-likelihood","category":"section"},{"location":"tutorials/visualization.html#Helper-Functions","page":"Visualization","title":"Helper Functions","text":"confidence_bands: Compute confidence interval bounds\nselect_vars: Parse variable selection keywords","category":"section"},{"location":"tutorials/estimation_methods.html#Estimation-Methods:-DSL-vs-Convenience-Functions","page":"Estimation Methods","title":"Estimation Methods: DSL vs Convenience Functions","text":"This tutorial demonstrates how to specify and estimate the same model using Filthy.jl's two main approaches:\n\nDSL approach: Use SSMSpec with optimize_ssm() or em_ssm() for maximum flexibility\nHigh-level API: Use specialized types like StateSpaceModel or DynamicFactorModel with fit!() for convenience\n\nWe'll use the Dynamic Nelson-Siegel (DNS) yield curve model as our running example.","category":"section"},{"location":"tutorials/estimation_methods.html#The-DNS-Model","page":"Estimation Methods","title":"The DNS Model","text":"The Dynamic Nelson-Siegel model decomposes the yield curve into three latent factors:\n\nbeginaligned\ny_t = Z(lambda) f_t + varepsilon_t quad varepsilon_t sim N(0 H) \nf_t+1 = T f_t + eta_t quad eta_t sim N(0 Q)\nendaligned\n\nwhere:\n\ny_t is the p times 1 vector of yields at different maturities\nf_t = L_t S_t C_t are Level, Slope, and Curvature factors\nZ(lambda) is the p times 3 loading matrix with decay parameter lambda\n\nThe factor loadings are:\n\nZ(lambda)_i = beginbmatrix 1  frac1-e^-lambdatau_ilambdatau_i  frac1-e^-lambdatau_ilambdatau_i - e^-lambdatau_i endbmatrix","category":"section"},{"location":"tutorials/estimation_methods.html#Simulating-Test-Data","page":"Estimation Methods","title":"Simulating Test Data","text":"First, let's simulate data from a known DNS model:\n\nusing Filthy\nusing LinearAlgebra\nusing Random\nusing Statistics\n\nRandom.seed!(42)\n\n# Maturities in months\nmaturities = [3, 6, 12, 24, 36, 60, 84, 120]\nn_maturities = length(maturities)\nn_obs = 200\n\n# True parameters\nλ_true = 0.0609\nT_true = Diagonal([0.99, 0.95, 0.90])\nQ_true = Diagonal([0.01, 0.02, 0.03])\nH_true = 0.0001 * I(n_maturities)\n\n# Build DNS loadings\nfunction dns_loadings(λ, maturities)\n    p = length(maturities)\n    Z = ones(p, 3)\n    for (i, τ) in enumerate(maturities)\n        x = λ * τ\n        Z[i, 2] = x < 1e-10 ? 1.0 - x/2 : (1 - exp(-x)) / x\n        Z[i, 3] = Z[i, 2] - exp(-x)\n    end\n    return Z\nend\n\nZ_true = dns_loadings(λ_true, maturities)\n\n# Simulate factors and yields\nL_Q = cholesky(Symmetric(Matrix(Q_true))).L\nL_H = cholesky(Symmetric(Matrix(H_true))).L\n\nfactors = zeros(3, n_obs)\nyields = zeros(n_maturities, n_obs)\n\nfor t in 1:n_obs\n    if t > 1\n        factors[:, t] = T_true * factors[:, t-1] + L_Q * randn(3)\n    end\n    yields[:, t] = Z_true * factors[:, t] + L_H * randn(n_maturities)\nend\n\nprintln(\"Simulated $n_obs observations at $(n_maturities) maturities\")","category":"section"},{"location":"tutorials/estimation_methods.html#Approach-1:-DSL-with-optimize_ssm-(MLE)","page":"Estimation Methods","title":"Approach 1: DSL with optimize_ssm (MLE)","text":"The DSL approach uses dns_model() to create a specification and optimize_ssm() for MLE:\n\n# Create DNS specification with diagonal dynamics\nspec = dns_model(maturities;\n    T_structure = :diagonal,  # Diagonal AR(1) for each factor\n    H_structure = :diagonal,  # Diagonal observation variances\n    Q_structure = :diagonal,  # Diagonal state variances\n    λ_init = 0.06,\n    T_init = 0.9,\n    Q_init = 0.01,\n    H_init = 0.001\n)\n\nprintln(\"Parameters: \", param_names(spec))\n# [:λ, :T_L, :T_S, :T_C, :Q_L, :Q_S, :Q_C, :H_1, ..., :H_8]\n\n# Estimate via MLE (gradient-based optimization)\nresult_mle = optimize_ssm(spec, yields; maxiters=500)\n\nprintln(\"Estimated λ: \", round(result_mle.θ.λ, digits=4), \" (true: $λ_true)\")\nprintln(\"Estimated T diagonal: \", [result_mle.θ.T_L, result_mle.θ.T_S, result_mle.θ.T_C])\nprintln(\"Log-likelihood: \", round(result_mle.loglik, digits=2))","category":"section"},{"location":"tutorials/estimation_methods.html#Extracting-Smoothed-Factors","page":"Estimation Methods","title":"Extracting Smoothed Factors","text":"# Build state-space with estimated parameters\nss = build_linear_state_space(spec, result_mle.θ, yields)\n\n# Run filter and smoother\nfilt = kalman_filter_full(ss.p, yields, ss.a1, ss.P1)\nsmooth = kalman_smoother(ss.p.Z, ss.p.T, filt.at, filt.Pt, filt.vt, filt.Ft)\n\n# Compare with true factors\nfor (i, name) in enumerate([\"Level\", \"Slope\", \"Curvature\"])\n    corr = cor(factors[i, :], smooth.alpha[i, :])\n    println(\"$name correlation: \", round(corr, digits=4))\nend","category":"section"},{"location":"tutorials/estimation_methods.html#Approach-2:-DSL-with-profile*em*ssm-(Profile-EM)","page":"Estimation Methods","title":"Approach 2: DSL with profileemssm (Profile EM)","text":"For DNS models, the decay parameter lambda appears nonlinearly in the loadings, making pure EM difficult. The profile EM approach:\n\nGrids over lambda values\nFor each lambda, runs EM to estimate T, Q, H\nReturns the lambda with highest log-likelihood\n\n# Create spec with full covariance structures (for EM)\nspec_full = dns_model(maturities;\n    T_structure = :full,      # Full 3×3 VAR matrix\n    H_structure = :diagonal,\n    Q_structure = :full,      # Full 3×3 covariance\n    λ_init = 0.06\n)\n\n# Profile EM estimation\nresult_em = profile_em_ssm(spec_full, yields;\n    λ_grid = 0.02:0.01:0.12,  # Grid of λ values to search\n    maxiter = 200,\n    verbose = true\n)\n\nprintln(\"\\nProfile EM Results:\")\nprintln(\"Optimal λ: \", round(result_em.λ_optimal, digits=4), \" (true: $λ_true)\")\nprintln(\"Log-likelihood: \", round(result_em.loglik, digits=2))\n\n# Access estimated matrices\nT_est = result_em.em_result.T\nQ_est = result_em.em_result.Q\nH_est = result_em.em_result.H\n\nprintln(\"\\nEstimated T (factor dynamics):\")\ndisplay(round.(T_est, digits=3))\n\nprintln(\"\\nEstimated Q diagonal: \", round.(diag(Q_est), digits=4))\nprintln(\"True Q diagonal: \", diag(Q_true))","category":"section"},{"location":"tutorials/estimation_methods.html#Approach-3:-StateSpaceModel-with-fit!","page":"Estimation Methods","title":"Approach 3: StateSpaceModel with fit!","text":"The StateSpaceModel type wraps an SSMSpec and provides a convenient fit! interface:\n\ninclude(\"src/inplace.jl\")  # Load in-place implementations\n\n# Create StateSpaceModel from spec\nspec = dns_model(maturities;\n    T_structure = :diagonal,\n    H_structure = :diagonal,\n    Q_structure = :diagonal,\n    λ_init = 0.06\n)\n\nmodel = StateSpaceModel(spec, n_obs)\n\n# Fit with MLE\nfit!(MLE(), model, yields)\n\nprintln(\"MLE Results:\")\nprintln(\"Converged: \", isconverged(model))\nprintln(\"Log-likelihood: \", round(loglikelihood(model), digits=2))\nprintln(\"Parameters: \", parameters(model))\n\n# Access filtered states directly\nf = filtered_states(model)\nprintln(\"Filtered state at t=100: \", f[:, 100])","category":"section"},{"location":"tutorials/estimation_methods.html#StateSpaceModel-with-EM","page":"Estimation Methods","title":"StateSpaceModel with EM","text":"For models where EM is applicable (currently local_level), you can use fit!(EM(), ...):\n\n# Local level model example (EM works well here)\nspec_ll = local_level(var_obs=:free, var_level=:free)\nmodel_ll = StateSpaceModel(spec_ll, n_obs)\n\n# Simulate local level data\ny_ll = cumsum(randn(n_obs)) + 0.5 * randn(n_obs)\ny_ll = reshape(y_ll, 1, n_obs)\n\n# Fit with EM\nfit!(EM(), model_ll, y_ll; maxiter=200, tol=1e-6, verbose=true)\n\nprintln(\"\\nLocal Level EM Results:\")\nprintln(\"Converged: \", isconverged(model_ll))\nprintln(\"Iterations: \", niterations(model_ll))\nprintln(\"Parameters: \", parameters(model_ll))","category":"section"},{"location":"tutorials/estimation_methods.html#Comparison:-When-to-Use-Each-Approach","page":"Estimation Methods","title":"Comparison: When to Use Each Approach","text":"Approach Best For Advantages Limitations\noptimize_ssm() General MLE AD-compatible, any model structure May converge slowly for many parameters\nprofile_em_ssm() DNS/Svensson models Robust for λ estimation, full covariances Requires grid search over λ\nem_ssm() Diagonal variances Fast closed-form updates Limited to specific model structures\nfit!(MLE(), ...) Convenience API Clean interface, auto memory management Same as optimize_ssm\nfit!(EM(), ...) Large-scale EM Zero-allocation, efficient Limited model support","category":"section"},{"location":"tutorials/estimation_methods.html#Complete-Example:-DNS-with-Full-Pipeline","page":"Estimation Methods","title":"Complete Example: DNS with Full Pipeline","text":"Here's a complete workflow combining specification, estimation, and analysis:\n\nusing Filthy\nusing LinearAlgebra\nusing Statistics\n\ninclude(\"src/inplace.jl\")\n\n# 1. SPECIFY MODEL\nmaturities = [3, 6, 12, 24, 60, 120]\nspec = dns_model(maturities;\n    T_structure = :full,\n    Q_structure = :full,\n    H_structure = :diagonal,\n    λ_init = 0.06\n)\n\nprintln(\"Model: \", spec.name)\nprintln(\"Parameters: \", length(spec.params))\nprintln(\"States: \", spec.n_states)\n\n# 2. LOAD/SIMULATE DATA\n# (Using simulated data from above)\n\n# 3. ESTIMATE\nresult = profile_em_ssm(spec, yields;\n    λ_grid = 0.03:0.005:0.10,\n    maxiter = 300,\n    verbose = false\n)\n\n# 4. EXTRACT RESULTS\nλ_opt = result.λ_optimal\nT_opt = result.em_result.T\nQ_opt = result.em_result.Q\nH_opt = result.em_result.H\n\nprintln(\"\\n=== Estimation Results ===\")\nprintln(\"λ: \", round(λ_opt, digits=4))\nprintln(\"T eigenvalues: \", round.(eigvals(T_opt), digits=3))\nprintln(\"Q diagonal: \", round.(diag(Q_opt), digits=5))\n\n# 5. SMOOTH FACTORS\nZ_opt = dns_loadings(λ_opt, maturities)\np_final = KFParms(Z_opt, H_opt, T_opt, Matrix{Float64}(I, 3, 3), Q_opt)\na1 = zeros(3)\nP1 = 1e4 * Matrix{Float64}(I, 3, 3)\n\nfilt = kalman_filter_full(p_final, yields, a1, P1)\nsmooth = kalman_smoother(Z_opt, T_opt, filt.at, filt.Pt, filt.vt, filt.Ft)\n\n# 6. ANALYZE FACTORS\nprintln(\"\\n=== Factor Analysis ===\")\nfor (i, name) in enumerate([\"Level\", \"Slope\", \"Curvature\"])\n    f = smooth.alpha[i, :]\n    println(\"$name: mean=$(round(mean(f), digits=2)), std=$(round(std(f), digits=2))\")\nend\n\n# 7. FORECAST (optional)\n# Build forecast from final state\nh = 12  # 12-period ahead\nf_last = smooth.alpha[:, end]\nf_forecast = zeros(3, h)\nfor t in 1:h\n    f_forecast[:, t] = T_opt^t * f_last\nend\n\ny_forecast = Z_opt * f_forecast\nprintln(\"\\n=== 12-Period Yield Forecast ===\")\nprintln(\"Short rate (3m): \", round.(y_forecast[1, :], digits=2))","category":"section"},{"location":"tutorials/estimation_methods.html#Summary","page":"Estimation Methods","title":"Summary","text":"Filthy.jl provides multiple estimation approaches to suit different needs:\n\noptimize_ssm(spec, y): General-purpose MLE using automatic differentiation\nem_ssm(spec, y): EM algorithm for models with closed-form M-steps\nprofile_em_ssm(spec, y): Profile EM for DNS models with nonlinear λ\nfit!(MLE(), model, y): Convenience wrapper with pre-allocated storage\nfit!(EM(), model, y): High-performance EM for large-scale applications\n\nChoose based on your model structure, computational requirements, and preference for API style.","category":"section"},{"location":"tutorials/estimation_methods.html#Next-Steps","page":"Estimation Methods","title":"Next Steps","text":"See Custom Models for building your own specifications\nSee Dynamic Factor Models for large-panel factor analysis\nSee Parameter Transformations for understanding constraints","category":"section"},{"location":"api/core.html#Core-Functions","page":"Core Functions","title":"Core Functions","text":"This page documents the core Kalman filtering and smoothing functions.","category":"section"},{"location":"api/core.html#Types","page":"Core Functions","title":"Types","text":"","category":"section"},{"location":"api/core.html#Kalman-Filter","page":"Core Functions","title":"Kalman Filter","text":"","category":"section"},{"location":"api/core.html#Log-Likelihood","page":"Core Functions","title":"Log-Likelihood","text":"","category":"section"},{"location":"api/core.html#Full-Filter-Output","page":"Core Functions","title":"Full Filter Output","text":"","category":"section"},{"location":"api/core.html#Kalman-Smoother","page":"Core Functions","title":"Kalman Smoother","text":"","category":"section"},{"location":"api/core.html#Prediction-and-Forecasting","page":"Core Functions","title":"Prediction and Forecasting","text":"","category":"section"},{"location":"api/core.html#Missing-Data-Utilities","page":"Core Functions","title":"Missing Data Utilities","text":"","category":"section"},{"location":"api/core.html#StaticArrays-Utilities","page":"Core Functions","title":"StaticArrays Utilities","text":"","category":"section"},{"location":"api/core.html#Filthy.KFParms","page":"Core Functions","title":"Filthy.KFParms","text":"KFParms{Zt, Ht, Tt, Rt, Qt}\n\nState-space model parameters for the Kalman filter.\n\nState Space Model\n\ny_t = Z * α_t + ε_t,    ε_t ~ N(0, H)\nα_{t+1} = T * α_t + R * η_t,    η_t ~ N(0, Q)\n\nFields\n\nZ: Observation matrix (p × m)\nH: Observation noise covariance (p × p)\nT: State transition matrix (m × m)\nR: State noise selection matrix (m × r)\nQ: State noise covariance (r × r)\n\n\n\n\n\n","category":"type"},{"location":"api/core.html#Filthy.KFParms_static","page":"Core Functions","title":"Filthy.KFParms_static","text":"KFParms_static(Z, H, T, R, Q)\n\nCreate KFParms with automatic StaticArrays conversion for small matrices.\n\nEquivalent to KFParms(to_static_if_small(Z), ...) for all parameters. Use this for potential performance gains when state/observation dimensions are small (≤ 13).\n\nExample\n\n# These become SMatrix automatically\np = KFParms_static([1.0;;], [1.0;;], [1.0;;], [1.0;;], [1.0;;])\ntypeof(p.Z)  # SMatrix{1, 1, Float64, 1}\n\n# Large matrices stay as Matrix\np = KFParms_static(rand(20,20), rand(20,20), rand(20,20), rand(20,20), rand(20,20))\ntypeof(p.Z)  # Matrix{Float64}\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.kalman_loglik","page":"Core Functions","title":"Filthy.kalman_loglik","text":"kalman_loglik(p::KFParms, y, a1, P1) -> loglik\n\nCompute the log-likelihood of a linear Gaussian state space model using the Kalman filter.\n\nThis is an AD-compatible implementation that:\n\nUses pure functional operations (no in-place mutations)\nAvoids try-catch blocks\nPropagates element types correctly for automatic differentiation\nHandles missing observations (NaN values)\n\nArguments\n\np::KFParms: State space parameters (Z, H, T, R, Q)\ny::AbstractMatrix: Observations (p × n matrix, where p is observation dim, n is time).                      Missing values should be marked as NaN.\na1::AbstractVector: Initial state mean\nP1::AbstractMatrix: Initial state covariance\n\nReturns\n\nloglik::Real: Log-likelihood of the observed (non-missing) data\n\nState Space Model\n\ny_t = Z * α_t + ε_t,    ε_t ~ N(0, H)\nα_{t+1} = T * α_t + R * η_t,    η_t ~ N(0, Q)\n\nMissing Data\n\nWhen y[:, t] contains any NaN, the observation is treated as missing. The filter skips the measurement update and propagates the state:     a{t+1} = T * at     P{t+1} = T * Pt * T' + R * Q * R'\n\n\n\n\n\nkalman_loglik(p::KFParms{<:SMatrix,...}, y, a1::SVector, P1::SMatrix) -> loglik\n\nFully static specialization of Kalman filter log-likelihood for small state dimensions.\n\nWhen all system matrices in KFParms are SMatrix and the initial state (a1, P1) are SVector/SMatrix, this method keeps all intermediate computations static, avoiding heap allocations in the inner loop.\n\nThis provides significant speedup (3x-17x depending on dimensions) for models where both state dimension m and observation dimension p are small (≤ STATIC_THRESHOLD).\n\nPerformance Notes\n\nZero allocations in the inner loop\nEnables loop unrolling and SIMD vectorization\nBest for m ≤ 8 and p ≤ 5 (speedup decreases for larger dimensions)\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.kalman_loglik_scalar","page":"Core Functions","title":"Filthy.kalman_loglik_scalar","text":"kalman_loglik_scalar(Z, H, T, R, Q, a1, P1, y) -> loglik\n\nScalar (univariate) version of the Kalman filter log-likelihood.\n\nOptimized for the common case of scalar state and observation. Handles missing observations marked as NaN.\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.kalman_filter_full","page":"Core Functions","title":"Filthy.kalman_filter_full","text":"kalman_filter_full(p::KFParms, y, a1, P1) -> KalmanFilterResult\n\nRun full Kalman filter returning both predicted and filtered states.\n\nFor n observations, returns n time points:\n\nat[:, t] = E[αₜ | y₁:ₜ₋₁] (predicted state before seeing yₜ)\natt[:, t] = E[αₜ | y₁:ₜ] (filtered state after seeing yₜ)\nPt[:, :, t] = Var[αₜ | y₁:ₜ₋₁] (predicted covariance)\nPtt[:, :, t] = Var[αₜ | y₁:ₜ] (filtered covariance)\n\nReturns a KalmanFilterResult with:\n\nloglik: Log-likelihood (of non-missing observations)\nat: Predicted state means (m × n)\nPt: Predicted state covariances (m × m × n)\natt: Filtered state means (m × n)\nPtt: Filtered state covariances (m × m × n)\nvt: Innovations (p × n), NaN for missing observations\nFt: Innovation covariances (p × p × n)\nKt: Kalman gains (m × p × n), zero for missing observations\nmissing_mask: BitVector indicating missing observations (length n)\n\nUse accessor methods: predicted_states, filtered_states, variances_predicted_states, variances_filtered_states, prediction_errors, variances_prediction_errors, kalman_gains, loglikelihood.\n\nMissing Data\n\nWhen y[:, t] contains any NaN, the observation is treated as missing. The filter skips the measurement update and propagates the state.\n\n\n\n\n\nkalman_filter_full(p::KFParms{<:SMatrix,...}, y, a1::SVector, P1::SMatrix) -> KalmanFilterResult\n\nFully static specialization of Kalman filter for small state dimensions.\n\nWhen all system matrices in KFParms are SMatrix and the initial state (a1, P1) are SVector/SMatrix, this method uses static arithmetic in the inner loop while storing results in regular arrays (which must be heap-allocated for the full filter).\n\nThe speedup comes from keeping intermediate state computations (a, P, v, F, K) as StaticArrays, avoiding temporary allocations within each iteration.\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.kalman_filter_full_scalar","page":"Core Functions","title":"Filthy.kalman_filter_full_scalar","text":"kalman_filter_full_scalar(Z, H, T, R, Q, a1, P1, y) -> KalmanFilterResultScalar\n\nScalar version of full Kalman filter for univariate state-space models. Handles missing observations marked as NaN.\n\nFor n observations, returns n time points:\n\nat[t] = E[αₜ | y₁:ₜ₋₁] (predicted state)\natt[t] = E[αₜ | y₁:ₜ] (filtered state)\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.kalman_smoother","page":"Core Functions","title":"Filthy.kalman_smoother","text":"kalman_smoother(Z, T, at, Pt, vt, Ft; compute_crosscov=false, missing_mask=nothing, Ptt=nothing)\n\nAD-compatible RTS smoother. Takes filter outputs and returns smoothed states.\n\nArguments\n\nZ: Observation matrix (p × m)\nT: Transition matrix (m × m)\nat: Predicted states (m × n), where at[:, t] = E[αₜ | y₁:ₜ₋₁]\nPt: Predicted covariances (m × m × n)\nvt: Innovations (p × n), where vt[:, t] = yₜ - Z * at[:, t]\nFt: Innovation covariances (p × p × n)\ncompute_crosscov: If true, compute lag-one covariances P_{t+1,t|n} (default: false)\nmissing_mask: Optional BitVector indicating missing observations (length n). If not provided, inferred from NaN values in vt.\nPtt: Optional filtered covariances (m × m × n). Required for cross-cov with missing data. If not provided and compute_crosscov=true, computed from predicted covariances.\n\nReturns\n\nNamed tuple with:\n\nalpha: Smoothed states (m × n), where alpha[:, t] = E[αₜ | y₁:ₙ]\nV: Smoothed covariances (m × m × n)\nP_crosslag: Cross-lag covariances P{t+1,t|n} (m × m × (n-1)), only if computecrosscov=true\n\nMissing Data Handling\n\nWhen an observation is missing (indicated by missing_mask[t] == true or NaN in vt[:, t]), the smoother uses a simplified recursion that skips the measurement update:     r{t-1} = T' * rt     N{t-1} = T' * Nt * T This is consistent with treating missing observations as having infinite variance.\n\nCross-lag covariance formula\n\nUsing Shumway & Stoffer (2017), the lag-one covariance smoother gives:     Jt = P{t|t} * T' * inv(P{t+1|t})     P{t+1,t|n} = V{t+1} * Jt' where P_{t|t} is the filtered (updated) covariance.\n\nFor missing observations at time t, P{t|t} = P{t|t-1} (no update).\n\nNotes\n\nThis follows Durbin & Koopman (2012), Chapter 4, equations (4.32)-(4.44). Uses predicted states (at = a_{t|t-1}) not filtered states for the recursion. This is used by the EM algorithm which requires E[αₜ αₜ₋₁' | y₁:ₙ] for M-step updates.\n\n\n\n\n\nkalman_smoother(result::KalmanFilterResult, Z, T; compute_crosscov=false)\n\nRun RTS smoother using filter result. Convenience method.\n\nUses missing_mask from the filter result for proper handling of missing observations.\n\nReturns\n\nNamed tuple with:\n\nalpha: Smoothed states (m × n)\nV: Smoothed covariances (m × m × n)\nP_crosslag: Cross-lag covariances (only if compute_crosscov=true)\n\n\n\n\n\nkalman_smoother(p::KFParms, y, a1, P1; compute_crosscov=false) -> NamedTuple\n\nHigh-level Kalman smoother that runs filter and smoother in one call.\n\nArguments\n\np::KFParms: State-space model parameters (Z, H, T, R, Q)\ny: Observations (p × n matrix)\na1: Initial state mean (m-vector)\nP1: Initial state covariance (m × m matrix)\ncompute_crosscov: If true, compute lag-one covariances (default: false)\n\nReturns\n\nNamed tuple with:\n\nα: Smoothed states (m × n), where α[:, t] = E[αₜ | y₁:ₙ]\nV: Smoothed state covariances (m × m × n)\nP_crosslag: Cross-lag covariances (only if compute_crosscov=true)\n\nExample\n\np = KFParms(Z, H, T, R, Q)\nresult = kalman_smoother(p, y, a1, P1)\nsmoothed_states = result.α\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.kalman_smoother_scalar","page":"Core Functions","title":"Filthy.kalman_smoother_scalar","text":"kalman_smoother_scalar(Z, T, at, Pt, vt, Ft; missing_mask=nothing)\n\nScalar version of RTS smoother for univariate state-space models.\n\nHandles missing observations (NaN in vt or indicated by missing_mask).\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.kalman_filter_and_smooth","page":"Core Functions","title":"Filthy.kalman_filter_and_smooth","text":"kalman_filter_and_smooth(p::KFParms, y, a1, P1)\n\nCombined filter and smoother for state-space models.\n\nReturns filtered states, smoothed states, and log-likelihood. AD-compatible.\n\nArguments\n\np::KFParms: State-space model parameters (Z, H, T, R, Q)\ny: Observations (p × n matrix)\na1: Initial state mean (m-vector)\nP1: Initial state covariance (m × m matrix)\n\nReturns\n\nNamed tuple with:\n\nloglik: Log-likelihood\na_filtered: Filtered states (m × n) - same as att from filter result\nP_filtered: Filtered covariances (m × m × n) - same as Ptt from filter result\nalpha_smooth: Smoothed states (m × n)\nV_smooth: Smoothed covariances (m × m × n)\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.predict","page":"Core Functions","title":"Filthy.predict","text":"predict(spec::SSMSpec, θ::NamedTuple, y::AbstractMatrix; use_static=true) -> NamedTuple\n\nCompute in-sample one-step-ahead predictions for a state-space model.\n\nFor n observations, returns:\n\nyhat: One-step-ahead predictions ŷₜ|ₜ₋₁ = Z * aₜ (obs_dim × n)\na: States (state_dim × (n+1)), where a[:, t] = E[αₜ | y₁:ₜ₋₁]\nP: State covariances (statedim × statedim × (n+1))\nv: Innovations vₜ = yₜ - ŷₜ|ₜ₋₁ (obs_dim × n)\nF: Innovation covariances (obsdim × obsdim × n)\nloglik: Log-likelihood\n\nNote: a[:, 1] is the initial state, a[:, n+1] is the forecast state.\n\nArguments\n\nuse_static::Bool=true: Use StaticArrays for small matrices (dimensions ≤ 13)\n\nExample\n\nspec = local_level()\nθ = (σ_obs = 1.0, σ_level = 0.5)\ny = randn(1, 100)\n\npred = predict(spec, θ, y)\nplot(vec(y), label=\"observed\")\nplot!(vec(pred.yhat), label=\"predicted\")\n\n\n\n\n\npredict(p::KFParms, y, a1, P1) -> NamedTuple\n\nLow-level prediction using KFParms directly.\n\nReturns predicted states (at = a_{t|t-1}) and related quantities.\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.forecast","page":"Core Functions","title":"Filthy.forecast","text":"forecast(spec::SSMSpec, θ::NamedTuple, y::AbstractMatrix, h::Int; use_static=true) -> NamedTuple\n\nForecast h steps ahead beyond the observed data.\n\nReturns a NamedTuple with:\n\nyhat: Forecasted observations (obs_dim × h)\na: Forecasted states (state_dim × h)\nP: Forecasted state covariances (statedim × statedim × h)\nF: Forecasted observation covariances (obsdim × obsdim × h)\n\nThe forecast starts from a[:, n+1] = E[αₙ₊₁ | y₁:ₙ] from the filter.\n\nArguments\n\nuse_static::Bool=true: Use StaticArrays for small matrices (dimensions ≤ 13)\n\nExample\n\nspec = local_level()\nθ = (σ_obs = 1.0, σ_level = 0.5)\ny = randn(1, 100)\n\nfc = forecast(spec, θ, y, 12)  # 12-step ahead forecast\nprintln(\"Forecast: \", fc.yhat)\nprintln(\"Forecast std: \", sqrt.(fc.F[1,1,:]))\n\n\n\n\n\nforecast(p::KFParms, y, a1, P1, h) -> NamedTuple\n\nLow-level forecasting using KFParms directly.\n\nStarts from the last filtered state and propagates forward h steps.\n\n\n\n\n\nforecast(model::StateSpaceModel, h::Int)\n\nForecast h steps ahead from fitted model.\n\nUses the filtered state at the last observation and iterates forward.\n\nArguments\n\nmodel: Fitted StateSpaceModel\nh: Forecast horizon\n\nReturns\n\nNamedTuple with:\n\nyhat: Forecasted observations (p × h)\na: Forecasted states (m × h)\nP: Forecasted state covariances (m × m × h)\nF: Forecasted observation covariances (p × p × h)\n\nExample\n\nmodel = StateSpaceModel(local_level(), 100)\nfit!(MLE(), model, y)\nfc = forecast(model, 10)\nfc.yhat  # 1 × 10 forecasts\n\n\n\n\n\nforecast(model::DynamicFactorModel, h::Int)\n\nCompute h-step ahead forecasts from a fitted DFM.\n\nUses the filtered state at the last observation (αₙ|ₙ, Pₙ|ₙ) and iterates the state equation forward:     E[αₙ₊ₕ|Y₁:ₙ] = Tʰ αₙ|ₙ     Var[αₙ₊ₕ|Y₁:ₙ] = Tʰ Pₙ|ₙ (T')ʰ + Σⱼ₌₀ʰ⁻¹ Tʲ RQR' (T')ʲ\n\nArguments\n\nmodel::DynamicFactorModel - Fitted DFM model\nh::Int - Forecast horizon\n\nReturns\n\nDynamicFactorModelForecast with forecasted states and observations\n\nExample\n\nmodel = DynamicFactorModel(100, 6, 200; factor_lags=3)\nfit!(EM(), model, y)\nfc = forecast(model, 4)\nci = forecast_interval(fc, 0.05)\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.forecast_paths","page":"Core Functions","title":"Filthy.forecast_paths","text":"forecast_paths(spec::SSMSpec, θ::NamedTuple, y::AbstractMatrix, h::Int, n_paths::Int; use_static=true) -> Array\n\nSimulate n_paths forecast trajectories of length h.\n\nReturns an array of size (obsdim × h × npaths).\n\nUseful for fan charts and prediction intervals.\n\nArguments\n\nuse_static::Bool=true: Use StaticArrays for small matrices (dimensions ≤ 13)\n\nExample\n\npaths = forecast_paths(spec, θ, y, 12, 1000)\nquantiles = [quantile(paths[1, j, :], [0.1, 0.5, 0.9]) for j in 1:12]\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.missing_to_nan","page":"Core Functions","title":"Filthy.missing_to_nan","text":"missing_to_nan(y::AbstractArray)\n\nConvert missing values to NaN. Returns a Float64 array.\n\nUse this if your data contains missing values:\n\ny_clean = missing_to_nan(y_with_missing)\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.nan_to_missing","page":"Core Functions","title":"Filthy.nan_to_missing","text":"nan_to_missing(y::AbstractArray)\n\nConvert NaN values to missing. Returns a Union{Float64, Missing} array.\n\nUse this if you prefer working with missing:\n\ny_missing = nan_to_missing(y_with_nan)\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.count_missing","page":"Core Functions","title":"Filthy.count_missing","text":"count_missing(y::AbstractMatrix) -> Int\n\nCount number of time periods with at least one missing observation.\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.ismissing_obs","page":"Core Functions","title":"Filthy.ismissing_obs","text":"ismissing_obs(y_t::AbstractVector) -> Bool\n\nCheck if observation vector contains any missing values (NaN).\n\n\n\n\n\nismissing_obs(y_t::Real) -> Bool\n\nCheck if scalar observation is missing (NaN).\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Filthy.STATIC_THRESHOLD","page":"Core Functions","title":"Filthy.STATIC_THRESHOLD","text":"Maximum dimension for automatic conversion to StaticArrays. Matrices with max(rows, cols) ≤ STATIC_THRESHOLD are converted.\n\n\n\n\n\n","category":"constant"},{"location":"api/core.html#Filthy.to_static_if_small","page":"Core Functions","title":"Filthy.to_static_if_small","text":"to_static_if_small(x)\n\nConvert array to StaticArray if dimensions are small enough (≤ STATIC_THRESHOLD). Returns the input unchanged if dimensions exceed threshold or if already a StaticArray.\n\nThis enables automatic performance optimization for small state-space models by using stack-allocated arrays that the compiler can unroll.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Optimization-and-Bayesian","page":"Optimization & Bayesian","title":"Optimization & Bayesian","text":"This page documents functions for parameter estimation and Bayesian inference.","category":"section"},{"location":"api/optimization.html#Optimization","page":"Optimization & Bayesian","title":"Optimization","text":"","category":"section"},{"location":"api/optimization.html#Parameter-Transformations","page":"Optimization & Bayesian","title":"Parameter Transformations","text":"","category":"section"},{"location":"api/optimization.html#Log-Density-Interface","page":"Optimization & Bayesian","title":"Log-Density Interface","text":"","category":"section"},{"location":"api/optimization.html#Prior-Distributions","page":"Optimization & Bayesian","title":"Prior Distributions","text":"","category":"section"},{"location":"api/optimization.html#Filthy.DSL.optimize_ssm","page":"Optimization & Bayesian","title":"Filthy.DSL.optimize_ssm","text":"optimize_ssm(spec, y; method=Optim.LBFGS(), kwargs...)\n\nOptimize state-space model parameters using Optimization.jl.\n\nWorks in unconstrained parameter space using automatic differentiation for gradient computation.\n\nArguments\n\nspec::SSMSpec: Model specification\ny::AbstractMatrix: Observations (p × n matrix)\nmethod: Optimization algorithm (default: L-BFGS)\nθ0: Initial parameter values (constrained). Default: initial_values(spec)\nad_backend: AD backend for gradients (default: Optimization.AutoForwardDiff())\nuse_static::Bool=true: Use StaticArrays for small matrices (dimensions ≤ 13)\nprob_kwargs: NamedTuple of kwargs passed to OptimizationProblem\nkwargs...: All other kwargs passed to Optimization.solve\n\nCommon solve kwargs\n\nmaxiters: Maximum iterations (default: 1000)\nmaxtime: Maximum time in seconds\nabstol: Absolute tolerance\nreltol: Relative tolerance\ncallback: Callback function (state, loss) -> Bool (return true to stop)\nprogress: Show progress bar (requires ProgressLogging.jl)\nshow_trace: Show optimization trace (Optim.jl specific)\n\nReturns\n\nNamed tuple with:\n\nθ: Optimal parameters (constrained space)\nloglik: Log-likelihood at optimum\nresult: Full Optimization.jl result object\nconverged: Whether optimization converged\n\nExample\n\nspec = local_level()\ny = randn(1, 100) .* 10 .+ 100\n\n# Basic usage\nresult = optimize_ssm(spec, y)\n\nprintln(\"Optimal parameters: \", result.θ)\nprintln(\"Log-likelihood: \", result.loglik)\n\n# With custom initial values\nresult2 = optimize_ssm(spec, y; θ0=(σ_obs=150.0, σ_level=50.0))\n\n# Different optimizer with options\nresult3 = optimize_ssm(spec, y;\n    method=Optim.Newton(),\n    maxiters=500,\n    show_trace=true\n)\n\n# With callback for monitoring\ncallback = (state, loss) -> begin\n    println(\"Iteration: loss = $loss\")\n    return false  # return true to stop\nend\nresult4 = optimize_ssm(spec, y; callback=callback)\n\n# Pass kwargs to OptimizationProblem (e.g., bounds in unconstrained space)\nresult5 = optimize_ssm(spec, y;\n    prob_kwargs=(lb=fill(-10.0, n_params(spec)),\n                 ub=fill(10.0, n_params(spec)))\n)\n\n# Disable StaticArrays (for debugging or if causing issues)\nresult6 = optimize_ssm(spec, y; use_static=false)\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Filthy.DSL.optimize_ssm_with_stderr","page":"Optimization & Bayesian","title":"Filthy.DSL.optimize_ssm_with_stderr","text":"optimize_ssm_with_stderr(spec, y; use_static=true, kwargs...)\n\nOptimize SSM and compute standard errors using the Hessian.\n\nReturns the same as optimize_ssm plus:\n\nstderr: Standard errors of parameters (in constrained space, approximate)\nhessian: Hessian matrix at the optimum (in unconstrained space)\n\nNote: Standard errors are approximate when using parameter transformations. For accurate standard errors on constrained parameters, use the delta method or bootstrap.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Filthy.DSL.build_transformation","page":"Optimization & Bayesian","title":"Filthy.DSL.build_transformation","text":"build_transformation(spec::SSMSpec)\n\nBuild a TransformVariables transformation from an SSMSpec.\n\nReturns a transformation that maps ℝⁿ → NamedTuple with the parameter names and appropriate constraints (positive for variances, bounded for others).\n\nExample\n\nspec = local_level()\nt = build_transformation(spec)\n# t transforms ℝ² → (σ_obs = ..., σ_level = ...)\n\nθ_nt = transform(t, randn(2))\n# θ_nt is a NamedTuple with positive values\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Filthy.DSL.transform_to_constrained","page":"Optimization & Bayesian","title":"Filthy.DSL.transform_to_constrained","text":"transform_to_constrained(spec, θ_unconstrained)\n\nTransform from unconstrained ℝⁿ to constrained parameter space. Returns (θ_constrained::NamedTuple, logjac::Real).\n\nUses TransformVariables.jl for the transformation.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Filthy.DSL.transform_to_unconstrained","page":"Optimization & Bayesian","title":"Filthy.DSL.transform_to_unconstrained","text":"transform_to_unconstrained(spec, θ_constrained::NamedTuple)\n\nTransform from constrained NamedTuple to unconstrained ℝⁿ. Inverse of transform_to_constrained.\n\n\n\n\n\ntransform_to_unconstrained(spec, θ_constrained::AbstractVector)\n\nTransform from constrained vector to unconstrained ℝⁿ. First converts vector to NamedTuple, then inverts.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Filthy.DSL.SSMLogDensity","page":"Optimization & Bayesian","title":"Filthy.DSL.SSMLogDensity","text":"SSMLogDensity(spec, y; prior=nothing, use_static=true)\n\nLog-density for a state-space model, evaluated in UNCONSTRAINED ℝⁿ space.\n\nThis integrates seamlessly with LogDensityProblems.jl for optimization and sampling. The transformation from unconstrained to constrained space is handled automatically using TransformVariables.jl.\n\nFields\n\nspec: Model specification (SSMSpec)\ntransformation: TransformVariables transformation (ℝⁿ → NamedTuple)\ny: Observation data (p × n matrix)\nprior: Optional prior on CONSTRAINED parameters (θ::NamedTuple -> log_prior)\nuse_static: Whether to use StaticArrays for small matrices (default: true)\n\nUsage\n\nspec = local_level()\ny = randn(1, 100)\n\n# Create log-density (works in unconstrained space)\nld = SSMLogDensity(spec, y)\n\n# Get initial point in unconstrained space\nθ0 = transform_to_unconstrained(spec, initial_values(spec))\n\n# Evaluate log-density\nlogdensity(ld, θ0)\n\n# For optimization (in unconstrained space, no bounds needed!)\nusing Optim\nresult = optimize(θ -> -logdensity(ld, θ), θ0, LBFGS())\n\n# Transform result back to NamedTuple\nθ_hat, _ = transform_to_constrained(spec, result.minimizer)\n# θ_hat is a NamedTuple like (σ_obs = 12.3, σ_level = 3.4)\n\n\n\n\n\n","category":"type"},{"location":"api/optimization.html#Filthy.DSL.logdensity","page":"Optimization & Bayesian","title":"Filthy.DSL.logdensity","text":"logdensity(ld::SSMLogDensity, θ_unconstrained)\n\nEvaluate the log-density at unconstrained parameters.\n\nReturns log p(y|θ) + log p(θ) + log|J| where J is the Jacobian of the transformation from unconstrained to constrained space.\n\nThe parameters are automatically transformed to a NamedTuple and passed to the likelihood function.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Filthy.DSL.FlatPrior","page":"Optimization & Bayesian","title":"Filthy.DSL.FlatPrior","text":"FlatPrior()\n\nImproper flat prior (log-density = 0 everywhere). Works with both Vector and NamedTuple arguments.\n\n\n\n\n\n","category":"type"},{"location":"api/optimization.html#Filthy.DSL.NormalPrior","page":"Optimization & Bayesian","title":"Filthy.DSL.NormalPrior","text":"NormalPrior(μ::NamedTuple, σ::NamedTuple)\n\nIndependent normal priors for parameters, specified by name.\n\nExample\n\nprior = NormalPrior(\n    (σ_obs = 10.0, σ_level = 5.0),   # means\n    (σ_obs = 5.0, σ_level = 2.0)     # standard deviations\n)\n\n\n\n\n\n","category":"type"},{"location":"api/optimization.html#Filthy.DSL.InverseGammaPrior","page":"Optimization & Bayesian","title":"Filthy.DSL.InverseGammaPrior","text":"InverseGammaPrior(α, β, param_names)\n\nInverse gamma priors for variance parameters at specified parameter names. Assumes θ contains standard deviations (will square them).\n\nExample\n\nprior = InverseGammaPrior(2.0, 1.0, (:σ_obs, :σ_level))\n\n\n\n\n\n","category":"type"},{"location":"api/optimization.html#Filthy.DSL.CompositePrior","page":"Optimization & Bayesian","title":"Filthy.DSL.CompositePrior","text":"CompositePrior(priors...)\n\nCombine multiple priors by summing their log-densities.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/transformations.html#Parameter-Transformations","page":"Parameter Transformations","title":"Parameter Transformations","text":"This tutorial explains how Filthy.jl handles parameter transformations for constrained optimization. Understanding this system is important for:\n\nCorrectly specifying parameter bounds\nInterpreting optimization results\nWorking with Bayesian inference\nImplementing custom models","category":"section"},{"location":"tutorials/transformations.html#Overview","page":"Parameter Transformations","title":"Overview","text":"State-space model parameters often have natural constraints:\n\nVariance parameters must be positive: sigma^2  0\nAR coefficients must satisfy stationarity: rho  1\nCorrelation parameters must be in -1 1\nProbabilities must be in 0 1\n\nFilthy.jl uses TransformVariables.jl to handle these constraints automatically. The key idea is to optimize in an unconstrained space mathbbR^n and transform to the constrained parameter space when needed.","category":"section"},{"location":"tutorials/transformations.html#How-It-Works","page":"Parameter Transformations","title":"How It Works","text":"","category":"section"},{"location":"tutorials/transformations.html#The-Two-Spaces","page":"Parameter Transformations","title":"The Two Spaces","text":"Constrained space (Theta): Where parameters have their natural interpretation\nExample: sigma^2_textobs = 2250 (a variance)\nUnconstrained space (mathbbR^n): Where optimization actually happens\nExample: theta_textunconstrained = 542 (the log of sigma^2)","category":"section"},{"location":"tutorials/transformations.html#Transformation-Flow","page":"Parameter Transformations","title":"Transformation Flow","text":"                    transform\nUnconstrained θ_u ─────────────> Constrained θ_c (NamedTuple)\n      ℝⁿ           (+ logjac)          Θ\n                                       │\n                                       ▼\n                              Build state-space matrices\n                                       │\n                                       ▼\n                              Compute log-likelihood\n\nThe optimizer works in unconstrained space. For each evaluation:\n\nTransform theta_u to theta_c (unconstrained to constrained)\nBuild state-space matrices using theta_c\nCompute log-likelihood\n(For Bayesian: add log Jacobian and log prior)","category":"section"},{"location":"tutorials/transformations.html#Specifying-Parameter-Bounds","page":"Parameter Transformations","title":"Specifying Parameter Bounds","text":"When you create a FreeParam, the bounds determine which transformation is applied:\n\n# Positive parameter (variance) - uses exp transform\nFreeParam(:var_obs, init=100.0, lower=0.0)\n\n# Bounded parameter (AR coefficient) - uses logit-like transform\nFreeParam(:ρ, init=0.8, lower=-0.99, upper=0.99)\n\n# Unbounded parameter - identity (no transform)\nFreeParam(:β, init=0.0)","category":"section"},{"location":"tutorials/transformations.html#Transformation-Rules","page":"Parameter Transformations","title":"Transformation Rules","text":"Bounds Transform Mathematical Form\n(-infty infty) Identity theta_c = theta_u\n0 infty) Exponential theta_c = exp(theta_u)\n(a b) Scaled logistic theta_c = a + (b-a) cdot textlogistic(theta_u)\n(-infty b Negative exp theta_c = b - exp(-theta_u)","category":"section"},{"location":"tutorials/transformations.html#For-Variance-Parameters","page":"Parameter Transformations","title":"For Variance Parameters","text":"Use lower=0.0 to ensure positivity:\n\n# Correct: estimate variance directly with positivity constraint\nH = [FreeParam(:var_obs, init=100.0, lower=0.0)]\n\nThe transformation automatically applied is:\n\nsigma^2 = exp(theta_u)\n\nSo if the optimizer finds theta_u = 46, you get sigma^2 = exp(46) approx 100.","category":"section"},{"location":"tutorials/transformations.html#Working-with-Transformations-Directly","page":"Parameter Transformations","title":"Working with Transformations Directly","text":"","category":"section"},{"location":"tutorials/transformations.html#Building-Transformations","page":"Parameter Transformations","title":"Building Transformations","text":"using Filthy\n\nspec = local_level()\nt = build_transformation(spec)\n\n# Transform from unconstrained to constrained\nθ_u = [4.6, 3.9]  # Unconstrained values\nθ_c = TransformVariables.transform(t, θ_u)\n# θ_c = (var_obs = 99.5, var_level = 49.4)","category":"section"},{"location":"tutorials/transformations.html#Transform-with-Jacobian","page":"Parameter Transformations","title":"Transform with Jacobian","text":"For Bayesian inference, you need the log Jacobian determinant:\n\nusing TransformVariables\n\nθ_c, logjac = transform_and_logjac(t, θ_u)\n# logjac accounts for the change of variables","category":"section"},{"location":"tutorials/transformations.html#Inverse-Transform","page":"Parameter Transformations","title":"Inverse Transform","text":"To go from constrained back to unconstrained:\n\nθ_u = transform_to_unconstrained(spec, θ_c)","category":"section"},{"location":"tutorials/transformations.html#Full-Covariance-Matrices","page":"Parameter Transformations","title":"Full Covariance Matrices","text":"For full positive-definite covariance matrices, Filthy.jl uses a special parameterization via cov_free:\n\nQ = cov_free(2, :Q)  # 2×2 PD covariance matrix\n\nThis creates:\n\nStandard deviation parameters: Q_σ_1, Q_σ_2 (positive, use exp transform)\nCorrelation parameters: Q_corr_1 (unconstrained, maps to valid correlation)\n\nThe matrix is reconstructed as:\n\nSigma = D cdot textCorr cdot D\n\nwhere D = textdiag(sigma_1 sigma_2) and textCorr is built from a Cholesky factor parameterization that guarantees positive definiteness.","category":"section"},{"location":"tutorials/transformations.html#Example:-Complete-Workflow","page":"Parameter Transformations","title":"Example: Complete Workflow","text":"using Filthy\n\n# Specify model with constrained parameters\nspec = custom_ssm(\n    Z = [1.0],\n    H = [FreeParam(:var_obs, init=100.0, lower=0.0)],  # Positive\n    T = [FreeParam(:ρ, init=0.8, lower=-0.99, upper=0.99)],  # Bounded\n    R = [1.0],\n    Q = [FreeParam(:var_state, init=50.0, lower=0.0)],  # Positive\n    a1 = [0.0],\n    P1 = [1e7]\n)\n\n# Simulate data\ny = randn(1, 200)\n\n# Optimize - all transformation handled automatically\nresult = optimize_ssm(spec, y)\n\n# Result is in constrained space\nprintln(\"var_obs = \", result.θ.var_obs)    # Positive value\nprintln(\"ρ = \", result.θ.ρ)                # In (-0.99, 0.99)\nprintln(\"var_state = \", result.θ.var_state)  # Positive value\n\n# If you need unconstrained values (e.g., for MCMC initialization)\nθ_u = transform_to_unconstrained(spec, result.θ)\nprintln(\"Unconstrained: \", θ_u)  # Values in ℝ³","category":"section"},{"location":"tutorials/transformations.html#Bayesian-Inference","page":"Parameter Transformations","title":"Bayesian Inference","text":"For Bayesian inference with MCMC, the log-density is computed in unconstrained space:\n\n# Create log-density object\nld = SSMLogDensity(spec, y)\n\n# Evaluate at unconstrained point\nθ_u = randn(n_params(spec))\nll = logdensity(ld, θ_u)\n# ll includes the log Jacobian automatically\n\nThe SSMLogDensity type implements LogDensityProblems.jl interface, so you can use it with any compatible sampler.","category":"section"},{"location":"tutorials/transformations.html#Tips-and-Best-Practices","page":"Parameter Transformations","title":"Tips and Best Practices","text":"Always use lower=0.0 for variances: This ensures the exp transform is applied.\nUse tight bounds for AR coefficients: lower=-0.99, upper=0.99 works better than (-1, 1) numerically.\nInitial values matter: Provide good initial values in the constrained space. They are automatically transformed.\nStandard errors are approximate: When using optimize_ssm_with_stderr, the standard errors are computed via the delta method and may be approximate for highly nonlinear transformations.\nFor debugging: Use transform_to_unconstrained and transform_to_constrained to verify parameter values at each stage.","category":"section"},{"location":"tutorials/transformations.html#API-Reference","page":"Parameter Transformations","title":"API Reference","text":"","category":"section"},{"location":"tutorials/transformations.html#Key-Functions","page":"Parameter Transformations","title":"Key Functions","text":"# Build transformation from spec\nt = build_transformation(spec)\n\n# Transform unconstrained → constrained\nθ_c, logjac = transform_to_constrained(spec, θ_u)\n\n# Transform constrained → unconstrained\nθ_u = transform_to_unconstrained(spec, θ_c)","category":"section"},{"location":"tutorials/transformations.html#Related-Types","page":"Parameter Transformations","title":"Related Types","text":"FreeParam: Specify a free parameter with bounds\nSSMLogDensity: Log-density in unconstrained space\nCovMatrixExpr: Expression for positive-definite covariance matrices","category":"section"},{"location":"tutorials/transformations.html#Technical-Implementation-Details","page":"Parameter Transformations","title":"Technical Implementation Details","text":"<!– NOTE TO MAINTAINERS: Add technical implementation details below. Topics to cover:\n\nExact mathematical formulas for each transform\nLog Jacobian computation\nNumerical considerations (overflow, underflow)\nAD compatibility notes\nPerformance considerations\n\n–>\n\nThis section will contain technical implementation details.","category":"section"},{"location":"api/dsl.html#DSL-and-Templates","page":"DSL & Templates","title":"DSL & Templates","text":"This page documents the domain-specific language for model specification and pre-built templates.","category":"section"},{"location":"api/dsl.html#Core-Types","page":"DSL & Templates","title":"Core Types","text":"","category":"section"},{"location":"api/dsl.html#Model-Introspection","page":"DSL & Templates","title":"Model Introspection","text":"","category":"section"},{"location":"api/dsl.html#Model-Building","page":"DSL & Templates","title":"Model Building","text":"","category":"section"},{"location":"api/dsl.html#Pre-Built-Templates","page":"DSL & Templates","title":"Pre-Built Templates","text":"","category":"section"},{"location":"api/dsl.html#Local-Level-Model","page":"DSL & Templates","title":"Local Level Model","text":"","category":"section"},{"location":"api/dsl.html#Local-Linear-Trend","page":"DSL & Templates","title":"Local Linear Trend","text":"","category":"section"},{"location":"api/dsl.html#AR(1)-Model","page":"DSL & Templates","title":"AR(1) Model","text":"","category":"section"},{"location":"api/dsl.html#ARMA-Model","page":"DSL & Templates","title":"ARMA Model","text":"","category":"section"},{"location":"api/dsl.html#Dynamic-Factor-Model","page":"DSL & Templates","title":"Dynamic Factor Model","text":"","category":"section"},{"location":"api/dsl.html#Custom-Model-Specification","page":"DSL & Templates","title":"Custom Model Specification","text":"","category":"section"},{"location":"api/dsl.html#Parameter-Expressions","page":"DSL & Templates","title":"Parameter Expressions","text":"","category":"section"},{"location":"api/dsl.html#DNS/Svensson-Yield-Curve-Helpers","page":"DSL & Templates","title":"DNS/Svensson Yield Curve Helpers","text":"","category":"section"},{"location":"api/dsl.html#Filthy.DSL.SSMParameter","page":"DSL & Templates","title":"Filthy.DSL.SSMParameter","text":"SSMParameter{T<:Real}\n\nRepresents a single estimable parameter in a state-space model.\n\nFields\n\nname::Symbol: Parameter name for identification\nlower::T: Lower bound for optimization (use -Inf for unbounded)\nupper::T: Upper bound for optimization (use Inf for unbounded)\ninit::T: Initial value for optimization\n\n\n\n\n\n","category":"type"},{"location":"api/dsl.html#Filthy.DSL.SSMSpec","page":"DSL & Templates","title":"Filthy.DSL.SSMSpec","text":"SSMSpec\n\nComplete specification of a state-space model.\n\nFields\n\nname::Symbol: Model name\nn_states::Int: Number of state variables\nn_obs::Int: Number of observables\nn_shocks::Int: Number of shocks\nparams::Vector{SSMParameter}: Free parameters to estimate\nZ::SSMMatrixSpec: Observation matrix specification\nH::SSMMatrixSpec: Observation covariance specification\nT::SSMMatrixSpec: Transition matrix specification\nR::SSMMatrixSpec: Selection matrix specification\nQ::SSMMatrixSpec: State covariance specification\na1::Vector{MatrixElement}: Initial state mean\nP1::SSMMatrixSpec: Initial state covariance\nmatrix_exprs::Dict{Symbol,Any}: Expression-based matrices (for DNS, etc.)\n\n\n\n\n\n","category":"type"},{"location":"api/dsl.html#Filthy.DSL.FixedValue","page":"DSL & Templates","title":"Filthy.DSL.FixedValue","text":"FixedValue{T}\n\nRepresents a fixed (non-estimable) value in a state-space model.\n\n\n\n\n\n","category":"type"},{"location":"api/dsl.html#Filthy.DSL.ParameterRef","page":"DSL & Templates","title":"Filthy.DSL.ParameterRef","text":"ParameterRef\n\nReference to a parameter by name, used for building matrix mappings.\n\n\n\n\n\n","category":"type"},{"location":"api/dsl.html#Filthy.DSL.param_names","page":"DSL & Templates","title":"Filthy.DSL.param_names","text":"param_names(spec::SSMSpec)\n\nReturn the names of all free parameters.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.n_params","page":"DSL & Templates","title":"Filthy.DSL.n_params","text":"n_params(spec::SSMSpec)\n\nReturn the number of free parameters.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.initial_values","page":"DSL & Templates","title":"Filthy.DSL.initial_values","text":"initial_values(spec::SSMSpec)\n\nReturn a vector of initial parameter values.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.param_bounds","page":"DSL & Templates","title":"Filthy.DSL.param_bounds","text":"param_bounds(spec::SSMSpec)\n\nReturn lower and upper bound vectors.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.build_linear_state_space","page":"DSL & Templates","title":"Filthy.DSL.build_linear_state_space","text":"build_linear_state_space(spec::SSMSpec, θ, y; use_static=true)\n\nBuild state-space model components from specification.\n\nAccepts either a Vector or NamedTuple of parameters.\n\nArguments\n\nspec::SSMSpec: Model specification\nθ: Parameters (Vector or NamedTuple)\ny: Observations (used for dimension inference in some models)\nuse_static::Bool=true: If true, automatically convert small matrices (dimensions ≤ 13) to StaticArrays for better performance\n\nReturns a NamedTuple with:\n\np: KFParms struct with system matrices (Z, H, T, R, Q)\na1: Initial state mean vector\nP1: Initial state covariance matrix\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.ssm_loglik","page":"DSL & Templates","title":"Filthy.DSL.ssm_loglik","text":"ssm_loglik(spec::SSMSpec, θ::NamedTuple, y::AbstractMatrix)\n\nCompute log-likelihood directly from spec and NamedTuple parameters.\n\nThis is the recommended high-level API for likelihood evaluation.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.objective_function","page":"DSL & Templates","title":"Filthy.DSL.objective_function","text":"objective_function(spec::SSMSpec, y)\n\nCreate a negative log-likelihood function for optimization.\n\nReturns a callable that computes -loglik for a given parameter vector theta. Uses the AD-compatible kalman_loglik function.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.local_level","page":"DSL & Templates","title":"Filthy.DSL.local_level","text":"local_level(; var_obs=:free, var_level=:free, diffuse=true)\n\nCreate specification for local level (random walk + noise) model.\n\nModel:     yₜ = μₜ + εₜ,  εₜ ~ N(0, varobs)     μₜ₊₁ = μₜ + ηₜ,  ηₜ ~ N(0, varlevel)\n\nArguments\n\nvar_obs: Observation noise variance. Use :free to estimate, a number to fix,            or (init=, lower=, upper=) for custom estimation settings.\nvar_level: Level noise variance. Same options as var_obs.\ndiffuse: Use diffuse initialization (default: true)\n\nExamples\n\n# Estimate both (default)\nspec = local_level()\n\n# Fix observation variance, estimate level variance\nspec = local_level(var_obs=225.0, var_level=:free)\n\n# Custom initial value and bounds\nspec = local_level(var_obs=(init=225.0, lower=1.0, upper=10000.0))\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.local_linear_trend","page":"DSL & Templates","title":"Filthy.DSL.local_linear_trend","text":"local_linear_trend(; var_obs=:free, var_level=:free, var_slope=:free, diffuse=true)\n\nCreate specification for local linear trend model.\n\nModel:     yₜ = μₜ + εₜ,  εₜ ~ N(0, varobs)     μₜ₊₁ = μₜ + νₜ + ηₜ,  ηₜ ~ N(0, varlevel)     νₜ₊₁ = νₜ + ζₜ,  ζₜ ~ N(0, var_slope)\n\nState: [μₜ, νₜ]\n\nArguments\n\nvar_obs, var_level, var_slope: Use :free to estimate, a number to fix, or (init=, lower=, upper=) for custom settings.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.ar1","page":"DSL & Templates","title":"Filthy.DSL.ar1","text":"ar1(; ρ=:free, var_obs=:free, var_state=:free)\n\nCreate specification for AR(1) plus noise model.\n\nModel:     yₜ = xₜ + εₜ,  εₜ ~ N(0, varobs)     xₜ₊₁ = ρ xₜ + ηₜ,  ηₜ ~ N(0, varstate)\n\nArguments\n\nρ: AR coefficient (|ρ| < 1). Use :free, a number, or (init=, lower=, upper=).\nvar_obs, var_state: Noise variances. Same options.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.arma","page":"DSL & Templates","title":"Filthy.DSL.arma","text":"arma(p::Int, q::Int; ar_init=nothing, ma_init=nothing, var_init=1.0)\n\nCreate specification for ARMA(p,q) model in state-space form.\n\nUses the innovations state-space representation.\n\nParameters: ar coefficients φ₁...φₚ, ma coefficients θ₁...θᵧ, var (innovation variance)\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.dynamic_factor","page":"DSL & Templates","title":"Filthy.DSL.dynamic_factor","text":"dynamic_factor(n_obs, n_factors; factor_lags=1, obs_lags=0, correlated_errors=false,\n               loadings_init=0.5, ar_init=0.5, var_obs_init=1.0,\n               var_factor_init=1.0, diffuse=true)\n\nCreate specification for a dynamic factor model with VAR factor dynamics and optional lagged factor loadings.\n\nModel:     yₜ = Λ₀ fₜ + Λ₁ fₜ₋₁ + ... + Λₛ fₜ₋ₛ + εₜ,  εₜ ~ N(0, H)     fₜ = Φ₁ fₜ₋₁ + Φ₂ fₜ₋₂ + ... + Φₚ fₜ₋ₚ + ηₜ,  ηₜ ~ N(0, Q)\n\nwhere:\n\nyₜ is n_obs × 1 vector of observations\nfₜ is n_factors × 1 vector of latent factors\nΛₗ is nobs × nfactors factor loadings matrix for lag l (l = 0, ..., s)\nΦₗ is nfactors × nfactors AR coefficient matrix for lag l (diagonal)\nH is observation error covariance (diagonal or full if correlated_errors=true)\nQ is factor innovation covariance (diagonal)\ns = obslags, p = factorlags\n\nThe model is cast in companion form with state vector:     αₜ = [fₜ', fₜ₋₁', ..., fₜ₋ₘ₊₁']'  where m = max(p, s+1)\n\nIdentification\n\nFor identification, the first n_factors rows of Λ₀ (contemporaneous loadings) are set to an identity matrix:\n\nλ₀{i,i} = 1 for i ≤ nfactors\nλ₀{i,j} = 0 for i < j ≤ nfactors\n\nAll other loadings (including lagged) are free parameters.\n\nArguments\n\nn_obs::Int: Number of observable variables\nn_factors::Int: Number of latent factors (must be < n_obs)\nfactor_lags::Int=1: Number of lags in factor VAR dynamics (p)\nobs_lags::Int=0: Number of lagged factors that load onto observations (s).                    If s > 0, observations depend on fₜ, fₜ₋₁, ..., fₜ₋ₛ.\ncorrelated_errors::Bool=false: If true, H is a full covariance matrix (inexact DFM).                                  If false, H is diagonal (exact DFM).\nloadings_init::Real=0.5: Initial value for free factor loadings\nar_init::Real=0.5: Initial value for AR coefficients (divided by lag number)\nvar_obs_init::Real=1.0: Initial value for observation error variances\nvar_factor_init::Real=1.0: Initial value for factor innovation variances\ndiffuse::Bool=true: Use diffuse initialization for factors\n\nState Space Representation\n\nState dimension is n_factors * max(factor_lags, obs_lags + 1).\n\nState vector: αₜ = [fₜ', fₜ₋₁', ..., fₜ₋ₘ₊₁']'\n\nTransition matrix (companion form):     T = [Φ₁  Φ₂  ... Φₚ  0  ... 0]   (padded with zeros if m > p)         [I   0   ... 0   0  ... 0]         [0   I   ... 0   0  ... 0]         [⋮   ⋮   ⋱   ⋮   ⋮  ⋱   ⋮]         [0   0   ... I   0  ... 0]\n\nObservation matrix: Z = [Λ₀  Λ₁  ...  Λₛ  0  ...  0]\n\nExample\n\n# Standard 1-factor model (no lagged loadings)\nspec = dynamic_factor(4, 1)\n\n# 1-factor model with 2 factor lags, no lagged loadings\nspec = dynamic_factor(4, 1; factor_lags=2)\n# yₜ = Λ₀ fₜ + εₜ\n# fₜ = φ₁ fₜ₋₁ + φ₂ fₜ₋₂ + ηₜ\n\n# 1-factor model with lagged loadings (s=1)\nspec = dynamic_factor(4, 1; factor_lags=2, obs_lags=1)\n# yₜ = Λ₀ fₜ + Λ₁ fₜ₋₁ + εₜ\n# fₜ = φ₁ fₜ₋₁ + φ₂ fₜ₋₂ + ηₜ\n# Parameters: λ0_i_j (contemporaneous), λ1_i_j (lag 1 loadings)\n\n# 2-factor model with obs_lags=2, factor_lags=1\nspec = dynamic_factor(5, 2; factor_lags=1, obs_lags=2)\n# yₜ = Λ₀ fₜ + Λ₁ fₜ₋₁ + Λ₂ fₜ₋₂ + εₜ\n# State: [f₁ₜ, f₂ₜ, f₁ₜ₋₁, f₂ₜ₋₁, f₁ₜ₋₂, f₂ₜ₋₂]\n\nParameters Created\n\nλ0_i_j: Contemporaneous loadings (i > j for identification in first k rows)\nλl_i_j: Loadings for lag l (l = 1, ..., obs_lags), all free\nφ_i_l: AR coefficient for factor i at lag l (l = 1, ..., factor_lags)\nvar_obs_i: Observation error variances (if correlated_errors=false)\nH_σ_i, H_corr_i: Covariance parameters (if correlated_errors=true, σ for DCorrD)\nvar_factor_i: Factor innovation variances\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.custom_ssm","page":"DSL & Templates","title":"Filthy.DSL.custom_ssm","text":"custom_ssm(; Z, H, T, R, Q, a1, P1, name=:CustomSSM)\n\nCreate a state-space model specification from explicit matrices.\n\nUse FreeParam(...) to mark elements that should be estimated. Use regular numbers for fixed values.\n\nArguments\n\nZ: Observation matrix (p × m) - relates states to observations\nH: Observation covariance (p × p)\nT: Transition matrix (m × m) - state dynamics\nR: Selection matrix (m × r) - maps shocks to states\nQ: State covariance (r × r)\na1: Initial state mean (m-vector)\nP1: Initial state covariance (m × m)\nname: Model name (default: :CustomSSM)\n\nExample: Local Level Model\n\nspec = custom_ssm(\n    Z = [1.0],\n    H = [FreeParam(:var_obs, init=225.0, lower=0.0)],\n    T = [1.0],\n    R = [1.0],\n    Q = [FreeParam(:var_level, init=100.0, lower=0.0)],\n    a1 = [0.0],\n    P1 = [1e7]\n)\n\n# Check parameters\nparam_names(spec)  # [:var_obs, :var_level]\n\nExample: Local Linear Trend\n\nusing LinearAlgebra\n\nspec = custom_ssm(\n    Z = [1.0 0.0],\n    H = [FreeParam(:var_obs, init=1.0, lower=0.0)],\n    T = [1.0 1.0;\n         0.0 1.0],\n    R = Matrix(1.0I, 2, 2),\n    Q = [FreeParam(:var_level, init=0.01, lower=0.0)  0.0;\n         0.0  FreeParam(:var_slope, init=0.0001, lower=0.0)],\n    a1 = [0.0, 0.0],\n    P1 = 1e7 * Matrix(1.0I, 2, 2)\n)\n\nExample: AR(1) with fixed ρ\n\nspec = custom_ssm(\n    Z = [1.0],\n    H = [FreeParam(:var_obs, init=1.0, lower=0.0)],\n    T = [0.9],  # Fixed AR coefficient\n    R = [1.0],\n    Q = [FreeParam(:var_state, init=1.0, lower=0.0)],\n    a1 = [0.0],\n    P1 = [1e4]\n)\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.FreeParam","page":"DSL & Templates","title":"Filthy.DSL.FreeParam","text":"FreeParam(name; init=0.0, lower=-Inf, upper=Inf)\n\nMark a matrix element as a free parameter to be estimated.\n\nArguments\n\nname::Symbol: Parameter name\ninit: Initial value for optimization\nlower, upper: Bounds for optimization (TransformVariables.jl handles transformations)\n\nExample\n\n# A variance parameter (estimate variance directly, lower=0 triggers asℝ₊)\nH = [FreeParam(:var_obs, init=100.0, lower=0.0)]\n\n# A coefficient with bounds\nT = [FreeParam(:ρ, init=0.8, lower=-0.99, upper=0.99)]\n\n\n\n\n\n","category":"type"},{"location":"api/dsl.html#Filthy.DSL.ParamExpr","page":"DSL & Templates","title":"Filthy.DSL.ParamExpr","text":"ParamExpr(params, data, expr)\n\nA matrix element that is a function of parameters and external data.\n\nArguments\n\nparams: Symbol or tuple of Symbols for parameter names\ndata: NamedTuple of external data used in the expression\nexpr: Function (param_values..., data...) -> scalar\n\nExample: Nelson-Siegel loading\n\nτ = 30  # maturity in months\n# Loading: (1 - exp(-τ*λ)) / (τ*λ)\nelem = ParamExpr(:λ, (τ=τ,), (λ, τ) -> (1 - exp(-τ*λ)) / (τ*λ))\n\n\n\n\n\n","category":"type"},{"location":"api/dsl.html#Filthy.DSL.MatrixExpr","page":"DSL & Templates","title":"Filthy.DSL.MatrixExpr","text":"MatrixExpr(params, data, builder)\n\nA full matrix that is built from parameters and external data.\n\nArguments\n\nparams: Vector of SSMParameter specs for parameters used\ndata: NamedTuple of external data\nbuilder: Function (θ_dict, data) -> Matrix where θ_dict maps param names to values\n\nExample: Dynamic Nelson-Siegel factor loadings\n\nmaturities = [3, 6, 12, 24, 60, 120]  # months\n\nfunction dns_loadings(θ, data)\n    λ = θ[:λ]\n    τ = data.maturities\n    p = length(τ)\n    Z = ones(p, 3)\n    for i in 1:p\n        x = τ[i] * λ\n        Z[i, 2] = (1 - exp(-x)) / x\n        Z[i, 3] = Z[i, 2] - exp(-x)\n    end\n    Z\nend\n\nZ = MatrixExpr(\n    [SSMParameter(:λ, init=0.0609, lower=0.001, upper=1.0)],\n    (maturities=maturities,),\n    dns_loadings\n)\n\n\n\n\n\n","category":"type"},{"location":"api/dsl.html#Filthy.DSL.build_dns_loadings","page":"DSL & Templates","title":"Filthy.DSL.build_dns_loadings","text":"build_dns_loadings(maturities; λ_init=0.0609, λ_lower=0.001, λ_upper=1.0)\n\nCreate a MatrixExpr for Dynamic Nelson-Siegel factor loadings.\n\nArguments\n\nmaturities: Vector of maturities (e.g., in months)\nλ_init, λ_lower, λ_upper: Parameter settings for decay rate λ\n\nReturns\n\nA MatrixExpr that builds the p×3 loading matrix Z where:\n\nColumn 1: Level factor (all 1s)\nColumn 2: Slope factor (1 - exp(-λτ)) / (λτ)\nColumn 3: Curvature factor (1 - exp(-λτ)) / (λτ) - exp(-λτ)\n\nExample\n\nZ = build_dns_loadings([3, 6, 12, 24, 60, 120])\n\nspec = custom_ssm(\n    Z = Z,\n    H = diag_free(6, :var_obs, init=0.01),\n    T = [FreeParam(:φ_L, init=0.99, lower=0.0, upper=0.9999) 0.0 0.0;\n         0.0 FreeParam(:φ_S, init=0.99, lower=0.0, upper=0.9999) 0.0;\n         0.0 0.0 FreeParam(:φ_C, init=0.99, lower=0.0, upper=0.9999)],\n    R = identity_mat(3),\n    Q = diag_free([:var_L, :var_S, :var_C], init=0.01),\n    a1 = [0.0, 0.0, 0.0],\n    P1 = 1e4 * identity_mat(3)\n)\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.build_svensson_loadings","page":"DSL & Templates","title":"Filthy.DSL.build_svensson_loadings","text":"build_svensson_loadings(maturities; λ1_init=0.0609, λ2_init=0.03, ...)\n\nCreate a MatrixExpr for Svensson (4-factor) yield curve loadings.\n\nAdds a second curvature factor with separate decay rate λ2.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.dns_loading1","page":"DSL & Templates","title":"Filthy.DSL.dns_loading1","text":"dns_loading1(λ, τ)\n\nFirst slope loading for Dynamic Nelson-Siegel: (1 - exp(-λτ)) / (λτ)\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Filthy.DSL.dns_loading2","page":"DSL & Templates","title":"Filthy.DSL.dns_loading2","text":"dns_loading2(λ, τ)\n\nCurvature loading for Dynamic Nelson-Siegel: (1 - exp(-λτ)) / (λτ) - exp(-λτ)\n\n\n\n\n\n","category":"function"},{"location":"tutorials/custom_models.html#Custom-Models","page":"Custom Models","title":"Custom Models","text":"This tutorial covers how to specify custom state space models using Filthy.jl's domain-specific language (DSL). You'll learn:\n\nThe custom_ssm function for specifying arbitrary models\nUsing FreeParam to mark estimated parameters\nMatrix helper functions for common patterns\nParameter-dependent matrices with MatrixExpr\nFull positive-definite covariance matrices with cov_free","category":"section"},{"location":"tutorials/custom_models.html#The-custom_ssm-Function","page":"Custom Models","title":"The custom_ssm Function","text":"The custom_ssm function lets you specify any linear state space model by providing the system matrices directly:\n\nusing Filthy\nusing LinearAlgebra\n\n# A simple local level model specified manually\nspec = custom_ssm(\n    Z = [1.0],                                    # Observation matrix\n    H = [FreeParam(:var_obs, init=100.0, lower=0.0)],  # Obs variance\n    T = [1.0],                                    # Transition matrix\n    R = [1.0],                                    # Selection matrix\n    Q = [FreeParam(:var_level, init=25.0, lower=0.0)], # State variance\n    a1 = [0.0],                                   # Initial state mean\n    P1 = [1e7],                                   # Initial state variance (diffuse)\n    name = :MyLocalLevel\n)\n\nprintln(\"Parameters: \", param_names(spec))\n# [:var_obs, :var_level]","category":"section"},{"location":"tutorials/custom_models.html#Key-Points","page":"Custom Models","title":"Key Points","text":"Fixed values: Use regular numbers for fixed matrix elements\nFree parameters: Use FreeParam(...) for parameters to be estimated\nBounds: Use lower=0.0 for variance parameters. TransformVariables.jl automatically applies appropriate transformations for constrained parameters.\nDimensions: Inferred automatically from the provided matrices","category":"section"},{"location":"tutorials/custom_models.html#The-FreeParam-Type","page":"Custom Models","title":"The FreeParam Type","text":"FreeParam marks a matrix element as an estimated parameter:\n\nFreeParam(name::Symbol;\n    init = 0.0,           # Initial value for optimization\n    lower = -Inf,         # Lower bound\n    upper = Inf           # Upper bound\n)","category":"section"},{"location":"tutorials/custom_models.html#Examples","page":"Custom Models","title":"Examples","text":"# Variance parameter (use lower=0.0 for positivity constraint)\nFreeParam(:var_obs, init=100.0, lower=0.0)\n\n# Bounded coefficient (e.g., AR coefficient)\nFreeParam(:ρ, init=0.8, lower=-0.99, upper=0.99)\n\n# Unbounded coefficient\nFreeParam(:β, init=0.0)","category":"section"},{"location":"tutorials/custom_models.html#The-@P-Macro","page":"Custom Models","title":"The @P Macro","text":"For quick parameter specification:\n\n@P(:σ, 1.0)  # Equivalent to FreeParam(:σ, init=1.0)","category":"section"},{"location":"tutorials/custom_models.html#Example:-Local-Linear-Trend-Model","page":"Custom Models","title":"Example: Local Linear Trend Model","text":"The local linear trend model has two states: level and slope.\n\nbeginaligned\ny_t = mu_t + varepsilon_t \nmu_t+1 = mu_t + nu_t + eta^mu_t \nnu_t+1 = nu_t + eta^nu_t\nendaligned\n\nspec = custom_ssm(\n    Z = [1.0 0.0],  # Only level is observed\n    H = [FreeParam(:var_obs, init=1.0, lower=0.0)],\n    T = [1.0 1.0;   # Level depends on previous level + slope\n         0.0 1.0],  # Slope is a random walk\n    R = Matrix(1.0I, 2, 2),  # Both states receive shocks\n    Q = [FreeParam(:var_level, init=0.01, lower=0.0)  0.0;\n         0.0  FreeParam(:var_slope, init=0.0001, lower=0.0)],\n    a1 = [0.0, 0.0],\n    P1 = 1e7 * Matrix(1.0I, 2, 2),\n    name = :LocalLinearTrend\n)\n\nprintln(\"States: \", spec.n_states)  # 2\nprintln(\"Parameters: \", param_names(spec))  # [:var_obs, :var_level, :var_slope]","category":"section"},{"location":"tutorials/custom_models.html#Matrix-Helper-Functions","page":"Custom Models","title":"Matrix Helper Functions","text":"Filthy.jl provides helper functions for common matrix patterns:","category":"section"},{"location":"tutorials/custom_models.html#Diagonal-Matrices-with-Free-Parameters","page":"Custom Models","title":"Diagonal Matrices with Free Parameters","text":"# Diagonal matrix with n free variance parameters (lower=0.0 by default)\nH = diag_free(3, :var_obs)\n# Creates parameters: var_obs_1, var_obs_2, var_obs_3\n\n# With custom initial value\nH = diag_free(3, :var_obs; init=2.0)","category":"section"},{"location":"tutorials/custom_models.html#Scalar-Matrices","page":"Custom Models","title":"Scalar Matrices","text":"# Scalar times identity\nH = scalar_free(3, :var_obs; init=1.0)\n# One parameter var_obs, applied to all diagonal elements","category":"section"},{"location":"tutorials/custom_models.html#Fixed-Diagonal-Matrices","page":"Custom Models","title":"Fixed Diagonal Matrices","text":"# Fixed diagonal values\nH = diag_fixed(3, [1.0, 2.0, 3.0])","category":"section"},{"location":"tutorials/custom_models.html#Identity-and-Zero-Matrices","page":"Custom Models","title":"Identity and Zero Matrices","text":"Z = identity_mat(3)     # 3×3 identity\nR = zeros_mat(3, 2)     # 3×2 zeros\nO = ones_mat(2, 2)      # 2×2 ones","category":"section"},{"location":"tutorials/custom_models.html#Selection-Matrix","page":"Custom Models","title":"Selection Matrix","text":"# Select specific states for observation\n# If n_states=4 and we observe states 1 and 3:\nZ = selection_mat([1, 3], 4)  # 2×4 matrix","category":"section"},{"location":"tutorials/custom_models.html#Companion-Matrix","page":"Custom Models","title":"Companion Matrix","text":"For VAR/ARMA models in companion form:\n\n# AR(2) companion matrix with free coefficients\nT = companion_mat(2, :φ; init=[0.5, 0.3])\n# Creates parameters φ_1, φ_2\n# Matrix: [φ_1  φ_2]\n#         [1    0  ]","category":"section"},{"location":"tutorials/custom_models.html#Lower-Triangular-Free","page":"Custom Models","title":"Lower Triangular Free","text":"# Lower triangular matrix with free parameters\nL = lower_triangular_free(3, :L)\n# Creates parameters for lower triangle: L_1_1, L_2_1, L_2_2, L_3_1, L_3_2, L_3_3","category":"section"},{"location":"tutorials/custom_models.html#Symmetric-Free","page":"Custom Models","title":"Symmetric Free","text":"# Symmetric matrix with free parameters\nS = symmetric_free(2, :S)\n# Creates parameters: S_1_1, S_2_1, S_2_2\n# The matrix is symmetric by construction","category":"section"},{"location":"tutorials/custom_models.html#Example:-Bivariate-VAR(1)","page":"Custom Models","title":"Example: Bivariate VAR(1)","text":"A bivariate VAR(1) model:\n\ny_t = Phi y_t-1 + varepsilon_t quad varepsilon_t sim N(0 Sigma)\n\nspec = custom_ssm(\n    Z = identity_mat(2),          # Observe both states\n    H = zeros_mat(2, 2),          # No measurement error (VAR is exact)\n    T = [FreeParam(:φ_11, init=0.5)  FreeParam(:φ_12, init=0.1);\n         FreeParam(:φ_21, init=0.1)  FreeParam(:φ_22, init=0.5)],\n    R = identity_mat(2),\n    Q = diag_free(2, :var_innov),  # Diagonal innovation covariance\n    a1 = [0.0, 0.0],\n    P1 = 1e4 * Matrix(1.0I, 2, 2),\n    name = :BivariateVAR1\n)","category":"section"},{"location":"tutorials/custom_models.html#Full-Covariance-Matrices-with-cov_free","page":"Custom Models","title":"Full Covariance Matrices with cov_free","text":"For models with correlated errors, use cov_free to specify a full positive-definite covariance matrix:\n\n# 3×3 positive definite covariance matrix\nQ = cov_free(3, :Q)\n\nThis uses the decomposition Sigma = D cdot textCorr cdot D where:\n\nD = textdiag(sigma_1 ldots sigma_n) contains standard deviations\ntextCorr is a correlation matrix (constructed via Cholesky factor)","category":"section"},{"location":"tutorials/custom_models.html#Parameters-Created","page":"Custom Models","title":"Parameters Created","text":"For cov_free(n, :prefix):\n\nn standard deviation parameters: prefix_σ_1, ..., prefix_σ_n\nn(n-1)/2 correlation parameters: prefix_corr_1, ...","category":"section"},{"location":"tutorials/custom_models.html#Example:-VAR-with-Correlated-Innovations","page":"Custom Models","title":"Example: VAR with Correlated Innovations","text":"spec = custom_ssm(\n    Z = identity_mat(2),\n    H = zeros_mat(2, 2),\n    T = [FreeParam(:φ_11, init=0.5)  FreeParam(:φ_12, init=0.0);\n         FreeParam(:φ_21, init=0.0)  FreeParam(:φ_22, init=0.5)],\n    R = identity_mat(2),\n    Q = cov_free(2, :Q),  # Full 2×2 covariance\n    a1 = [0.0, 0.0],\n    P1 = 1e4 * Matrix(1.0I, 2, 2),\n    name = :VAR1_Correlated\n)\n\nprintln(\"Parameters: \", param_names(spec))\n# [:φ_11, :φ_12, :φ_21, :φ_22, :Q_σ_1, :Q_σ_2, :Q_corr_1]","category":"section"},{"location":"tutorials/custom_models.html#Parameter-Dependent-Matrices-with-MatrixExpr","page":"Custom Models","title":"Parameter-Dependent Matrices with MatrixExpr","text":"For advanced models where matrix elements depend on parameters in complex ways (e.g., yield curve models), use MatrixExpr:\n\nstruct MatrixExpr\n    params::Vector{SSMParameter}  # Parameters used\n    data::NamedTuple              # Static data (e.g., maturities)\n    builder::Function             # Function to build the matrix\n    dims::Tuple{Int,Int}          # Matrix dimensions\nend","category":"section"},{"location":"tutorials/custom_models.html#Example:-Nelson-Siegel-Yield-Curve-Model","page":"Custom Models","title":"Example: Nelson-Siegel Yield Curve Model","text":"The Dynamic Nelson-Siegel model has loadings that depend on a decay parameter λ:\n\nZ_ij = begincases\n1  j=1 \nfrac1-e^-lambda tau_ilambda tau_i  j=2 \nfrac1-e^-lambda tau_ilambda tau_i - e^-lambda tau_i  j=3\nendcases\n\nusing Filthy\n\n# Builder function for DNS loadings\nfunction dns_builder(θ::Dict, data)\n    λ = θ[:λ]\n    τ = data.maturities\n    n = length(τ)\n    T = eltype(λ)\n\n    Z = zeros(T, n, 3)\n    for i in 1:n\n        Z[i, 1] = one(T)\n        Z[i, 2] = dns_loading1(λ, τ[i])\n        Z[i, 3] = dns_loading2(λ, τ[i])\n    end\n    return Z\nend\n\n# Create the MatrixExpr\nmaturities = [3, 6, 12, 24, 60, 120]  # in months\nZ_expr = MatrixExpr(\n    [SSMParameter(:λ, 0.001, 0.5, 0.0609)],  # name, lower, upper, init\n    (maturities = maturities,),\n    dns_builder,\n    (length(maturities), 3)\n)\n\n# Use in custom_ssm\nspec = custom_ssm(\n    Z = Z_expr,\n    H = diag_free(6, :var_obs),\n    T = diag_free(3, :φ; lower=-0.999, upper=0.999, init=0.9),\n    R = identity_mat(3),\n    Q = diag_free(3, :var_factor),\n    a1 = [0.0, 0.0, 0.0],\n    P1 = 1e4 * Matrix(1.0I, 3, 3),\n    name = :DynamicNelsonSiegel\n)","category":"section"},{"location":"tutorials/custom_models.html#Built-in-DNS-Helpers","page":"Custom Models","title":"Built-in DNS Helpers","text":"Filthy.jl provides convenience functions for DNS models:\n\n# Build DNS Z matrix directly\nZ = build_dns_loadings(maturities, :λ; λ_init=0.0609)\n\n# Or Svensson (4-factor) loadings\nZ = build_svensson_loadings(maturities, :λ1, :λ2; λ1_init=0.05, λ2_init=0.10)","category":"section"},{"location":"tutorials/custom_models.html#Putting-It-All-Together:-A-Complete-Example","page":"Custom Models","title":"Putting It All Together: A Complete Example","text":"Here's a complete workflow for a custom bivariate model:\n\nusing Filthy\nusing LinearAlgebra\nusing Random\n\nRandom.seed!(123)\n\n# Specify the model\nspec = custom_ssm(\n    Z = [1.0 0.0;    # First obs loads on first state\n         0.0 1.0],   # Second obs loads on second state\n    H = diag_free(2, :var_obs; init=0.5),\n    T = [FreeParam(:φ_1, init=0.8, lower=-0.99, upper=0.99)  0.0;\n         0.0  FreeParam(:φ_2, init=0.6, lower=-0.99, upper=0.99)],\n    R = identity_mat(2),\n    Q = cov_free(2, :Q; init_σ=1.0),\n    a1 = [0.0, 0.0],\n    P1 = 10.0 * Matrix(1.0I, 2, 2),\n    name = :BivariateAR1\n)\n\n# Check the specification\nprintln(\"Model: \", spec.name)\nprintln(\"States: \", spec.n_states)\nprintln(\"Parameters: \", param_names(spec))\nprintln(\"Number of parameters: \", n_params(spec))\n\n# Simulate some data\nT = 200\ny = randn(2, T)\n\n# Estimate parameters\nresult = optimize_ssm(spec, y)\nprintln(\"\\nEstimated parameters:\")\nfor (name, val) in pairs(result.θ)\n    println(\"  $name = $val\")\nend\n\n# Get smoothed states\nss = build_linear_state_space(spec, result.θ, y)\nfilt = kalman_filter_full(ss.p, y, ss.a1, ss.P1)\nsmooth = kalman_smoother(ss.p.Z, ss.p.T, filt.at, filt.Pt, filt.vt, filt.Ft)\nprintln(\"\\nSmoothed state at t=100: \", smooth.alpha[:, 100])","category":"section"},{"location":"tutorials/custom_models.html#Tips-and-Best-Practices","page":"Custom Models","title":"Tips and Best Practices","text":"Parameter naming: Use descriptive names with prefixes (e.g., var_obs, var_level) for clarity.\nInitial values: Good initial values help optimization converge. Use domain knowledge when possible.\nVariance parameters: Use lower=0.0 for variance parameters. TransformVariables.jl automatically applies asℝ₊ transformation to enforce positivity.\nBounds: Use bounds for constrained parameters (e.g., AR coefficients in (-1, 1) for stationarity).\nDiffuse initialization: Use large values in P1 (e.g., 1e7) for diffuse priors on initial states.\nDimension checks: custom_ssm validates dimensions automatically—let it catch errors early.","category":"section"},{"location":"tutorials/custom_models.html#Next-Steps","page":"Custom Models","title":"Next Steps","text":"Learn about Parameter Transformations for understanding how bounds are handled\nLearn about Dynamic Factor Models for multivariate factor analysis\nSee the Core Functions and DSL & Templates for complete API documentation","category":"section"},{"location":"api/matrix_helpers.html#Matrix-Helpers","page":"Matrix Helpers","title":"Matrix Helpers","text":"This page documents helper functions for constructing state space matrices.","category":"section"},{"location":"api/matrix_helpers.html#Diagonal-Matrices","page":"Matrix Helpers","title":"Diagonal Matrices","text":"","category":"section"},{"location":"api/matrix_helpers.html#Identity-and-Zero-Matrices","page":"Matrix Helpers","title":"Identity and Zero Matrices","text":"","category":"section"},{"location":"api/matrix_helpers.html#Covariance-Matrices","page":"Matrix Helpers","title":"Covariance Matrices","text":"","category":"section"},{"location":"api/matrix_helpers.html#Structured-Matrices","page":"Matrix Helpers","title":"Structured Matrices","text":"","category":"section"},{"location":"api/matrix_helpers.html#Selection-and-Companion-Matrices","page":"Matrix Helpers","title":"Selection and Companion Matrices","text":"","category":"section"},{"location":"api/matrix_helpers.html#Filthy.DSL.diag_free","page":"Matrix Helpers","title":"Filthy.DSL.diag_free","text":"diag_free(names; init=1.0, lower=0.0, upper=Inf)\ndiag_free(n, prefix; init=1.0, lower=0.0, upper=Inf)\n\nCreate a diagonal matrix with free parameters on the diagonal.\n\nArguments\n\nnames: Vector of parameter names, e.g., [:var_level, :var_slope]\nOR n, prefix: Number of elements and prefix, creates :prefix_1, :prefix_2, etc.\ninit: Initial value(s) - scalar or vector (variance values)\nlower, upper: Bounds - scalar or vector (lower=0.0 triggers asℝ₊ transform)\n\nExamples\n\n# 2×2 diagonal with named parameters (variance values)\nQ = diag_free([:var_level, :var_slope], init=[0.01, 0.0001])\n\n# 3×3 diagonal with auto-generated names\nQ = diag_free(3, :var, init=1.0)  # Creates :var_1, :var_2, :var_3\n\n# 1×1 scalar variance\nH = diag_free([:var_obs], init=100.0)\n# Or equivalently:\nH = scalar_free(:var_obs, init=100.0)\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Filthy.DSL.scalar_free","page":"Matrix Helpers","title":"Filthy.DSL.scalar_free","text":"scalar_free(name; init=1.0, lower=0.0, upper=Inf)\n\nCreate a 1×1 matrix with a single free parameter. Convenience wrapper for diag_free.\n\nExample\n\nH = scalar_free(:var_obs, init=225.0)\n# Equivalent to: H = [FreeParam(:var_obs, init=225.0, lower=0.0)]\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Filthy.DSL.diag_fixed","page":"Matrix Helpers","title":"Filthy.DSL.diag_fixed","text":"diag_fixed(values)\n\nCreate a diagonal matrix with fixed values on the diagonal.\n\nExample\n\nQ = diag_fixed([0.1, 0.01, 0.001])  # Fixed 3×3 diagonal\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Filthy.DSL.identity_mat","page":"Matrix Helpers","title":"Filthy.DSL.identity_mat","text":"identity_mat(n)\n\nCreate an n×n identity matrix (fixed values).\n\nExample\n\nR = identity_mat(3)  # 3×3 identity\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Filthy.DSL.zeros_mat","page":"Matrix Helpers","title":"Filthy.DSL.zeros_mat","text":"zeros_mat(m, n=m)\n\nCreate an m×n zero matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Filthy.DSL.ones_mat","page":"Matrix Helpers","title":"Filthy.DSL.ones_mat","text":"ones_mat(m, n=m)\n\nCreate an m×n matrix of ones.\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Filthy.DSL.cov_free","page":"Matrix Helpers","title":"Filthy.DSL.cov_free","text":"cov_free(n, prefix; init_σ=1.0)\n\nCreate a full n×n positive definite covariance matrix specification.\n\nUses Σ = D * Corr * D decomposition where:\n\nD = Diagonal(σ) with n standard deviation parameters (positive, named prefix_σ_1, etc.)\nCorr = L'L from corr_cholesky_factor(n) with n(n-1)/2 correlation parameters (unconstrained)\n\nTotal parameters: n + n(n-1)/2 = n(n+1)/2\n\nArguments\n\nn::Int: Matrix dimension\nprefix::Symbol: Prefix for parameter names\ninit_σ::Float64=1.0: Initial value for standard deviation parameters\n\nExample\n\n# 2×2 covariance with 3 parameters: Q_σ_1, Q_σ_2, Q_corr_1\nQ = cov_free(2, :Q)\n\n# 3×3 covariance with 6 parameters\nQ = cov_free(3, :Q, init_σ=0.5)\n\nSee also\n\ndiag_free: For diagonal covariance matrices (no correlation)\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Filthy.DSL.CovFree","page":"Matrix Helpers","title":"Filthy.DSL.CovFree","text":"CovFree\n\nMarker type for a full positive definite covariance matrix with free parameters.\n\nUsed in custom_ssm to specify that a covariance matrix (Q or H) should be parameterized as Σ = D * Corr * D where:\n\nD = Diagonal(σ) with n positive standard deviation parameters\nCorr = L'L where L comes from corr_cholesky_factor(n) with n(n-1)/2 correlation parameters\n\nTotal parameters: n + n(n-1)/2 = n(n+1)/2\n\nFields\n\nn::Int: Matrix dimension\nprefix::Symbol: Prefix for parameter names (e.g., :Q creates :Qσ1, :Qcorr1, etc.)\ninit_σ::Float64: Initial value for standard deviation parameters (default: 1.0)\n\nExample\n\nQ = cov_free(3, :Q)  # Creates 3×3 covariance with 6 parameters\n\n\n\n\n\n","category":"type"},{"location":"api/matrix_helpers.html#Filthy.DSL.lower_triangular_free","page":"Matrix Helpers","title":"Filthy.DSL.lower_triangular_free","text":"lower_triangular_free(n, prefix; init=0.0, lower=-Inf, upper=Inf)\n\nCreate a lower triangular matrix with free parameters below and on the diagonal. Useful for Cholesky factors.\n\nExample\n\n# Cholesky factor of covariance matrix\nL = lower_triangular_free(2, :L)\n# Creates:\n# [FreeParam(:L_1_1)  0.0            ]\n# [FreeParam(:L_2_1)  FreeParam(:L_2_2)]\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Filthy.DSL.symmetric_free","page":"Matrix Helpers","title":"Filthy.DSL.symmetric_free","text":"symmetric_free(n, prefix; init_diag=1.0, init_offdiag=0.0,\n               lower_diag=0.0, lower_offdiag=-Inf,\n               upper_diag=Inf, upper_offdiag=Inf)\n\nCreate a symmetric matrix with free parameters. Diagonal and off-diagonal elements can have different settings.\n\nExample\n\n# Symmetric covariance with variance on diagonal, covariances off-diagonal\nΣ = symmetric_free(2, :Σ, init_diag=1.0, init_offdiag=0.1)\n# Creates a symmetric matrix where Σ[1,2] = Σ[2,1] (same parameter)\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Filthy.DSL.selection_mat","page":"Matrix Helpers","title":"Filthy.DSL.selection_mat","text":"selection_mat(m, r)\n\nCreate a selection matrix R of size m×r. Common patterns:\n\nselection_mat(m, m) → Identity (all states have shocks)\nselection_mat(m, r) where r < m → First r states have shocks\n\nExample\n\n# 3 states, 2 shocks (first 2 states)\nR = selection_mat(3, 2)\n# [1 0]\n# [0 1]\n# [0 0]\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Filthy.DSL.companion_mat","page":"Matrix Helpers","title":"Filthy.DSL.companion_mat","text":"companion_mat(n)\n\nCreate an n×n companion matrix structure (for AR(n) models). Returns a matrix with 1s on the superdiagonal and FreeParams in the first column.\n\nExample\n\nT = companion_mat(2, :φ)  # AR(2) transition\n# [FreeParam(:φ_1)  1.0]\n# [FreeParam(:φ_2)  0.0]\n\n\n\n\n\n","category":"function"},{"location":"tutorials/dynamic_factor.html#Dynamic-Factor-Models","page":"Dynamic Factor Models","title":"Dynamic Factor Models","text":"This tutorial covers dynamic factor models (DFMs) in Filthy.jl. DFMs are widely used in macroeconomics, finance, and other fields to extract common latent factors from a large panel of observed time series.\n\nFilthy.jl provides two approaches:\n\nDSL approach (dynamic_factor template): Flexible specification with MLE estimation\nHigh-level API (DynamicFactorModel): Specialized type with EM estimation for large panels","category":"section"},{"location":"tutorials/dynamic_factor.html#Model-Overview","page":"Dynamic Factor Models","title":"Model Overview","text":"","category":"section"},{"location":"tutorials/dynamic_factor.html#Mathematical-Formulation","page":"Dynamic Factor Models","title":"Mathematical Formulation","text":"The general dynamic factor model is:\n\nbeginaligned\nX_t = Lambda(L) f_t + e_t \nf_t = Psi(L) f_t-1 + eta_t quad eta_t sim N(0 Sigma_eta) \ne_it = delta(L) e_it-1 + v_it quad v_it sim N(0 sigma^2_i)\nendaligned\n\nWhere:\n\nX_t is the N times 1 vector of observables\nf_t is the k times 1 vector of latent factors\nLambda(L) = Lambda_0 + Lambda_1 L + cdots are dynamic factor loadings\nPsi(L) is the factor VAR polynomial\ndelta(L) captures AR dynamics in idiosyncratic errors","category":"section"},{"location":"tutorials/dynamic_factor.html#High-Level-API:-DynamicFactorModel","page":"Dynamic Factor Models","title":"High-Level API: DynamicFactorModel","text":"For large-scale applications, use the DynamicFactorModel type with EM estimation:\n\nusing Filthy\ninclude(\"src/inplace.jl\")\n\n# Create model: 100 observables, 6 factors, 200 time periods\nmodel = DynamicFactorModel(\n    100,              # N: number of observables\n    6,                # k: number of factors\n    200;              # n: number of time periods\n    factor_lags = 3,  # VAR(3) factor dynamics\n    error_lags = 1    # AR(1) idiosyncratic errors\n)\n\n# Fit with EM algorithm\nfit!(EM(), model, y; maxiter=500, tol=1e-6, verbose=true)\n\n# Check convergence\nprintln(\"Converged: \", isconverged(model))\nprintln(\"Log-likelihood: \", loglikelihood(model))\n\n# Access results\nf = factors(model)              # k × n smoothed factors\nΛ = loadings(model)             # Factor loadings [Λ₀, Λ₁, ...]\nΦ = var_coefficients(model)     # VAR coefficients [Φ₁, ..., Φ_q]\nδ = ar_coefficients(model)      # AR error coefficients\nΣ_η = innovation_cov(model)     # Factor innovation covariance\nσ²_v = idiosyncratic_variances(model)  # Idiosyncratic variances","category":"section"},{"location":"tutorials/dynamic_factor.html#Model-Configurations","page":"Dynamic Factor Models","title":"Model Configurations","text":"Configuration loading_lags factor_lags error_lags Description\nSimple DFM 0 q 0 Static loadings, VAR(q) factors\nAR errors 0 q r Static loadings, AR(r) errors\nDynamic loadings p q 0 Dynamic loadings with p lags\nFull DFM p q r Dynamic loadings + AR errors","category":"section"},{"location":"tutorials/dynamic_factor.html#State-Dimension","page":"Dynamic Factor Models","title":"State Dimension","text":"The state vector stacks current and lagged factors (and errors if r > 0):\n\nalpha_t = f_t f_t-1 ldots f_t-s+1 e_t e_t-1 ldots e_t-r+1\n\nwhere s = max(q p+1). State dimension: m = k times s + N times r.","category":"section"},{"location":"tutorials/dynamic_factor.html#DSL-Approach:-dynamic_factor-Template","page":"Dynamic Factor Models","title":"DSL Approach: dynamic_factor Template","text":"For smaller models or when you need MLE with AD:\n\nusing Filthy\n\n# 8 observables, 2 factors, AR(1) dynamics\nspec = dynamic_factor(8, 2)\n\nprintln(\"Parameters: \", param_names(spec))\nprintln(\"Number of states: \", spec.n_states)\n\n# Estimate via MLE\nresult = optimize_ssm(spec, y)","category":"section"},{"location":"tutorials/dynamic_factor.html#Identification","page":"Dynamic Factor Models","title":"Identification","text":"For identification, the first k rows of Lambda_0 form a lower triangular structure:\n\nlambda_ii = 1 for i leq k\nlambda_ij = 0 for i  j leq k\n\nLambda_0 = beginbmatrix\n1  0 \nlambda_21  1 \nlambda_31  lambda_32 \nvdots  vdots\nendbmatrix","category":"section"},{"location":"tutorials/dynamic_factor.html#Multiple-Lags","page":"Dynamic Factor Models","title":"Multiple Lags","text":"# 5 observables, 2 factors, VAR(3) factor dynamics, 1 lag in loadings\nspec = dynamic_factor(5, 2; factor_lags=3, obs_lags=1)\n\nprintln(\"States: \", spec.n_states)  # 6 = 2 factors × max(3, 1+1)","category":"section"},{"location":"tutorials/dynamic_factor.html#Complete-Example:-Macroeconomic-Factor-Model","page":"Dynamic Factor Models","title":"Complete Example: Macroeconomic Factor Model","text":"","category":"section"},{"location":"tutorials/dynamic_factor.html#Using-DynamicFactorModel-(Recommended-for-Large-Panels)","page":"Dynamic Factor Models","title":"Using DynamicFactorModel (Recommended for Large Panels)","text":"using Filthy\nusing LinearAlgebra\nusing Statistics\ninclude(\"src/inplace.jl\")\n\n# Load FRED-QD or similar macro data\n# y should be N × n (observables × time)\n\nN, n = 100, 200  # 100 series, 200 quarters\nk = 6            # 6 factors\n\n# Create and fit model\nmodel = DynamicFactorModel(N, k, n;\n    factor_lags = 3,   # VAR(3)\n    error_lags = 1     # AR(1) errors\n)\n\nfit!(EM(), model, y; maxiter=200, verbose=true)\n\n# Variance decomposition\nf = factors(model)\nΛ = loadings(model)[1]  # Contemporaneous loadings\nfactor_cov = (f * f') / n\n\ncommunalities = zeros(N)\nfor i in 1:N\n    λ_i = Λ[i, :]\n    communalities[i] = λ_i' * factor_cov * λ_i\nend\n\nidio_var = idiosyncratic_variances(model)\nvar_explained = communalities ./ (communalities .+ idio_var)\n\nprintln(\"Average variance explained: \", mean(var_explained) * 100, \"%\")","category":"section"},{"location":"tutorials/dynamic_factor.html#Using-DSL-(For-Smaller-Models-with-MLE)","page":"Dynamic Factor Models","title":"Using DSL (For Smaller Models with MLE)","text":"using Filthy\nusing Random\nusing LinearAlgebra\n\nRandom.seed!(42)\n\n# Simulate data from a 2-factor model\nn_obs, T = 8, 200\n\ntrue_loadings = [1.0 0.0; 0.5 1.0; 0.8 0.3; 0.3 0.7;\n                 0.6 0.4; 0.4 0.6; 0.7 0.2; 0.2 0.8]\ntrue_φ = [0.9, 0.7]\ntrue_σ_factor = [0.5, 0.4]\ntrue_σ_obs = fill(0.3, n_obs)\n\n# Simulate factors\nf = zeros(2, T)\nfor t in 2:T\n    f[:, t] = Diagonal(true_φ) * f[:, t-1] + Diagonal(true_σ_factor) * randn(2)\nend\n\n# Generate observations\ny = zeros(n_obs, T)\nfor t in 1:T\n    y[:, t] = true_loadings * f[:, t] + Diagonal(true_σ_obs) * randn(n_obs)\nend\n\n# Specify and estimate model\nspec = dynamic_factor(n_obs, 2;\n    loadings_init = 0.5,\n    ar_init = 0.7,\n    σ_obs_init = 0.5,\n    σ_factor_init = 0.5\n)\n\nresult = optimize_ssm(spec, y)\n\nprintln(\"Estimated AR coefficients:\")\nprintln(\"  φ_1_1 = \", round(result.θ.φ_1_1, digits=3), \" (true: 0.9)\")\nprintln(\"  φ_2_1 = \", round(result.θ.φ_2_1, digits=3), \" (true: 0.7)\")\n\n# Extract smoothed factors\nss = build_linear_state_space(spec, result.θ, y)\nfilt = kalman_filter_full(ss.p, y, ss.a1, ss.P1)\nsmooth = kalman_smoother(ss.p.Z, ss.p.T, filt.at, filt.Pt, filt.vt, filt.Ft)\n\nprintln(\"\\nCorrelation with true factors:\")\nprintln(\"  Factor 1: \", round(cor(smooth.alpha[1, :], f[1, :]), digits=3))\nprintln(\"  Factor 2: \", round(cor(smooth.alpha[2, :], f[2, :]), digits=3))","category":"section"},{"location":"tutorials/dynamic_factor.html#Forecasting","page":"Dynamic Factor Models","title":"Forecasting","text":"# Forecast h steps ahead\nfc = forecast(model, 12)  # 12-step forecast\n\n# Access forecast components\nfc.obs_mean      # N × h forecasted observations\nfc.obs_cov       # N × N × h forecast covariances\nfc.factor_mean   # k × h forecasted factors\nfc.factor_cov    # k × k × h factor forecast covariances","category":"section"},{"location":"tutorials/dynamic_factor.html#Tips-and-Best-Practices","page":"Dynamic Factor Models","title":"Tips and Best Practices","text":"Number of factors: Start with fewer factors. Use information criteria or scree plots to select k.\nInitialization: The EM algorithm uses PCA for initialization. For MLE, good initial values help.\nLarge panels: Use DynamicFactorModel with EM for N > 20. The in-place implementation is memory-efficient.\nMissing data: Both approaches handle missing values (NaN) automatically.\nAR errors: Include AR errors (error_lags > 0) when idiosyncratic components are persistent.\nDynamic loadings: Use loading_lags > 0 when factors have delayed effects on observables.\nConvergence: Check isconverged(model) and examine the log-likelihood history.","category":"section"},{"location":"tutorials/dynamic_factor.html#Function-Reference","page":"Dynamic Factor Models","title":"Function Reference","text":"","category":"section"},{"location":"tutorials/dynamic_factor.html#DynamicFactorModel-Constructor","page":"Dynamic Factor Models","title":"DynamicFactorModel Constructor","text":"DynamicFactorModel(n_obs, n_factors, n_times;\n    loading_lags = 0,   # Lags in λ(L), 0 = static loadings\n    factor_lags = 1,    # Lags in factor VAR\n    error_lags = 0,     # Lags in AR errors, 0 = white noise\n    T = Float64         # Element type\n)","category":"section"},{"location":"tutorials/dynamic_factor.html#dynamic_factor-Template","page":"Dynamic Factor Models","title":"dynamic_factor Template","text":"dynamic_factor(n_obs, n_factors;\n    factor_lags = 1,           # AR lags in factor dynamics\n    obs_lags = 0,              # Lagged factor loadings\n    correlated_errors = false, # Full H matrix if true\n    loadings_init = 0.5,       # Initial loading values\n    ar_init = 0.5,             # Initial AR coefficient\n    σ_obs_init = 1.0,          # Initial obs error std dev\n    σ_factor_init = 1.0,       # Initial factor std dev\n    diffuse = true             # Diffuse initialization\n)","category":"section"},{"location":"tutorials/dynamic_factor.html#Next-Steps","page":"Dynamic Factor Models","title":"Next Steps","text":"Learn about Custom Models for more flexible specifications\nCheck the Core Functions API reference","category":"section"},{"location":"tutorials/getting_started.html#Getting-Started","page":"Getting Started","title":"Getting Started","text":"This tutorial introduces the basic workflow for working with state space models in Filthy.jl. We'll cover:\n\nCreating a model specification using templates\nComputing the log-likelihood\nRunning the Kalman filter\nRunning the Kalman smoother\nEstimating parameters via maximum likelihood","category":"section"},{"location":"tutorials/getting_started.html#The-Local-Level-Model","page":"Getting Started","title":"The Local Level Model","text":"We'll use the classic local level model (also known as the random walk plus noise model) as our running example. This model decomposes a time series into a slowly-evolving level plus observation noise:\n\nbeginaligned\ny_t = mu_t + varepsilon_t quad varepsilon_t sim N(0 sigma^2_varepsilon) \nmu_t+1 = mu_t + eta_t quad eta_t sim N(0 sigma^2_eta)\nendaligned\n\nIn state space form:\n\nZ = 1 (the observation loads directly on the state)\nH = sigma^2_varepsilon (observation variance)\nT = 1 (random walk dynamics)\nR = 1 (state receives the shock directly)\nQ = sigma^2_eta (state innovation variance)","category":"section"},{"location":"tutorials/getting_started.html#Creating-a-Model-Specification","page":"Getting Started","title":"Creating a Model Specification","text":"using Filthy\n\n# Create a local level model with initial parameter values\nspec = local_level()\n\n# Inspect the specification\nprintln(\"Parameters: \", param_names(spec))\nprintln(\"Initial values: \", initial_values(spec))\n\nOutput:\n\nParameters: (:var_obs, :var_level)\nInitial values: [225.0, 100.0]","category":"section"},{"location":"tutorials/getting_started.html#Loading-Data","page":"Getting Started","title":"Loading Data","text":"Let's work with the classic Nile River dataset:\n\nusing DelimitedFiles\n\n# Load the Nile data\nnile = readdlm(\"test/Nile.csv\", ',', Float64)\ny = nile'  # Convert to 1×n matrix (p × T format)\n\nprintln(\"Number of observations: \", size(y, 2))","category":"section"},{"location":"tutorials/getting_started.html#Computing-the-Log-Likelihood","page":"Getting Started","title":"Computing the Log-Likelihood","text":"Given a model specification and parameters, we can compute the log-likelihood:\n\n# Get initial parameter values\nθ = initial_values(spec)\n\n# Build the state space components\nss = build_linear_state_space(spec, θ, y)\n\n# Compute log-likelihood\nll = kalman_loglik(ss.p, y, ss.a1, ss.P1)\nprintln(\"Log-likelihood at initial values: \", ll)\n\nThe build_linear_state_space function returns a named tuple with:\n\np: A KFParms struct containing the system matrices (Z, H, T, R, Q)\na1: Initial state mean vector\nP1: Initial state covariance matrix","category":"section"},{"location":"tutorials/getting_started.html#Kalman-Filter","page":"Getting Started","title":"Kalman Filter","text":"The Kalman filter computes the sequence of filtered state estimates and their covariances:\n\n# Run the full Kalman filter\nfilt = kalman_filter_full(ss.p, y, ss.a1, ss.P1)\n\n# Access results\nat = filt.at       # Predicted state means E[α_t|y_{1:t-1}] (m × n)\nPt = filt.Pt       # Predicted state covariances (m × m × n)\natt = filt.att     # Filtered state means E[α_t|y_{1:t}] (m × n)\nPtt = filt.Ptt     # Filtered state covariances (m × m × n)\nvt = filt.vt       # Prediction errors (p × n)\nFt = filt.Ft       # Prediction error variances (p × p × n)\nKt = filt.Kt       # Kalman gains (m × p × n)\nll = filt.loglik   # Log-likelihood\n\nprintln(\"Final filtered state: \", att[:, end])\nprintln(\"Log-likelihood: \", ll)","category":"section"},{"location":"tutorials/getting_started.html#Filter-Output-Structure","page":"Getting Started","title":"Filter Output Structure","text":"The KalmanFilterResult contains:\n\nat[:, t]: State estimate at time t given observations up to t-1 (i.e., a_tt-1)\natt[:, t]: State estimate at time t given observations up to t (i.e., a_tt)\nPt[:, :, t], Ptt[:, :, t]: Corresponding covariance matrices\nvt[:, t]: Innovation (prediction error) at time t\nFt[:, :, t]: Innovation covariance at time t\nKt[:, :, t]: Kalman gain at time t","category":"section"},{"location":"tutorials/getting_started.html#Kalman-Smoother","page":"Getting Started","title":"Kalman Smoother","text":"The Kalman smoother computes smoothed state estimates using all available observations:\n\n# Run the Kalman smoother (uses filter output)\nsmooth = kalman_smoother(ss.p.Z, ss.p.T, filt.at, filt.Pt, filt.vt, filt.Ft)\n\n# Access results\nalpha = smooth.alpha  # Smoothed state means E[α_t|y_{1:n}] (m × n)\nV = smooth.V          # Smoothed state covariances (m × m × n)\n\nprintln(\"Smoothed state at t=50: \", alpha[:, 50])","category":"section"},{"location":"tutorials/getting_started.html#With-Cross-Lag-Covariances","page":"Getting Started","title":"With Cross-Lag Covariances","text":"For EM algorithm applications, you may need cross-lag covariances:\n\nsmooth = kalman_smoother(ss.p.Z, ss.p.T, filt.at, filt.Pt, filt.vt, filt.Ft;\n                          compute_crosscov=true)\n\n# Additional output\nP_crosslag = smooth.P_crosslag  # Cov[α_{t+1}, α_t|y_{1:n}] (m × m × n-1)","category":"section"},{"location":"tutorials/getting_started.html#Parameter-Estimation","page":"Getting Started","title":"Parameter Estimation","text":"Filthy.jl integrates with Optimization.jl for maximum likelihood estimation:\n\n# Estimate parameters via MLE\nresult = optimize_ssm(spec, y)\n\n# Access results\nθ_mle = result.θ           # Estimated parameters (NamedTuple)\nloglik = result.loglik     # Maximized log-likelihood\n\nprintln(\"Estimated var_obs: \", θ_mle.var_obs)\nprintln(\"Estimated var_level: \", θ_mle.var_level)\nprintln(\"Log-likelihood: \", loglik)","category":"section"},{"location":"tutorials/getting_started.html#EM-Algorithm","page":"Getting Started","title":"EM Algorithm","text":"For models with only variance parameters, the EM algorithm can be faster:\n\n# Estimate via EM\nem_result = em_ssm(spec, y; maxiter=200, verbose=true)\n\nprintln(\"Converged: \", em_result.converged)\nprintln(\"Iterations: \", em_result.iterations)\nprintln(\"Parameters: \", em_result.θ)","category":"section"},{"location":"tutorials/getting_started.html#Filter-and-Smooth-with-Estimated-Parameters","page":"Getting Started","title":"Filter and Smooth with Estimated Parameters","text":"# Build state space with MLE parameters\nss_mle = build_linear_state_space(spec, result.θ, y)\n\n# Run filter and smoother\nfilt = kalman_filter_full(ss_mle.p, y, ss_mle.a1, ss_mle.P1)\nsmooth = kalman_smoother(ss_mle.p.Z, ss_mle.p.T, filt.at, filt.Pt, filt.vt, filt.Ft)\n\n# Compare filtered vs smoothed estimates\nprintln(\"Filtered state std: \", std(filt.att[1, :]))\nprintln(\"Smoothed state std: \", std(smooth.alpha[1, :]))\n\nThe smoothed estimates are generally more accurate than filtered estimates because they use information from the entire sample.","category":"section"},{"location":"tutorials/getting_started.html#Other-Pre-Built-Templates","page":"Getting Started","title":"Other Pre-Built Templates","text":"Filthy.jl provides several other templates:","category":"section"},{"location":"tutorials/getting_started.html#Local-Linear-Trend","page":"Getting Started","title":"Local Linear Trend","text":"# Level + slope model\nspec = local_linear_trend()\n# Parameters: (:var_obs, :var_level, :var_slope)","category":"section"},{"location":"tutorials/getting_started.html#AR(1)-Process","page":"Getting Started","title":"AR(1) Process","text":"# AR(1) with measurement noise\nspec = ar1(ρ_init=0.9)\n# Parameters: (:ρ, :var_obs, :var_state)","category":"section"},{"location":"tutorials/getting_started.html#ARMA(p,-q)","page":"Getting Started","title":"ARMA(p, q)","text":"# ARMA(2, 1) model\nspec = arma(2, 1)\n# Parameters: (:ar_1, :ar_2, :ma_1, :var)","category":"section"},{"location":"tutorials/getting_started.html#Dynamic-Nelson-Siegel","page":"Getting Started","title":"Dynamic Nelson-Siegel","text":"# DNS yield curve model\nmaturities = [3, 6, 12, 24, 36, 60, 84, 120]\nspec = dns_model(maturities)","category":"section"},{"location":"tutorials/getting_started.html#Working-with-Missing-Data","page":"Getting Started","title":"Working with Missing Data","text":"Filthy.jl handles missing observations automatically using NaN:\n\n# Create data with missing values\ny_missing = copy(y)\ny_missing[1, 20:25] .= NaN  # Mark as missing\n\n# Filter and smooth work normally\nfilt = kalman_filter_full(ss.p, y_missing, ss.a1, ss.P1)\nsmooth = kalman_smoother(ss.p.Z, ss.p.T, filt.at, filt.Pt, filt.vt, filt.Ft;\n                          missing_mask=filt.missing_mask)\n\n# The smoother will interpolate through missing periods","category":"section"},{"location":"tutorials/getting_started.html#Next-Steps","page":"Getting Started","title":"Next Steps","text":"Learn how to specify Custom Models using the DSL\nUnderstand Parameter Transformations for constrained optimization\nExplore Dynamic Factor Models for multivariate analysis\nSee the Core Functions and DSL & Templates for complete API documentation","category":"section"},{"location":"index.html#Filthy.jl","page":"Home","title":"Filthy.jl","text":"A Julia package for Linear State Space Models with Kalman filtering and smoothing.","category":"section"},{"location":"index.html#Overview","page":"Home","title":"Overview","text":"Filthy.jl provides a comprehensive toolkit for working with linear Gaussian state space models. It combines high-performance Kalman filtering algorithms with an ergonomic domain-specific language (DSL) for model specification.","category":"section"},{"location":"index.html#Key-Features","page":"Home","title":"Key Features","text":"High-Performance Kalman Filter: AD-compatible implementations supporting both scalar and matrix observations\nKalman Smoother: Full backward smoothing with disturbance smoothing\nErgonomic DSL: Specify models using intuitive matrix notation with automatic parameter handling\nPre-built Templates: Common models like local level, local linear trend, AR(1), ARMA, and dynamic factor models\nOptimization Integration: Seamless integration with Optimization.jl for maximum likelihood estimation\nBayesian Support: Prior specification and LogDensityProblems.jl interface for MCMC sampling\nStaticArrays Support: Automatic conversion to StaticArrays for small models\nMissing Data Handling: Native support for missing observations","category":"section"},{"location":"index.html#State-Space-Model-Formulation","page":"Home","title":"State Space Model Formulation","text":"Filthy.jl implements the standard linear Gaussian state space model:\n\nbeginaligned\ny_t = Z alpha_t + varepsilon_t quad varepsilon_t sim N(0 H) \nalpha_t+1 = T alpha_t + R eta_t quad eta_t sim N(0 Q)\nendaligned\n\nwhere:\n\ny_t is the p times 1 observation vector\nalpha_t is the m times 1 state vector\nZ is the p times m observation matrix\nH is the p times p observation error covariance\nT is the m times m transition matrix\nR is the m times r selection matrix\nQ is the r times r state innovation covariance","category":"section"},{"location":"index.html#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(\"Filthy\")","category":"section"},{"location":"index.html#Quick-Start","page":"Home","title":"Quick Start","text":"using Filthy\n\n# Create a local level model specification\nspec = local_level(var_obs_init=225.0, var_level_init=100.0)\n\n# Your data (p × T matrix, where p = number of series, T = time periods)\ny = randn(1, 100)\n\n# Estimate parameters via maximum likelihood\nresult = optimize_ssm(spec, y)\n\n# Access estimated parameters\nprintln(\"Estimated parameters: \", result.θ)\nprintln(\"Log-likelihood: \", result.loglik)\n\n# Get filtered and smoothed states\nss = build_linear_state_space(spec, result.θ, y)\nfilt = kalman_filter_full(ss.p, y, ss.a1, ss.P1)\nsmooth = kalman_smoother(ss.p.Z, ss.p.T, filt.at, filt.Pt, filt.vt, filt.Ft)","category":"section"},{"location":"index.html#Documentation-Structure","page":"Home","title":"Documentation Structure","text":"Getting Started: Basic tutorial covering Kalman filtering, smoothing, and parameter estimation\nCustom Models: Advanced tutorial on specifying custom state space models\nDynamic Factor Models: Tutorial on dynamic factor model specification\nCore Functions: Kalman filter and smoother API\nDSL & Templates: Model specification API","category":"section"},{"location":"index.html#Related-Packages","page":"Home","title":"Related Packages","text":"StateSpaceModels.jl: Alternative state space modeling package\nOptimization.jl: Unified optimization interface used by Filthy.jl\nTransformVariables.jl: Parameter transformations used internally\nLogDensityProblems.jl: Interface for Bayesian inference","category":"section"},{"location":"index.html#License","page":"Home","title":"License","text":"Filthy.jl is released under the MIT License.","category":"section"}]
}
