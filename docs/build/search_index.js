var documenterSearchIndex = {"docs":
[{"location":"tutorials/visualization.html#Visualization","page":"Visualization","title":"Visualization","text":"Siphon.jl provides plotting recipes via RecipesBase.jl for visualizing Kalman filter and smoother outputs with confidence bands. These recipes work with any Plots.jl backend.","category":"section"},{"location":"tutorials/visualization.html#Quick-Start-(Recommended:-KalmanWorkspace)","page":"Visualization","title":"Quick Start (Recommended: KalmanWorkspace)","text":"The recommended approach uses KalmanWorkspace for zero-allocation, in-place computation:\n\nusing Plots\nusing Siphon\n\n# Create workspace and run filter + smoother\nws = KalmanWorkspace(Z, H, T, R, Q, a1, P1, n)\nkalman_filter!(ws, y)\nkalman_smoother!(ws)\n\n# Plot smoothed states with 95% confidence bands\nplot(ws)","category":"section"},{"location":"tutorials/visualization.html#Plotting-with-KalmanWorkspace","page":"Visualization","title":"Plotting with KalmanWorkspace","text":"The KalmanWorkspace recipe is the primary plotting interface. It supports filtered, predicted, and smoothed states:\n\n# Create and run\nws = KalmanWorkspace(Z, H, T, R, Q, a1, P1, n)\nkalman_filter!(ws, y)\nkalman_smoother!(ws)\n\n# Plot smoothed states (default)\nplot(ws)\nplot(ws, what=:smoothed)\n\n# Plot filtered or predicted states\nplot(ws, what=:filtered)\nplot(ws, what=:predicted)\n\n# Customize\nplot(ws, vars=1, level=0.90)     # Single state with 90% CI\nplot(ws, vars=[1,3])             # Multiple states\nplot(ws, band=false)             # No confidence bands\nplot(ws, time=dates)             # Custom time axis","category":"section"},{"location":"tutorials/visualization.html#Keywords","page":"Visualization","title":"Keywords","text":"Keyword Default Description\nvars :all Variables to plot: Int, Vector, Range, or :all\nlevel 0.95 Confidence level for bands (0 < level < 1)\nwhat :smoothed State type: :smoothed, :filtered, or :predicted\nband true Whether to show confidence bands\ntime nothing Custom time axis (default: 1:n)","category":"section"},{"location":"tutorials/visualization.html#Functional-API-(Alternative)","page":"Visualization","title":"Functional API (Alternative)","text":"For simpler cases or when AD compatibility is needed, you can use the functional API:\n\n# Define model and run filter\np = KFParms(Z, H, T, R, Q)\nresult = kalman_filter(p, y, a1, P1)\n\n# Plot filtered states with 95% confidence bands\nplot(result)\nplot(result, vars=1)             # State 1 only\nplot(result, vars=[1,3])         # States 1 and 3\nplot(result, level=0.90)         # 90% CI\nplot(result, band=false)         # No confidence bands\nplot(result, filtered=false)     # Predicted states instead of filtered","category":"section"},{"location":"tutorials/visualization.html#Plotting-Smoothed-States","page":"Visualization","title":"Plotting Smoothed States","text":"The SmootherResult wrapper enables plotting smoothed states from any source:\n\n# Method 1: From KalmanWorkspace (recommended)\nws = KalmanWorkspace(Z, H, T, R, Q, a1, P1, n)\nkalman_filter!(ws, y)\nkalman_smoother!(ws)\nsmooth = SmootherResult(ws)\nplot(smooth)\n\n# Method 2: Convenience constructor (runs filter + smoother internally)\np = KFParms(Z, H, T, R, Q)\nsmooth = SmootherResult(p, y, a1, P1)\nplot(smooth, vars=1, level=0.90)\n\n# Method 3: Wrap existing smoother NamedTuple output\nfilt = kalman_filter(p, y, a1, P1)\nnt = kalman_smoother(p.Z, p.T, filt.at, filt.Pt, filt.vt, filt.Ft)\nsmooth = SmootherResult(nt)\nplot(smooth)","category":"section"},{"location":"tutorials/visualization.html#Keywords-2","page":"Visualization","title":"Keywords","text":"Keyword Default Description\nvars :all Variables to plot\nlevel 0.95 Confidence level for bands\nband true Whether to show confidence bands","category":"section"},{"location":"tutorials/visualization.html#Plotting-Forecasts","page":"Visualization","title":"Plotting Forecasts","text":"Wrap forecast output in ForecastResult:\n\n# Run forecast\nfc_nt = forecast(spec, θ, y, 24)  # 24-step ahead forecast\nfc = ForecastResult(fc_nt)\n\n# Plot forecasted observations (default)\nplot(fc)\nplot(fc, what=:observations)\n\n# Plot forecasted states\nplot(fc, what=:states)\nplot(fc, what=:states, vars=1)","category":"section"},{"location":"tutorials/visualization.html#Keywords-3","page":"Visualization","title":"Keywords","text":"Keyword Default Description\nvars :all Variables to plot\nlevel 0.95 Confidence level for bands\nwhat :observations Plot :observations or :states\nband true Whether to show confidence bands","category":"section"},{"location":"tutorials/visualization.html#Comparing-Filtered-vs-Smoothed","page":"Visualization","title":"Comparing Filtered vs Smoothed","text":"Plot both filtered and smoothed states together:\n\nresult = kalman_filter(p, y, a1, P1)\nsmooth = SmootherResult(p, y, a1, P1)\n\n# Compare on same plot\nplot((result, smooth), vars=1)\nplot((result, smooth), vars=[1,2], band=true)\n\nThe comparison shows filtered states as dashed blue lines and smoothed states as solid green lines, with optional confidence bands around the smoother.","category":"section"},{"location":"tutorials/visualization.html#Plotting-Observable-Predictions","page":"Visualization","title":"Plotting Observable Predictions","text":"Plot one-step-ahead predictions of observables (yhatt = Z * a_t):\n\nresult = kalman_filter(p, y, a1, P1)\n\n# One-step-ahead predictions\nplot(result, ObservablePlot)\n\n# With actual observations overlay\nplot(result, ObservablePlot, actual=y)\n\n# Select specific observables\nplot(result, ObservablePlot, vars=1, level=0.90)","category":"section"},{"location":"tutorials/visualization.html#Complete-Example:-Nile-River-Flow","page":"Visualization","title":"Complete Example: Nile River Flow","text":"using Plots\nusing Siphon\nusing DelimitedFiles\n\n# Load Nile data\nnile = readdlm(\"Nile.csv\", ',', Float64)\ny = reshape(nile[:, 1], 1, :)\nn = size(y, 2)\n\n# Local level model parameters (MLE estimates)\nZ = [1.0;;]       # Observation matrix\nH = [15099.0;;]   # Observation variance\nT = [1.0;;]       # Transition matrix\nR = [1.0;;]       # Selection matrix\nQ = [1469.0;;]    # State variance\na1 = [0.0]        # Initial state mean\nP1 = [1e7;;]      # Initial state variance (diffuse)\n\n# Create workspace and run filter + smoother\nws = KalmanWorkspace(Z, H, T, R, Q, a1, P1, n)\nkalman_filter!(ws, y)\nkalman_smoother!(ws)\n\n# Plot smoothed states (recommended)\np1 = plot(ws, what=:smoothed, title=\"Smoothed Level\",\n          xlabel=\"Year\", ylabel=\"Flow\")\n\n# Plot filtered states\np2 = plot(ws, what=:filtered, title=\"Filtered Level\",\n          xlabel=\"Year\", ylabel=\"Flow\")\n\n# Plot predicted states\np3 = plot(ws, what=:predicted, title=\"Predicted Level\",\n          xlabel=\"Year\", ylabel=\"Flow\")\n\n# For comparison with functional API\nresult = kalman_filter(KFParms(Z, H, T, R, Q), y, a1, P1)\nsmooth = SmootherResult(ws)  # Create from workspace\n\n# Compare filtered vs smoothed\np4 = plot((result, smooth), title=\"Filtered vs Smoothed\",\n          xlabel=\"Year\", ylabel=\"Flow\")\n\n# Observable predictions with actual data\np5 = plot(result, ObservablePlot, actual=y,\n          title=\"One-Step-Ahead Predictions\",\n          xlabel=\"Year\", ylabel=\"Flow\")\n\n# Combine into single figure\nplot(p1, p2, p4, p5, layout=(2,2), size=(1000, 800))","category":"section"},{"location":"tutorials/visualization.html#Customizing-Plots","page":"Visualization","title":"Customizing Plots","text":"Since recipes use RecipesBase.jl, you can combine them with standard Plots.jl attributes:\n\nplot(result,\n    vars=1,\n    level=0.95,\n    title=\"Custom Title\",\n    xlabel=\"Time\",\n    ylabel=\"Value\",\n    linewidth=3,\n    legend=:bottomright,\n    size=(800, 400)\n)","category":"section"},{"location":"tutorials/visualization.html#Color-Scheme","page":"Visualization","title":"Color Scheme","text":"The default color scheme uses consistent colors across plot types:\n\nFiltered states: Steel blue (line) with light blue (bands)\nSmoothed states: Dark green (line) with light green (bands)\nForecasts: Dark orange (dashed line) with light orange (bands)\nPredicted states: Purple (line) with light purple (bands)","category":"section"},{"location":"tutorials/visualization.html#API-Reference","page":"Visualization","title":"API Reference","text":"","category":"section"},{"location":"tutorials/visualization.html#Core-Types","page":"Visualization","title":"Core Types","text":"KalmanWorkspace: In-place filter/smoother workspace (recommended)\nKalmanFilterResult: Functional API filter output\nSmootherResult: Wrapper for smoother output\nForecastResult: Wrapper for forecast output\nObservablePlot: Marker type for observable prediction plots","category":"section"},{"location":"tutorials/visualization.html#In-place-Functions","page":"Visualization","title":"In-place Functions","text":"kalman_filter!: Run filter in-place\nkalman_smoother!: Run smoother in-place\nfilter_and_smooth!: Run both in-place","category":"section"},{"location":"tutorials/visualization.html#Accessor-Functions","page":"Visualization","title":"Accessor Functions","text":"smoothed_states: Get smoothed state means\nfiltered_states: Get filtered state means\npredicted_states: Get predicted state means\nvariances_smoothed_states: Get smoothed state covariances\nloglikelihood: Get log-likelihood","category":"section"},{"location":"tutorials/visualization.html#Helper-Functions","page":"Visualization","title":"Helper Functions","text":"confidence_bands: Compute confidence interval bounds\nselect_vars: Parse variable selection keywords","category":"section"},{"location":"tutorials/estimation_methods.html#Estimation-Methods","page":"Estimation Methods","title":"Estimation Methods","text":"This tutorial demonstrates how to specify and estimate the same model using Siphon.jl's different approaches:\n\nUnified fit! API (recommended): Use StateSpaceModel with fit!(MLE(), ...) or fit!(EM(), ...)\nDirect optimization: Use SSMSpec with optimize_ssm() for more control\nProfile EM: Use profile_em_ssm() for DNS models with nonlinear λ\n\nWe'll use the Dynamic Nelson-Siegel (DNS) yield curve model as our running example.","category":"section"},{"location":"tutorials/estimation_methods.html#The-DNS-Model","page":"Estimation Methods","title":"The DNS Model","text":"The Dynamic Nelson-Siegel model decomposes the yield curve into three latent factors:\n\nbeginaligned\ny_t = Z(lambda) f_t + varepsilon_t quad varepsilon_t sim N(0 H) \nf_t+1 = T f_t + eta_t quad eta_t sim N(0 Q)\nendaligned\n\nwhere:\n\ny_t is the p times 1 vector of yields at different maturities\nf_t = L_t S_t C_t are Level, Slope, and Curvature factors\nZ(lambda) is the p times 3 loading matrix with decay parameter lambda\n\nThe factor loadings are:\n\nZ(lambda)_i = beginbmatrix 1  frac1-e^-lambdatau_ilambdatau_i  frac1-e^-lambdatau_ilambdatau_i - e^-lambdatau_i endbmatrix","category":"section"},{"location":"tutorials/estimation_methods.html#Simulating-Test-Data","page":"Estimation Methods","title":"Simulating Test Data","text":"First, let's simulate data from a known DNS model:\n\nusing Siphon\nusing LinearAlgebra\nusing Random\nusing Statistics\n\nRandom.seed!(42)\n\n# Maturities in months\nmaturities = [3, 6, 12, 24, 36, 60, 84, 120]\nn_maturities = length(maturities)\nn_obs = 200\n\n# True parameters\nλ_true = 0.0609\nT_true = Diagonal([0.99, 0.95, 0.90])\nQ_true = Diagonal([0.01, 0.02, 0.03])\nH_true = 0.0001 * I(n_maturities)\n\n# Build DNS loadings\nfunction dns_loadings(λ, maturities)\n    p = length(maturities)\n    Z = ones(p, 3)\n    for (i, τ) in enumerate(maturities)\n        x = λ * τ\n        Z[i, 2] = x < 1e-10 ? 1.0 - x/2 : (1 - exp(-x)) / x\n        Z[i, 3] = Z[i, 2] - exp(-x)\n    end\n    return Z\nend\n\nZ_true = dns_loadings(λ_true, maturities)\n\n# Simulate factors and yields\nL_Q = cholesky(Symmetric(Matrix(Q_true))).L\nL_H = cholesky(Symmetric(Matrix(H_true))).L\n\nfactors = zeros(3, n_obs)\nyields = zeros(n_maturities, n_obs)\n\nfor t in 1:n_obs\n    if t > 1\n        factors[:, t] = T_true * factors[:, t-1] + L_Q * randn(3)\n    end\n    yields[:, t] = Z_true * factors[:, t] + L_H * randn(n_maturities)\nend\n\nprintln(\"Simulated $n_obs observations at $(n_maturities) maturities\")","category":"section"},{"location":"tutorials/estimation_methods.html#Approach-1:-DSL-with-optimize_ssm-(MLE)","page":"Estimation Methods","title":"Approach 1: DSL with optimize_ssm (MLE)","text":"The DSL approach uses dns_model() to create a specification and optimize_ssm() for MLE:\n\n# Create DNS specification with diagonal dynamics\nspec = dns_model(maturities;\n    T_structure = :diagonal,  # Diagonal AR(1) for each factor\n    H_structure = :diagonal,  # Diagonal observation variances\n    Q_structure = :diagonal,  # Diagonal state variances\n    λ_init = 0.06,\n    T_init = 0.9,\n    Q_init = 0.01,\n    H_init = 0.001\n)\n\nprintln(\"Parameters: \", param_names(spec))\n# [:λ, :T_L, :T_S, :T_C, :Q_L, :Q_S, :Q_C, :H_1, ..., :H_8]\n\n# Estimate via MLE (gradient-based optimization)\nresult_mle = optimize_ssm(spec, yields; maxiters=500)\n\nprintln(\"Estimated λ: \", round(result_mle.θ.λ, digits=4), \" (true: $λ_true)\")\nprintln(\"Estimated T diagonal: \", [result_mle.θ.T_L, result_mle.θ.T_S, result_mle.θ.T_C])\nprintln(\"Log-likelihood: \", round(result_mle.loglik, digits=2))","category":"section"},{"location":"tutorials/estimation_methods.html#Extracting-Smoothed-Factors","page":"Estimation Methods","title":"Extracting Smoothed Factors","text":"# Build state-space with estimated parameters\nss = build_linear_state_space(spec, result_mle.θ, yields)\n\n# Run filter and smoother\nfilt = kalman_filter(ss.p, yields, ss.a1, ss.P1)\nsmooth = kalman_smoother(ss.p.Z, ss.p.T, filt.at, filt.Pt, filt.vt, filt.Ft)\n\n# Compare with true factors\nfor (i, name) in enumerate([\"Level\", \"Slope\", \"Curvature\"])\n    corr = cor(factors[i, :], smooth.alpha[i, :])\n    println(\"$name correlation: \", round(corr, digits=4))\nend","category":"section"},{"location":"tutorials/estimation_methods.html#Approach-2:-DSL-with-profile*em*ssm-(Profile-EM)","page":"Estimation Methods","title":"Approach 2: DSL with profileemssm (Profile EM)","text":"For DNS models, the decay parameter lambda appears nonlinearly in the loadings, making pure EM difficult. The profile EM approach:\n\nGrids over lambda values\nFor each lambda, runs EM to estimate T, Q, H\nReturns the lambda with highest log-likelihood\n\n# Create spec with full covariance structures (for EM)\nspec_full = dns_model(maturities;\n    T_structure = :full,      # Full 3×3 VAR matrix\n    H_structure = :diagonal,\n    Q_structure = :full,      # Full 3×3 covariance\n    λ_init = 0.06\n)\n\n# Profile EM estimation\nresult_em = profile_em_ssm(spec_full, yields;\n    λ_grid = 0.02:0.01:0.12,  # Grid of λ values to search\n    maxiter = 200,\n    verbose = true\n)\n\nprintln(\"\\nProfile EM Results:\")\nprintln(\"Optimal λ: \", round(result_em.λ_optimal, digits=4), \" (true: $λ_true)\")\nprintln(\"Log-likelihood: \", round(result_em.loglik, digits=2))\n\n# Access estimated matrices\nT_est = result_em.em_result.T\nQ_est = result_em.em_result.Q\nH_est = result_em.em_result.H\n\nprintln(\"\\nEstimated T (factor dynamics):\")\ndisplay(round.(T_est, digits=3))\n\nprintln(\"\\nEstimated Q diagonal: \", round.(diag(Q_est), digits=4))\nprintln(\"True Q diagonal: \", diag(Q_true))","category":"section"},{"location":"tutorials/estimation_methods.html#Approach-3:-StateSpaceModel-with-fit!-(Recommended)","page":"Estimation Methods","title":"Approach 3: StateSpaceModel with fit! (Recommended)","text":"The StateSpaceModel type wraps an SSMSpec and provides a unified fit! interface:\n\n# Create StateSpaceModel from spec\nspec = dns_model(maturities;\n    T_structure = :diagonal,\n    H_structure = :diagonal,\n    Q_structure = :diagonal,\n    λ_init = 0.06\n)\n\nmodel = StateSpaceModel(spec, n_obs)\n\n# Fit with MLE\nfit!(MLE(), model, yields)\n\nprintln(\"MLE Results:\")\nprintln(\"Converged: \", model.converged)\nprintln(\"Log-likelihood: \", round(loglikelihood(model), digits=2))\nprintln(\"Parameters: \", model.theta_values)\n\n# Access filtered states directly\nprintln(\"Filtered state at t=100: \", model.att[:, 100])","category":"section"},{"location":"tutorials/estimation_methods.html#StateSpaceModel-with-EM","page":"Estimation Methods","title":"StateSpaceModel with EM","text":"For models where EM is applicable, you can use fit!(EM(), ...):\n\n# Local level model example (EM works well here)\nspec_ll = local_level()\nmodel_ll = StateSpaceModel(spec_ll, n_obs)\n\n# Simulate local level data\ny_ll = cumsum(randn(n_obs)) + 0.5 * randn(n_obs)\ny_ll = reshape(y_ll, 1, n_obs)\n\n# Fit with EM\nfit!(EM(), model_ll, y_ll; maxiter=200, tol=1e-6, verbose=true)\n\nprintln(\"\\nLocal Level EM Results:\")\nprintln(\"Converged: \", model_ll.converged)\nprintln(\"Iterations: \", model_ll.iterations)\nprintln(\"Parameters: \", model_ll.theta_values)","category":"section"},{"location":"tutorials/estimation_methods.html#Comparison:-When-to-Use-Each-Approach","page":"Estimation Methods","title":"Comparison: When to Use Each Approach","text":"Approach Best For Advantages Limitations\nfit!(MLE(), ...) General-purpose Clean API, auto memory management May converge slowly for many parameters\nfit!(EM(), ...) Variance estimation Fast closed-form updates, zero-allocation Limited to specific model structures\noptimize_ssm() Fine-grained control Direct access to optimizer settings Lower-level API\nprofile_em_ssm() DNS/Svensson models Robust for λ estimation, full covariances Requires grid search over λ","category":"section"},{"location":"tutorials/estimation_methods.html#Complete-Example:-DNS-with-Full-Pipeline","page":"Estimation Methods","title":"Complete Example: DNS with Full Pipeline","text":"Here's a complete workflow combining specification, estimation, and analysis:\n\nusing Siphon\nusing LinearAlgebra\nusing Statistics\n\n# 1. SPECIFY MODEL\nmaturities = [3, 6, 12, 24, 60, 120]\nspec = dns_model(maturities;\n    T_structure = :full,\n    Q_structure = :full,\n    H_structure = :diagonal,\n    λ_init = 0.06\n)\n\nprintln(\"Model: \", spec.name)\nprintln(\"Parameters: \", length(spec.params))\nprintln(\"States: \", spec.n_states)\n\n# 2. LOAD/SIMULATE DATA\n# (Using simulated data from above)\n\n# 3. ESTIMATE\nresult = profile_em_ssm(spec, yields;\n    λ_grid = 0.03:0.005:0.10,\n    maxiter = 300,\n    verbose = false\n)\n\n# 4. EXTRACT RESULTS\nλ_opt = result.λ_optimal\nT_opt = result.em_result.T\nQ_opt = result.em_result.Q\nH_opt = result.em_result.H\n\nprintln(\"\\n=== Estimation Results ===\")\nprintln(\"λ: \", round(λ_opt, digits=4))\nprintln(\"T eigenvalues: \", round.(eigvals(T_opt), digits=3))\nprintln(\"Q diagonal: \", round.(diag(Q_opt), digits=5))\n\n# 5. SMOOTH FACTORS\nZ_opt = dns_loadings(λ_opt, maturities)\np_final = KFParms(Z_opt, H_opt, T_opt, Matrix{Float64}(I, 3, 3), Q_opt)\na1 = zeros(3)\nP1 = 1e4 * Matrix{Float64}(I, 3, 3)\n\nfilt = kalman_filter(p_final, yields, a1, P1)\nsmooth = kalman_smoother(Z_opt, T_opt, filt.at, filt.Pt, filt.vt, filt.Ft)\n\n# 6. ANALYZE FACTORS\nprintln(\"\\n=== Factor Analysis ===\")\nfor (i, name) in enumerate([\"Level\", \"Slope\", \"Curvature\"])\n    f = smooth.alpha[i, :]\n    println(\"$name: mean=$(round(mean(f), digits=2)), std=$(round(std(f), digits=2))\")\nend\n\n# 7. FORECAST (optional)\n# Build forecast from final state\nh = 12  # 12-period ahead\nf_last = smooth.alpha[:, end]\nf_forecast = zeros(3, h)\nfor t in 1:h\n    f_forecast[:, t] = T_opt^t * f_last\nend\n\ny_forecast = Z_opt * f_forecast\nprintln(\"\\n=== 12-Period Yield Forecast ===\")\nprintln(\"Short rate (3m): \", round.(y_forecast[1, :], digits=2))","category":"section"},{"location":"tutorials/estimation_methods.html#Summary","page":"Estimation Methods","title":"Summary","text":"Siphon.jl provides multiple estimation approaches to suit different needs:\n\nfit!(MLE(), model, y): Recommended general-purpose MLE with unified API\nfit!(EM(), model, y): EM algorithm for models with closed-form M-steps\nprofile_em_ssm(spec, y): Profile EM for DNS models with nonlinear λ\noptimize_ssm(spec, y): Direct optimization for fine-grained control\n\nChoose based on your model structure and computational requirements.","category":"section"},{"location":"tutorials/estimation_methods.html#Next-Steps","page":"Estimation Methods","title":"Next Steps","text":"See Custom Models for building your own specifications\nSee Dynamic Factor Models for large-panel factor analysis\nSee Parameter Transformations for understanding constraints","category":"section"},{"location":"api/core.html#Core-Functions","page":"Core Functions","title":"Core Functions","text":"This page documents the core Kalman filtering and smoothing functions.","category":"section"},{"location":"api/core.html#Types","page":"Core Functions","title":"Types","text":"","category":"section"},{"location":"api/core.html#Kalman-Filter","page":"Core Functions","title":"Kalman Filter","text":"","category":"section"},{"location":"api/core.html#Log-Likelihood","page":"Core Functions","title":"Log-Likelihood","text":"","category":"section"},{"location":"api/core.html#Full-Filter-Output","page":"Core Functions","title":"Full Filter Output","text":"","category":"section"},{"location":"api/core.html#Kalman-Smoother","page":"Core Functions","title":"Kalman Smoother","text":"","category":"section"},{"location":"api/core.html#Prediction-and-Forecasting","page":"Core Functions","title":"Prediction and Forecasting","text":"","category":"section"},{"location":"api/core.html#Missing-Data-Utilities","page":"Core Functions","title":"Missing Data Utilities","text":"","category":"section"},{"location":"api/core.html#StaticArrays-Utilities","page":"Core Functions","title":"StaticArrays Utilities","text":"","category":"section"},{"location":"api/core.html#Siphon.KFParms","page":"Core Functions","title":"Siphon.KFParms","text":"KFParms{Zt, Ht, Tt, Rt, Qt}\n\nState-space model parameters for the Kalman filter.\n\nState Space Model\n\ny_t = Z * α_t + ε_t,    ε_t ~ N(0, H)\nα_{t+1} = T * α_t + R * η_t,    η_t ~ N(0, Q)\n\nFields\n\nZ: Observation matrix (p × m)\nH: Observation noise covariance (p × p)\nT: State transition matrix (m × m)\nR: State noise selection matrix (m × r)\nQ: State noise covariance (r × r)\n\n\n\n\n\n","category":"type"},{"location":"api/core.html#Siphon.KFParms_static","page":"Core Functions","title":"Siphon.KFParms_static","text":"KFParms_static(Z, H, T, R, Q)\n\nCreate KFParms with automatic StaticArrays conversion for small matrices.\n\nEquivalent to KFParms(to_static_if_small(Z), ...) for all parameters. Use this for potential performance gains when state/observation dimensions are small (≤ 13).\n\nExample\n\n# These become SMatrix automatically\np = KFParms_static([1.0;;], [1.0;;], [1.0;;], [1.0;;], [1.0;;])\ntypeof(p.Z)  # SMatrix{1, 1, Float64, 1}\n\n# Large matrices stay as Matrix\np = KFParms_static(rand(20,20), rand(20,20), rand(20,20), rand(20,20), rand(20,20))\ntypeof(p.Z)  # Matrix{Float64}\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.kalman_loglik","page":"Core Functions","title":"Siphon.kalman_loglik","text":"kalman_loglik(p::KFParms, y, a1, P1) -> loglik\n\nCompute the log-likelihood of a linear Gaussian state space model using the Kalman filter.\n\nThis is an AD-compatible implementation that:\n\nUses pure functional operations (no in-place mutations)\nAvoids try-catch blocks\nPropagates element types correctly for automatic differentiation\nHandles missing observations (NaN values)\n\nArguments\n\np::KFParms: State space parameters (Z, H, T, R, Q)\ny::AbstractMatrix: Observations (p × n matrix, where p is observation dim, n is time).                      Missing values should be marked as NaN.\na1::AbstractVector: Initial state mean\nP1::AbstractMatrix: Initial state covariance\n\nReturns\n\nloglik::Real: Log-likelihood of the observed (non-missing) data\n\nState Space Model\n\ny_t = Z * α_t + ε_t,    ε_t ~ N(0, H)\nα_{t+1} = T * α_t + R * η_t,    η_t ~ N(0, Q)\n\nMissing Data\n\nWhen y[:, t] contains any NaN, the observation is treated as missing. The filter skips the measurement update and propagates the state:     a{t+1} = T * at     P{t+1} = T * Pt * T' + R * Q * R'\n\n\n\n\n\nkalman_loglik(p::KFParms{<:SMatrix,...}, y, a1::SVector, P1::SMatrix) -> loglik\n\nFully static specialization of Kalman filter log-likelihood for small state dimensions.\n\nWhen all system matrices in KFParms are SMatrix and the initial state (a1, P1) are SVector/SMatrix, this method keeps all intermediate computations static, avoiding heap allocations in the inner loop.\n\nThis provides significant speedup (3x-17x depending on dimensions) for models where both state dimension m and observation dimension p are small (≤ STATIC_THRESHOLD).\n\nPerformance Notes\n\nZero allocations in the inner loop\nEnables loop unrolling and SIMD vectorization\nBest for m ≤ 8 and p ≤ 5 (speedup decreases for larger dimensions)\n\n\n\n\n\nkalman_loglik(p::KFParms, y, a1, P1_star, P1_inf; tol=1e-8) -> loglik\n\nCompute log-likelihood using exact diffuse initialization (Durbin-Koopman method).\n\nThis is the unified API: passing 5 positional arguments (with P1inf as the 5th) triggers exact diffuse initialization. For standard (non-diffuse) filtering, use the 4-arg version `kalmanloglik(p, y, a1, P1)`.\n\nThe initial state covariance is P1 = P1star + κ * P1inf where κ → ∞. Only observations after the diffuse period contribute to the log-likelihood.\n\nArguments\n\np::KFParms: State space parameters (Z, H, T, R, Q)\ny::AbstractMatrix: Observations (p × n matrix)\na1::AbstractVector: Initial state mean\nP1_star::AbstractMatrix: Finite part of initial covariance\nP1_inf::AbstractMatrix: Diffuse part of initial covariance (typically I or diagonal)\ntol::Real=1e-8: Tolerance for detecting end of diffuse period\n\nExample\n\n# Local level model with exact diffuse initialization\np = KFParms([1.0;;], [σ²_obs;;], [1.0;;], [1.0;;], [σ²_state;;])\ny = randn(1, 100)\na1 = [0.0]\nP1_star = [0.0;;]  # No finite uncertainty initially\nP1_inf = [1.0;;]   # Full diffuse on the level\n\nll = kalman_loglik(p, y, a1, P1_star, P1_inf)\n\nSee also: kalman_filter (5-arg version for full filter output)\n\n\n\n\n\nkalman_loglik(model::StateSpaceModel, y::AbstractMatrix) -> Real\n\nCompute log-likelihood using the model's current parameters.\n\nThis is a convenience method that builds system matrices from stored parameters and computes the log-likelihood without modifying the model's internal state.\n\nArguments\n\nmodel: StateSpaceModel with fitted parameters\ny: Observations (p × n matrix), missing values as NaN\n\nReturns\n\nLog-likelihood value (Real)\n\nExample\n\nspec = local_level()\nθ = (var_obs=100.0, var_level=50.0)\nmodel = StateSpaceModel(spec, θ, 100)\n\ny = randn(1, 100)\nll = kalman_loglik(model, y)\n\nSee also: kalman_filter!, loglikelihood\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.kalman_loglik_scalar","page":"Core Functions","title":"Siphon.kalman_loglik_scalar","text":"kalman_loglik_scalar(Z, H, T, R, Q, a1, P1, y) -> loglik\n\nScalar (univariate) version of the Kalman filter log-likelihood.\n\nOptimized for the common case of scalar state and observation. Handles missing observations marked as NaN.\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.kalman_filter","page":"Core Functions","title":"Siphon.kalman_filter","text":"kalman_filter(p::KFParms, y, a1, P1) -> KalmanFilterResult\n\nRun full Kalman filter returning both predicted and filtered states.\n\nFor n observations, returns n time points:\n\nat[:, t] = E[αₜ | y₁:ₜ₋₁] (predicted state before seeing yₜ)\natt[:, t] = E[αₜ | y₁:ₜ] (filtered state after seeing yₜ)\nPt[:, :, t] = Var[αₜ | y₁:ₜ₋₁] (predicted covariance)\nPtt[:, :, t] = Var[αₜ | y₁:ₜ] (filtered covariance)\n\nReturns a KalmanFilterResult with:\n\nloglik: Log-likelihood (of non-missing observations)\nat: Predicted state means (m × n)\nPt: Predicted state covariances (m × m × n)\natt: Filtered state means (m × n)\nPtt: Filtered state covariances (m × m × n)\nvt: Innovations (p × n), NaN for missing observations\nFt: Innovation covariances (p × p × n)\nKt: Kalman gains (m × p × n), zero for missing observations\nmissing_mask: BitVector indicating missing observations (length n)\n\nUse accessor methods: predicted_states, filtered_states, variances_predicted_states, variances_filtered_states, prediction_errors, variances_prediction_errors, kalman_gains, loglikelihood.\n\nMissing Data\n\nWhen y[:, t] contains any NaN, the observation is treated as missing. The filter skips the measurement update and propagates the state.\n\n\n\n\n\nkalman_filter(p::KFParms{<:SMatrix,...}, y, a1::SVector, P1::SMatrix) -> KalmanFilterResult\n\nFully static specialization of Kalman filter for small state dimensions.\n\nWhen all system matrices in KFParms are SMatrix and the initial state (a1, P1) are SVector/SMatrix, this method uses static arithmetic in the inner loop while storing results in regular arrays (which must be heap-allocated for the full filter).\n\nThe speedup comes from keeping intermediate state computations (a, P, v, F, K) as StaticArrays, avoiding temporary allocations within each iteration.\n\n\n\n\n\nkalman_filter(p::KFParms, y, a1, P1_star, P1_inf; tol=1e-8) -> DiffuseFilterResult\n\nRun full Kalman filter with exact diffuse initialization.\n\nThis is the unified API: passing 5 positional arguments (with P1inf as the 5th) triggers exact diffuse initialization. For standard (non-diffuse) filtering, use the 4-arg version `kalmanfilter(p, y, a1, P1)`.\n\nReturns a DiffuseFilterResult containing all filter outputs plus diffuse-specific quantities (Pinf, Pstar during diffuse period, diffuse flags).\n\nArguments\n\np::KFParms: State space parameters (Z, H, T, R, Q)\ny::AbstractMatrix: Observations (p × n matrix)\na1::AbstractVector: Initial state mean\nP1_star::AbstractMatrix: Finite part of initial covariance\nP1_inf::AbstractMatrix: Diffuse part of initial covariance\ntol::Real=1e-8: Tolerance for detecting end of diffuse period\n\nExample\n\np = KFParms([1.0;;], [100.0;;], [1.0;;], [1.0;;], [10.0;;])\ny = randn(1, 50)\na1 = [0.0]\nP1_star = [0.0;;]\nP1_inf = [1.0;;]\n\nresult = kalman_filter(p, y, a1, P1_star, P1_inf)\nd = diffuse_period(result)  # Number of diffuse observations\n\nSee also: kalman_loglik (5-arg version for log-likelihood only)\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.kalman_filter_scalar","page":"Core Functions","title":"Siphon.kalman_filter_scalar","text":"kalman_filter_scalar(Z, H, T, R, Q, a1, P1, y) -> KalmanFilterResultScalar\n\nScalar version of full Kalman filter for univariate state-space models. Handles missing observations marked as NaN.\n\nFor n observations, returns n time points:\n\nat[t] = E[αₜ | y₁:ₜ₋₁] (predicted state)\natt[t] = E[αₜ | y₁:ₜ] (filtered state)\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.kalman_smoother","page":"Core Functions","title":"Siphon.kalman_smoother","text":"kalman_smoother(Z, T, at, Pt, vt, Ft; compute_crosscov=false, missing_mask=nothing, Ptt=nothing)\n\nAD-compatible RTS smoother. Takes filter outputs and returns smoothed states.\n\nArguments\n\nZ: Observation matrix (p × m)\nT: Transition matrix (m × m)\nat: Predicted states (m × n), where at[:, t] = E[αₜ | y₁:ₜ₋₁]\nPt: Predicted covariances (m × m × n)\nvt: Innovations (p × n), where vt[:, t] = yₜ - Z * at[:, t]\nFt: Innovation covariances (p × p × n)\ncompute_crosscov: If true, compute lag-one covariances P_{t+1,t|n} (default: false)\nmissing_mask: Optional BitVector indicating missing observations (length n). If not provided, inferred from NaN values in vt.\nPtt: Optional filtered covariances (m × m × n). Required for cross-cov with missing data. If not provided and compute_crosscov=true, computed from predicted covariances.\n\nReturns\n\nNamed tuple with:\n\nalpha: Smoothed states (m × n), where alpha[:, t] = E[αₜ | y₁:ₙ]\nV: Smoothed covariances (m × m × n)\nP_crosslag: Cross-lag covariances P{t+1,t|n} (m × m × (n-1)), only if computecrosscov=true\n\nMissing Data Handling\n\nWhen an observation is missing (indicated by missing_mask[t] == true or NaN in vt[:, t]), the smoother uses a simplified recursion that skips the measurement update:     r{t-1} = T' * rt     N{t-1} = T' * Nt * T This is consistent with treating missing observations as having infinite variance.\n\nCross-lag covariance formula\n\nUsing Shumway & Stoffer (2017), the lag-one covariance smoother gives:     Jt = P{t|t} * T' * inv(P{t+1|t})     P{t+1,t|n} = V{t+1} * Jt' where P_{t|t} is the filtered (updated) covariance.\n\nFor missing observations at time t, P{t|t} = P{t|t-1} (no update).\n\nNotes\n\nThis follows Durbin & Koopman (2012), Chapter 4, equations (4.32)-(4.44). Uses predicted states (at = a_{t|t-1}) not filtered states for the recursion. This is used by the EM algorithm which requires E[αₜ αₜ₋₁' | y₁:ₙ] for M-step updates.\n\n\n\n\n\nkalman_smoother(result::KalmanFilterResult, Z, T; compute_crosscov=false)\n\nRun RTS smoother using filter result. Convenience method.\n\nUses missing_mask from the filter result for proper handling of missing observations.\n\nReturns\n\nNamed tuple with:\n\nalpha: Smoothed states (m × n)\nV: Smoothed covariances (m × m × n)\nP_crosslag: Cross-lag covariances (only if compute_crosscov=true)\n\n\n\n\n\nkalman_smoother(p::KFParms, y, a1, P1; compute_crosscov=false) -> NamedTuple\n\nHigh-level Kalman smoother that runs filter and smoother in one call.\n\nArguments\n\np::KFParms: State-space model parameters (Z, H, T, R, Q)\ny: Observations (p × n matrix)\na1: Initial state mean (m-vector)\nP1: Initial state covariance (m × m matrix)\ncompute_crosscov: If true, compute lag-one covariances (default: false)\n\nReturns\n\nNamed tuple with:\n\nα: Smoothed states (m × n), where α[:, t] = E[αₜ | y₁:ₙ]\nV: Smoothed state covariances (m × m × n)\nP_crosslag: Cross-lag covariances (only if compute_crosscov=true)\n\nExample\n\np = KFParms(Z, H, T, R, Q)\nresult = kalman_smoother(p, y, a1, P1)\nsmoothed_states = result.α\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.kalman_smoother_scalar","page":"Core Functions","title":"Siphon.kalman_smoother_scalar","text":"kalman_smoother_scalar(Z, T, at, Pt, vt, Ft; missing_mask=nothing) -> (alpha, V)\n\nScalar version of RTS smoother for univariate state-space models.\n\nArguments\n\nZ::Real: Observation coefficient\nT::Real: State transition coefficient\nat::AbstractVector: Predicted states from filter (length n)\nPt::AbstractVector: Predicted variances from filter (length n)\nvt::AbstractVector: Innovations from filter (length n), NaN for missing\nFt::AbstractVector: Innovation variances from filter (length n)\nmissing_mask: Optional BitVector indicating missing observations\n\nReturns\n\nalpha::Vector: Smoothed states E[αₜ | y₁:ₙ] (length n)\nV::Vector: Smoothed variances Var[αₜ | y₁:ₙ] (length n)\n\nExample\n\n# Scalar local level: α_{t+1} = α_t + η_t, y_t = α_t + ε_t\nZ, T = 1.0, 1.0\nfilt = kalman_filter_scalar(Z, 100.0, T, 1.0, 10.0, 0.0, 1e7, y)\nalpha, V = kalman_smoother_scalar(Z, T, filt.at, filt.Pt, filt.vt, filt.Ft)\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.kalman_filter_and_smooth","page":"Core Functions","title":"Siphon.kalman_filter_and_smooth","text":"kalman_filter_and_smooth(p::KFParms, y, a1, P1)\n\nCombined filter and smoother for state-space models.\n\nReturns filtered states, smoothed states, and log-likelihood. AD-compatible.\n\nArguments\n\np::KFParms: State-space model parameters (Z, H, T, R, Q)\ny: Observations (p × n matrix)\na1: Initial state mean (m-vector)\nP1: Initial state covariance (m × m matrix)\n\nReturns\n\nNamed tuple with:\n\nloglik: Log-likelihood\na_filtered: Filtered states (m × n) - same as att from filter result\nP_filtered: Filtered covariances (m × m × n) - same as Ptt from filter result\nalpha_smooth: Smoothed states (m × n)\nV_smooth: Smoothed covariances (m × m × n)\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.predict","page":"Core Functions","title":"Siphon.predict","text":"predict(spec::SSMSpec, θ::NamedTuple, y::AbstractMatrix; use_static=true) -> NamedTuple\n\nCompute in-sample one-step-ahead predictions for a state-space model.\n\nFor n observations, returns:\n\nyhat: One-step-ahead predictions ŷₜ|ₜ₋₁ = Z * aₜ (obs_dim × n)\na: States (state_dim × (n+1)), where a[:, t] = E[αₜ | y₁:ₜ₋₁]\nP: State covariances (statedim × statedim × (n+1))\nv: Innovations vₜ = yₜ - ŷₜ|ₜ₋₁ (obs_dim × n)\nF: Innovation covariances (obsdim × obsdim × n)\nloglik: Log-likelihood\n\nNote: a[:, 1] is the initial state, a[:, n+1] is the forecast state.\n\nArguments\n\nuse_static::Bool=true: Use StaticArrays for small matrices (dimensions ≤ 13)\n\nExample\n\nspec = local_level()\nθ = (σ_obs = 1.0, σ_level = 0.5)\ny = randn(1, 100)\n\npred = predict(spec, θ, y)\nplot(vec(y), label=\"observed\")\nplot!(vec(pred.yhat), label=\"predicted\")\n\n\n\n\n\npredict(p::KFParms, y, a1, P1) -> NamedTuple\n\nCompute in-sample one-step-ahead predictions using KFParms directly.\n\nArguments\n\np::KFParms: State space parameters (Z, H, T, R, Q)\ny::AbstractMatrix: Observations (obs_dim × n)\na1::AbstractVector: Initial state mean (state_dim)\nP1::AbstractMatrix: Initial state covariance (statedim × statedim)\n\nReturns\n\nNamed tuple with:\n\nyhat: One-step-ahead predictions ŷₜ|ₜ₋₁ = Z * aₜ (obs_dim × n)\nat: Predicted states E[αₜ | y₁:ₜ₋₁] (state_dim × n)\nPt: Predicted state covariances (statedim × statedim × n)\natt: Filtered states E[αₜ | y₁:ₜ] (state_dim × n)\nPtt: Filtered state covariances (statedim × statedim × n)\nvt: Innovations vₜ = yₜ - ŷₜ|ₜ₋₁ (obs_dim × n)\nFt: Innovation covariances (obsdim × obsdim × n)\nloglik: Log-likelihood\nmissing_mask: BitVector indicating missing observations\n\nExample\n\np = KFParms(Z, H, T, R, Q)\npred = predict(p, y, a1, P1)\nplot(vec(y), label=\"observed\")\nplot!(vec(pred.yhat), label=\"predicted\")\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.forecast","page":"Core Functions","title":"Siphon.forecast","text":"forecast(spec::SSMSpec, θ::NamedTuple, y::AbstractMatrix, h::Int; use_static=true) -> NamedTuple\n\nForecast h steps ahead beyond the observed data.\n\nReturns a NamedTuple with:\n\nyhat: Forecasted observations (obs_dim × h)\na: Forecasted states (state_dim × h)\nP: Forecasted state covariances (statedim × statedim × h)\nF: Forecasted observation covariances (obsdim × obsdim × h)\n\nThe forecast starts from a[:, n+1] = E[αₙ₊₁ | y₁:ₙ] from the filter.\n\nArguments\n\nuse_static::Bool=true: Use StaticArrays for small matrices (dimensions ≤ 13)\n\nExample\n\nspec = local_level()\nθ = (σ_obs = 1.0, σ_level = 0.5)\ny = randn(1, 100)\n\nfc = forecast(spec, θ, y, 12)  # 12-step ahead forecast\nprintln(\"Forecast: \", fc.yhat)\nprintln(\"Forecast std: \", sqrt.(fc.F[1,1,:]))\n\n\n\n\n\nforecast(p::KFParms, y, a1, P1, h) -> NamedTuple\n\nLow-level forecasting using KFParms directly.\n\nStarts from the last filtered state and propagates forward h steps.\n\n\n\n\n\nforecast(model::StateSpaceModel, h::Int)\n\nForecast h steps ahead from fitted model.\n\nUses the filtered state at the last observation and iterates forward.\n\nArguments\n\nmodel: Fitted StateSpaceModel\nh: Forecast horizon\n\nReturns\n\nNamedTuple with:\n\nyhat: Forecasted observations (p × h)\na: Forecasted states (m × h)\nP: Forecasted state covariances (m × m × h)\nF: Forecasted observation covariances (p × p × h)\n\nExample\n\nmodel = StateSpaceModel(local_level(), 100)\nfit!(MLE(), model, y)\nfc = forecast(model, 10)\nfc.yhat  # 1 × 10 forecasts\n\n\n\n\n\nforecast(model::DynamicFactorModel, h::Int)\n\nCompute h-step ahead forecasts from a fitted DFM.\n\nUses the filtered state at the last observation (αₙ|ₙ, Pₙ|ₙ) and iterates the state equation forward:     E[αₙ₊ₕ|Y₁:ₙ] = Tʰ αₙ|ₙ     Var[αₙ₊ₕ|Y₁:ₙ] = Tʰ Pₙ|ₙ (T')ʰ + Σⱼ₌₀ʰ⁻¹ Tʲ RQR' (T')ʲ\n\nArguments\n\nmodel::DynamicFactorModel - Fitted DFM model\nh::Int - Forecast horizon\n\nReturns\n\nDynamicFactorModelForecast with forecasted states and observations\n\nExample\n\nmodel = DynamicFactorModel(100, 6, 200; factor_lags=3)\nfit!(EM(), model, y)\nfc = forecast(model, 4)\nci = forecast_interval(fc, 0.05)\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.forecast_paths","page":"Core Functions","title":"Siphon.forecast_paths","text":"forecast_paths(spec::SSMSpec, θ::NamedTuple, y::AbstractMatrix, h::Int, n_paths::Int; use_static=true) -> Array\n\nSimulate n_paths forecast trajectories of length h.\n\nReturns an array of size (obsdim × h × npaths).\n\nUseful for fan charts and prediction intervals.\n\nArguments\n\nuse_static::Bool=true: Use StaticArrays for small matrices (dimensions ≤ 13)\n\nExample\n\npaths = forecast_paths(spec, θ, y, 12, 1000)\nquantiles = [quantile(paths[1, j, :], [0.1, 0.5, 0.9]) for j in 1:12]\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.missing_to_nan","page":"Core Functions","title":"Siphon.missing_to_nan","text":"missing_to_nan(y::AbstractArray)\n\nConvert missing values to NaN. Returns a Float64 array.\n\nUse this if your data contains missing values:\n\ny_clean = missing_to_nan(y_with_missing)\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.nan_to_missing","page":"Core Functions","title":"Siphon.nan_to_missing","text":"nan_to_missing(y::AbstractArray)\n\nConvert NaN values to missing. Returns a Union{Float64, Missing} array.\n\nUse this if you prefer working with missing:\n\ny_missing = nan_to_missing(y_with_nan)\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.count_missing","page":"Core Functions","title":"Siphon.count_missing","text":"count_missing(y::AbstractMatrix) -> Int\n\nCount number of time periods with at least one missing observation.\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.ismissing_obs","page":"Core Functions","title":"Siphon.ismissing_obs","text":"ismissing_obs(y_t::AbstractVector) -> Bool\n\nCheck if observation vector contains any missing values (NaN).\n\n\n\n\n\nismissing_obs(y_t::Real) -> Bool\n\nCheck if scalar observation is missing (NaN).\n\n\n\n\n\n","category":"function"},{"location":"api/core.html#Siphon.STATIC_THRESHOLD","page":"Core Functions","title":"Siphon.STATIC_THRESHOLD","text":"Maximum dimension for automatic conversion to StaticArrays. Matrices with max(rows, cols) ≤ STATIC_THRESHOLD are converted.\n\n\n\n\n\n","category":"constant"},{"location":"api/core.html#Siphon.to_static_if_small","page":"Core Functions","title":"Siphon.to_static_if_small","text":"to_static_if_small(x)\n\nConvert array to StaticArray if dimensions are small enough (≤ STATIC_THRESHOLD). Returns the input unchanged if dimensions exceed threshold or if already a StaticArray.\n\nThis enables automatic performance optimization for small state-space models by using stack-allocated arrays that the compiler can unroll.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Estimation-and-Bayesian","page":"Estimation & Bayesian","title":"Estimation & Bayesian","text":"This page documents functions for parameter estimation and Bayesian inference.","category":"section"},{"location":"api/optimization.html#High-Level-Unified-API","page":"Estimation & Bayesian","title":"High-Level Unified API","text":"Siphon.jl provides a unified API centered around StateSpaceModel. There are two ways to create a model:","category":"section"},{"location":"api/optimization.html#Option-1:-Create-Model-with-Known-Parameters","page":"Estimation & Bayesian","title":"Option 1: Create Model with Known Parameters","text":"If you already have parameter values (from prior knowledge, simulation, or external estimation):\n\nusing Siphon\n\nspec = local_level()\nθ = (var_obs=15099.0, var_level=1469.0)\n\n# Create model with known parameters\nmodel = StateSpaceModel(spec, θ, n_obs)\n\n# Run filter and smoother\nll = kalman_filter!(model, y)\nkalman_smoother!(model)\n\n# Access results\nloglikelihood(model)        # Log-likelihood\nfiltered_states(model)      # E[αₜ|y₁:ₜ]\nsmoothed_states(model)      # E[αₜ|y₁:ₙ]\nparameters(model)           # Parameter NamedTuple","category":"section"},{"location":"api/optimization.html#Option-2:-Estimate-Parameters","page":"Estimation & Bayesian","title":"Option 2: Estimate Parameters","text":"If you want to estimate parameters from data:\n\nusing Siphon\n\nspec = local_level()\nmodel = StateSpaceModel(spec, n_obs)\n\n# Maximum likelihood estimation\nfit!(MLE(), model, y)\n\n# Or EM algorithm estimation\nfit!(EM(), model, y; maxiter=200, tol=1e-6, verbose=true)\n\n# Access results\nloglikelihood(model)        # Log-likelihood at optimum\nparameters(model)           # Fitted parameters as NamedTuple\nisconverged(model)          # Whether estimation converged\nniterations(model)          # Number of iterations (EM only)\n\n# Access system matrices at fitted parameters\nmats = system_matrices(model)  # All matrices as NamedTuple\nmats.Z  # Observation matrix\nmats.H  # Observation covariance\nmats.T  # Transition matrix\nmats.R  # Selection matrix\nmats.Q  # State covariance\n\n# Or access individually\nobs_matrix(model)        # Z\nobs_cov(model)           # H\ntransition_matrix(model) # T\nselection_matrix(model)  # R\nstate_cov(model)         # Q\n\n# Access filter/smoother results\nfiltered_states(model)     # E[αₜ|y₁:ₜ]\nsmoothed_states(model)     # E[αₜ|y₁:ₙ] (computed on demand)\nprediction_errors(model)   # yₜ - E[yₜ|y₁:ₜ₋₁]","category":"section"},{"location":"api/optimization.html#StateSpaceModel-Constructors","page":"Estimation & Bayesian","title":"StateSpaceModel Constructors","text":"","category":"section"},{"location":"api/optimization.html#Estimation-Methods","page":"Estimation & Bayesian","title":"Estimation Methods","text":"","category":"section"},{"location":"api/optimization.html#Unified-Filter/Smoother-Methods","page":"Estimation & Bayesian","title":"Unified Filter/Smoother Methods","text":"These methods work directly on StateSpaceModel objects:\n\n# Log-likelihood (doesn't modify model state)\nll = kalman_loglik(model, y)\n\n# Filter (stores results in model)\nll = kalman_filter!(model, y)\n\n# Smoother (uses stored filter results)\nkalman_smoother!(model)","category":"section"},{"location":"api/optimization.html#Model-Accessors","page":"Estimation & Bayesian","title":"Model Accessors","text":"After fitting a StateSpaceModel, use these functions to access results:","category":"section"},{"location":"api/optimization.html#Parameters","page":"Estimation & Bayesian","title":"Parameters","text":"","category":"section"},{"location":"api/optimization.html#System-Matrices","page":"Estimation & Bayesian","title":"System Matrices","text":"","category":"section"},{"location":"api/optimization.html#Filter-and-Smoother-Results","page":"Estimation & Bayesian","title":"Filter and Smoother Results","text":"","category":"section"},{"location":"api/optimization.html#Direct-Optimization","page":"Estimation & Bayesian","title":"Direct Optimization","text":"For more control, use optimize_ssm directly:","category":"section"},{"location":"api/optimization.html#Profile-EM-for-DNS-Models","page":"Estimation & Bayesian","title":"Profile EM for DNS Models","text":"For Dynamic Nelson-Siegel models with nonlinear λ parameter:","category":"section"},{"location":"api/optimization.html#Initial-State-Conventions-and-MARSS-Compatibility","page":"Estimation & Bayesian","title":"Initial State Conventions and MARSS Compatibility","text":"Siphon.jl internally uses the tinitx=1 convention where (a₁, P₁) represents the initial state distribution at time t=1. Both profile_em_ssm and DynamicFactorModel support the tinitx parameter to control how the initial state covariance is computed.\n\nFor a comprehensive explanation of initial state conventions, see the Initial State Tutorial.","category":"section"},{"location":"api/optimization.html#The-tinitx-Parameter","page":"Estimation & Bayesian","title":"The tinitx Parameter","text":"The tinitx parameter controls when the initial state covariance V0 is defined:\n\nSetting V0 Interpretation P1 Computation\ntinitx=0 (default) Covariance at t=0 P1 = T × V0 × T' + R × Q × R'\ntinitx=1 Covariance at t=1 P1 = V0 (no transformation)\n\nWith tinitx=0, the initial covariance incorporates one step of state dynamics, matching MARSS's default behavior.","category":"section"},{"location":"api/optimization.html#The-V0-Parameter","page":"Estimation & Bayesian","title":"The V0 Parameter","text":"The V0 parameter specifies the initial state covariance value:\n\nScalar: V0=100.0 creates V0 × I (identity scaled by V0)\nMatrix: Can also pass a full covariance matrix (for profile_em_ssm)\n\nDefault: V0=100.0 (MARSS default)","category":"section"},{"location":"api/optimization.html#Usage-Examples","page":"Estimation & Bayesian","title":"Usage Examples","text":"# DNS models via profile_em_ssm\nresult = profile_em_ssm(spec, y; tinitx=0, V0=100.0)  # Default: MARSS-style\nresult = profile_em_ssm(spec, y; tinitx=1, V0=1e7)    # Diffuse prior at t=1\n\n# Dynamic Factor Models\nmodel = DynamicFactorModel(N, k, n; tinitx=0, V0=100.0)  # Default: MARSS-style\nmodel = DynamicFactorModel(N, k, n; tinitx=1, V0=1e7)    # Diffuse prior at t=1","category":"section"},{"location":"api/optimization.html#Choosing-tinitx-and-V0","page":"Estimation & Bayesian","title":"Choosing tinitx and V0","text":"Use Case Recommended Setting\nMatch MARSS default tinitx=0, V0=100.0\nDiffuse prior (large uncertainty) tinitx=1, V0=1e7\nInformative prior at t=1 tinitx=1, V0=<your value>\nShort time series tinitx=0 (accounts for dynamics)\n\nNote: With tinitx=0, very large V0 values (e.g., 1e7) may cause numerical instability because the transformation T × V0 × T' amplifies values. Use tinitx=1 with large V0 for diffuse priors.","category":"section"},{"location":"api/optimization.html#Initial-State-Updating-in-EM-(update_initial_state)","page":"Estimation & Bayesian","title":"Initial State Updating in EM (update_initial_state)","text":"By default, (a₁, P₁) remains fixed throughout EM iterations. Set update_initial_state=true to update them at each M-step using smoothed state estimates:\n\n# For profile EM (DNS models)\nresult = profile_em_ssm(spec, y; update_initial_state=true)\n\n# For high-level API\nfit!(EM(), model, y; update_initial_state=true)\n\nHow it works:\n\nEM Iteration With update_initial_state=false (default) With update_initial_state=true\n0 (start) a1, P1 from tinitx/V0 a1, P1 from tinitx/V0\n1 Same a1, P1 Updated from smoother\n2 Same a1, P1 Updated from smoother\n... Same a1, P1 Updated from smoother\n\nWhen update_initial_state=true, after each E-step:\n\na₁_new = E[α₁ | y₁:n]      (smoothed state mean at t=1)\nP₁_new = Var[α₁ | y₁:n]    (smoothed state covariance at t=1)\n\nWhen to use update_initial_state=true:\n\nShort time series where t=1 significantly affects the likelihood\nComparing results with MARSS R package\nEstimating the unconditional mean/variance of the state process\n\nWhen to keep fixed (default):\n\nUsing a diffuse prior\nLong time series where initial state has negligible effect\nNumerical stability concerns\n\nThe final initial state estimates are returned in EMResult.a1 and EMResult.P1.","category":"section"},{"location":"api/optimization.html#Parameter-Transformations","page":"Estimation & Bayesian","title":"Parameter Transformations","text":"","category":"section"},{"location":"api/optimization.html#Log-Density-Interface","page":"Estimation & Bayesian","title":"Log-Density Interface","text":"","category":"section"},{"location":"api/optimization.html#Prior-Distributions","page":"Estimation & Bayesian","title":"Prior Distributions","text":"","category":"section"},{"location":"api/optimization.html#Siphon.StateSpaceModel","page":"Estimation & Bayesian","title":"Siphon.StateSpaceModel","text":"StateSpaceModel{T}\n\nMutable state-space model container wrapping an SSMSpec.\n\nSupports both MLE and EM estimation with automatic backend selection for EM.\n\nUsage\n\n# Create from SSMSpec\nspec = local_level(var_obs=:free, var_level=:free)\nmodel = StateSpaceModel(spec, 200)\n\n# Fit with MLE (pure Kalman filter, AD-compatible)\nfit!(MLE(), model, y)\n\n# Or fit with EM (auto-selects backend based on dimensions)\nfit!(EM(), model, y; maxiter=500, verbose=true)\n\n# Access results\nparams = parameters(model)         # NamedTuple\nf = filtered_states(model)         # m × n\ns = smoothed_states(model)         # computed on-demand\n\n# Forecast\nfc = forecast(model, 10)\n\nBackend Selection for EM\n\nIf max(nstates, nobs) > 13: uses in-place KalmanWorkspace (zero allocations)\nOtherwise: uses pure StaticArrays implementation\n\nFields (internal)\n\nSee source for field documentation.\n\n\n\n\n\n","category":"type"},{"location":"api/optimization.html#Siphon.MLE","page":"Estimation & Bayesian","title":"Siphon.MLE","text":"MLE\n\nEstimation method type for direct Maximum Likelihood Estimation. Used with fit!(MLE(), model, data; kwargs...).\n\n\n\n\n\n","category":"type"},{"location":"api/optimization.html#Siphon.EM","page":"Estimation & Bayesian","title":"Siphon.EM","text":"EM\n\nEstimation method type for EM (Expectation-Maximization) algorithm. Used with fit!(EM(), model, data; kwargs...).\n\n\n\n\n\n","category":"type"},{"location":"api/optimization.html#Siphon.fit!","page":"Estimation & Bayesian","title":"Siphon.fit!","text":"fit!(::MLE, model::StateSpaceModel, y::AbstractMatrix; kwargs...)\n\nFit state-space model using Maximum Likelihood Estimation.\n\nUses the pure AD-compatible Kalman filter via Optimization.jl. Automatically uses StaticArrays for small models (dims ≤ 13).\n\nArguments\n\nMLE(): Estimation method selector\nmodel: StateSpaceModel to fit (mutated in-place)\ny: Observations (p × n matrix), missing values as NaN\n\nKeyword Arguments\n\nmethod: Optimization algorithm (default: LBFGS from Optim.jl)\nverbose: Print optimization progress (default: false)\nAdditional kwargs passed to optimize_ssm\n\nReturns\n\nThe fitted model (same object, mutated)\n\nExample\n\nspec = local_level(var_obs=:free, var_level=:free)\nmodel = StateSpaceModel(spec, 100)\nfit!(MLE(), model, randn(1, 100))\nparameters(model)  # (var_obs=..., var_level=...)\n\n\n\n\n\nfit!(::EM, model::StateSpaceModel, y::AbstractMatrix; kwargs...)\n\nFit state-space model using EM algorithm.\n\nAutomatically selects backend based on dimensions:\n\nIf max(nstates, nobs) > 13: Use in-place KalmanWorkspace (zero allocations)\nOtherwise: Use pure implementation\n\nArguments\n\nEM(): Estimation method selector\nmodel: StateSpaceModel to fit (mutated in-place)\ny: Observations (p × n matrix), missing values as NaN\n\nKeyword Arguments\n\nmaxiter::Int=500: Maximum EM iterations\ntol::Real=1e-6: Convergence tolerance (relative change in log-likelihood)\nverbose::Bool=false: Print progress\n\nReturns\n\nThe fitted model (same object, mutated)\n\nNote\n\nEM estimation requires that free parameters are marked in the SSMSpec. For general state-space models, EM typically updates H and Q (covariances).\n\nExample\n\nspec = local_level(var_obs=:free, var_level=:free)\nmodel = StateSpaceModel(spec, 100)\nfit!(EM(), model, randn(1, 100); maxiter=200, verbose=true)\n\n\n\n\n\nfit!(::EM, model::DynamicFactorModel, y::AbstractMatrix;\n     maxiter::Int=500, tol::Real=1e-6, verbose::Bool=false)\n\nFit a dynamic factor model using the EM algorithm.\n\nArguments\n\nEM(): Estimation method selector\nmodel: DynamicFactorModel to fit (mutated in-place)\ny: Observations (N × n matrix), missing values as NaN\n\nKeyword Arguments\n\nmaxiter: Maximum EM iterations (default: 500)\ntol: Convergence tolerance for log-likelihood (default: 1e-6)\nverbose: Print progress (default: false)\n\nReturns\n\nThe fitted model (same object, mutated)\n\nExample\n\nmodel = DynamicFactorModel(100, 6, 200; factor_lags=3)\nfit!(EM(), model, y; verbose=true)\nf = factors(model)  # k × n smoothed factors\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.kalman_loglik-Tuple{StateSpaceModel, AbstractMatrix}","page":"Estimation & Bayesian","title":"Siphon.kalman_loglik","text":"kalman_loglik(model::StateSpaceModel, y::AbstractMatrix) -> Real\n\nCompute log-likelihood using the model's current parameters.\n\nThis is a convenience method that builds system matrices from stored parameters and computes the log-likelihood without modifying the model's internal state.\n\nArguments\n\nmodel: StateSpaceModel with fitted parameters\ny: Observations (p × n matrix), missing values as NaN\n\nReturns\n\nLog-likelihood value (Real)\n\nExample\n\nspec = local_level()\nθ = (var_obs=100.0, var_level=50.0)\nmodel = StateSpaceModel(spec, θ, 100)\n\ny = randn(1, 100)\nll = kalman_loglik(model, y)\n\nSee also: kalman_filter!, loglikelihood\n\n\n\n\n\n","category":"method"},{"location":"api/optimization.html#Siphon.kalman_filter!-Tuple{StateSpaceModel, AbstractMatrix}","page":"Estimation & Bayesian","title":"Siphon.kalman_filter!","text":"kalman_filter!(model::StateSpaceModel, y::AbstractMatrix) -> Real\n\nRun Kalman filter and store results in model. Returns log-likelihood.\n\nAfter calling this function:\n\nfiltered_states(model) returns E[αₜ|y₁:ₜ]\npredicted_states(model) returns E[αₜ|y₁:ₜ₋₁]\nloglikelihood(model) returns the log-likelihood\nModel is ready for kalman_smoother!(model)\n\nArguments\n\nmodel: StateSpaceModel with fitted parameters (mutated in-place)\ny: Observations (p × n matrix), missing values as NaN\n\nReturns\n\nLog-likelihood value (Real)\n\nExample\n\nspec = local_level()\nθ = (var_obs=100.0, var_level=50.0)\nmodel = StateSpaceModel(spec, θ, 100)\n\ny = randn(1, 100)\nll = kalman_filter!(model, y)\nfiltered_states(model)  # Access filtered states\n\nSee also: kalman_smoother!, kalman_loglik\n\n\n\n\n\n","category":"method"},{"location":"api/optimization.html#Siphon.kalman_smoother!-Tuple{StateSpaceModel}","page":"Estimation & Bayesian","title":"Siphon.kalman_smoother!","text":"kalman_smoother!(model::StateSpaceModel) -> Nothing\n\nRun Kalman smoother using stored filter results.\n\nRequires kalman_filter!(model, y) to have been called first. After calling this function, smoothed_states(model) returns E[αₜ|y₁:ₙ].\n\nArguments\n\nmodel: StateSpaceModel with valid filter results (mutated in-place)\n\nExample\n\nspec = local_level()\nθ = (var_obs=100.0, var_level=50.0)\nmodel = StateSpaceModel(spec, θ, 100)\n\ny = randn(1, 100)\nkalman_filter!(model, y)\nkalman_smoother!(model)\nsmoothed_states(model)  # Access smoothed states\n\nSee also: kalman_filter!, smoothed_states\n\n\n\n\n\n","category":"method"},{"location":"api/optimization.html#Siphon.parameters","page":"Estimation & Bayesian","title":"Siphon.parameters","text":"parameters(r::KalmanFilterResult) -> KFParms\n\nReturn the model parameters (Z, H, T, R, Q).\n\n\n\n\n\nReturn fitted parameters as NamedTuple. Throws if not fitted.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.loglikelihood","page":"Estimation & Bayesian","title":"Siphon.loglikelihood","text":"loglikelihood(r::KalmanFilterResult) -> Real\n\nReturn log-likelihood of non-missing observations.\n\n\n\n\n\nloglikelihood(ws::KalmanWorkspace) -> Real\n\nReturn log-likelihood of non-missing observations.\n\n\n\n\n\nReturn log-likelihood at fitted parameters. Throws if not fitted.\n\n\n\n\n\nloglikelihood(model::DynamicFactorModel) -> T\n\nReturn final log-likelihood. Throws error if model not fitted.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.system_matrices","page":"Estimation & Bayesian","title":"Siphon.system_matrices","text":"system_matrices(model::StateSpaceModel)\n\nReturn all system matrices as a NamedTuple (Z, H, T, R, Q) at fitted parameters.\n\nThis is more efficient than calling individual accessors when multiple matrices are needed.\n\nExample\n\nmats = system_matrices(model)\nmats.Z  # observation matrix\nmats.H  # observation covariance\nmats.T  # transition matrix\nmats.R  # selection matrix\nmats.Q  # state covariance\n\nThrows if model is not fitted.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.obs_matrix","page":"Estimation & Bayesian","title":"Siphon.obs_matrix","text":"obs_matrix(r::KalmanFilterResult)\n\nReturn observation matrix Z (p × m).\n\n\n\n\n\nobs_matrix(model::StateSpaceModel)\n\nReturn observation matrix Z (p × m) at fitted parameters.\n\nThrows if model is not fitted.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.obs_cov","page":"Estimation & Bayesian","title":"Siphon.obs_cov","text":"obs_cov(r::KalmanFilterResult)\n\nReturn observation noise covariance H (p × p).\n\n\n\n\n\nobs_cov(model::StateSpaceModel)\n\nReturn observation noise covariance H (p × p) at fitted parameters.\n\nThrows if model is not fitted.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.transition_matrix","page":"Estimation & Bayesian","title":"Siphon.transition_matrix","text":"transition_matrix(r::KalmanFilterResult)\n\nReturn state transition matrix T (m × m).\n\n\n\n\n\ntransition_matrix(model::StateSpaceModel)\n\nReturn state transition matrix T (m × m) at fitted parameters.\n\nThrows if model is not fitted.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.selection_matrix","page":"Estimation & Bayesian","title":"Siphon.selection_matrix","text":"selection_matrix(r::KalmanFilterResult)\n\nReturn state noise selection matrix R (m × r).\n\n\n\n\n\nselection_matrix(model::StateSpaceModel)\n\nReturn state noise selection matrix R (m × r) at fitted parameters.\n\nThrows if model is not fitted.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.state_cov","page":"Estimation & Bayesian","title":"Siphon.state_cov","text":"state_cov(r::KalmanFilterResult)\n\nReturn state noise covariance Q (r × r).\n\n\n\n\n\nstate_cov(model::StateSpaceModel)\n\nReturn state noise covariance Q (r × r) at fitted parameters.\n\nThrows if model is not fitted.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.filtered_states","page":"Estimation & Bayesian","title":"Siphon.filtered_states","text":"filtered_states(r::KalmanFilterResult) -> Matrix\n\nReturn filtered state means E[αₜ | y₁:ₜ] for t = 1:n. (FKF: att)\n\n\n\n\n\nfiltered_states(ws::KalmanWorkspace) -> Matrix\n\nReturn filtered state means E[αₜ | y₁:ₜ] (m × n).\n\n\n\n\n\nReturn filtered states E[αₜ|y₁:ₜ] (m × n). Throws if not fitted.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.predicted_states","page":"Estimation & Bayesian","title":"Siphon.predicted_states","text":"predicted_states(r::KalmanFilterResult) -> Matrix\n\nReturn predicted state means E[αₜ | y₁:ₜ₋₁] for t = 1:n. (FKF: at)\n\n\n\n\n\npredicted_states(ws::KalmanWorkspace) -> Matrix\n\nReturn predicted state means E[αₜ | y₁:ₜ₋₁] (m × n).\n\n\n\n\n\nReturn predicted states E[αₜ|y₁:ₜ₋₁] (m × n). Throws if not fitted.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.smoothed_states","page":"Estimation & Bayesian","title":"Siphon.smoothed_states","text":"smoothed_states(w::SmootherWorkspace) -> Matrix\n\nReturn smoothed state means E[αₜ | y₁:ₙ] from workspace.\n\n\n\n\n\nsmoothed_states(ws::KalmanWorkspace) -> Matrix\n\nReturn smoothed state means E[αₜ | y₁:ₙ] (m × n). Must call kalman_smoother!(ws) first.\n\n\n\n\n\nsmoothed_states(model::StateSpaceModel)\n\nReturn smoothed states E[αₜ|y₁:ₙ] (m × n).\n\nComputes smoother on first call and caches the result.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.prediction_errors","page":"Estimation & Bayesian","title":"Siphon.prediction_errors","text":"prediction_errors(r::KalmanFilterResult) -> Matrix\n\nReturn prediction errors (innovations) vₜ = yₜ - E[yₜ | y₁:ₜ₋₁] for t = 1:n. (FKF: vt) NaN for missing observations.\n\n\n\n\n\nprediction_errors(ws::KalmanWorkspace) -> Matrix\n\nReturn prediction errors (innovations) vₜ = yₜ - Z*aₜ (p × n). NaN for missing observations.\n\n\n\n\n\nReturn prediction errors / innovations yₜ - E[yₜ|y₁:ₜ₋₁] (p × n). Throws if not fitted.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.DSL.optimize_ssm","page":"Estimation & Bayesian","title":"Siphon.DSL.optimize_ssm","text":"optimize_ssm(spec, y; method=Optim.LBFGS(), kwargs...)\n\nOptimize state-space model parameters using Optimization.jl.\n\nWorks in unconstrained parameter space using automatic differentiation for gradient computation.\n\nArguments\n\nspec::SSMSpec: Model specification\ny::AbstractMatrix: Observations (p × n matrix)\nmethod: Optimization algorithm (default: L-BFGS)\nθ0: Initial parameter values (constrained). Default: initial_values(spec)\nad_backend: AD backend for gradients (default: Optimization.AutoForwardDiff())\nuse_static::Bool=true: Use StaticArrays for small matrices (dimensions ≤ 13)\nprob_kwargs: NamedTuple of kwargs passed to OptimizationProblem\nkwargs...: All other kwargs passed to Optimization.solve\n\nCommon solve kwargs\n\nmaxiters: Maximum iterations (default: 1000)\nmaxtime: Maximum time in seconds\nabstol: Absolute tolerance\nreltol: Relative tolerance\ncallback: Callback function (state, loss) -> Bool (return true to stop)\nprogress: Show progress bar (requires ProgressLogging.jl)\nshow_trace: Show optimization trace (Optim.jl specific)\n\nReturns\n\nNamed tuple with:\n\nθ: Optimal parameters (constrained space)\nloglik: Log-likelihood at optimum\nresult: Full Optimization.jl result object\nconverged: Whether optimization converged\n\nExample\n\nspec = local_level()\ny = randn(1, 100) .* 10 .+ 100\n\n# Basic usage\nresult = optimize_ssm(spec, y)\n\nprintln(\"Optimal parameters: \", result.θ)\nprintln(\"Log-likelihood: \", result.loglik)\n\n# With custom initial values\nresult2 = optimize_ssm(spec, y; θ0=(σ_obs=150.0, σ_level=50.0))\n\n# Different optimizer with options\nresult3 = optimize_ssm(spec, y;\n    method=Optim.Newton(),\n    maxiters=500,\n    show_trace=true\n)\n\n# With callback for monitoring\ncallback = (state, loss) -> begin\n    println(\"Iteration: loss = $loss\")\n    return false  # return true to stop\nend\nresult4 = optimize_ssm(spec, y; callback=callback)\n\n# Pass kwargs to OptimizationProblem (e.g., bounds in unconstrained space)\nresult5 = optimize_ssm(spec, y;\n    prob_kwargs=(lb=fill(-10.0, n_params(spec)),\n                 ub=fill(10.0, n_params(spec)))\n)\n\n# Disable StaticArrays (for debugging or if causing issues)\nresult6 = optimize_ssm(spec, y; use_static=false)\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.DSL.optimize_ssm_with_stderr","page":"Estimation & Bayesian","title":"Siphon.DSL.optimize_ssm_with_stderr","text":"optimize_ssm_with_stderr(spec, y; use_static=true, kwargs...)\n\nOptimize SSM and compute standard errors using the Hessian.\n\nReturns the same as optimize_ssm plus:\n\nstderr: Standard errors of parameters (in constrained space, approximate)\nhessian: Hessian matrix at the optimum (in unconstrained space)\n\nNote: Standard errors are approximate when using parameter transformations. For accurate standard errors on constrained parameters, use the delta method or bootstrap.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.DSL.profile_em_ssm","page":"Estimation & Bayesian","title":"Siphon.DSL.profile_em_ssm","text":"profile_em_ssm(spec::SSMSpec, y::AbstractMatrix; kwargs...) -> ProfileEMResult\n\nEstimate a state-space model using profile EM: grid search over λ (or other MatrixExpr parameters) with EM for remaining parameters at each grid point.\n\nThis is particularly useful for DNS models where λ enters the Z matrix non-linearly and cannot be efficiently estimated via standard EM.\n\nAlgorithm\n\nGrid over λ values\nFor each λ: fix Z(λ), run EM to estimate T, H, Q\nReturn λ with highest profile log-likelihood\n\nArguments\n\nspec::SSMSpec: Model specification with MatrixExpr for Z (e.g., from dns_model())\ny::AbstractMatrix: Observations (p × n)\n\nKeyword Arguments\n\nλ_grid: Grid of λ values to search (default: 0.01:0.005:0.2)\nλ_param::Symbol=:λ: Name of the profiled parameter\nverbose::Bool=false: Print progress\nmaxiter::Int=500: Max EM iterations per λ\ntol_ll::Real=1e-6: EM convergence tolerance\nwarm_start::Bool=true: Use previous EM solution as starting point\nupdate_initial_state::Bool=false: If true, update initial state (a1, P1) at each EM iteration using smoothed state estimates (MARSS-style). Default: false.\ntinitx::Int=0: Initial state timing convention (MARSS-style):\ntinitx=0: Initial state (a1, P1) is at t=0. P1 is computed as T * V0 * T' + R * Q * R'.\ntinitx=1: Initial state (a1, P1) is at t=1. P1 = V0 directly (no transformation).\nV0::Union{Real,AbstractMatrix}=100.0: Initial state covariance. Can be a scalar (interpreted as V0 * I) or a matrix. Interpretation depends on tinitx.\n\nReturns\n\nProfileEMResult with optimal parameters and profile likelihood\n\nExample\n\nmaturities = [3, 6, 12, 24, 60, 120]\nspec = dns_model(maturities; T_structure=:full, Q_structure=:full)\n\n# Profile EM estimation\nresult = profile_em_ssm(spec, yields; λ_grid=0.02:0.005:0.15, verbose=true)\n\nprintln(\"Optimal λ: \", result.λ_optimal)\nprintln(\"Log-likelihood: \", result.loglik)\n\n# Extract smoothed factors\nmodel = StateSpaceModel(spec, result.θ, size(yields, 2))\nkalman_filter!(model, yields)\nkalman_smoother!(model)\nsmooth = smoothed_states(model)\n\nNotes\n\nRequires spec to have :Z in matrix_exprs as a MatrixExpr\nWorks best with full T and Q structures (:full) for EM estimation\nWarm-starting significantly improves speed\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.DSL.ProfileEMResult","page":"Estimation & Bayesian","title":"Siphon.DSL.ProfileEMResult","text":"ProfileEMResult{T<:Real, NT<:NamedTuple}\n\nResult from profile EM estimation where some parameters are optimized via grid search (profile likelihood) while others use EM closed-form updates.\n\nFields\n\nλ_optimal::T: Optimal value of profiled parameter\nθ::NT: All parameter estimates at optimum (NamedTuple)\nloglik::T: Log-likelihood at optimum\nem_result: Full EM result at best λ (NamedTuple)\nλ_grid::Vector{T}: Grid of λ values searched\nloglik_profile::Vector{T}: Profile log-likelihood at each grid point\n\nUsage\n\nspec = dns_model(maturities; T_structure=:full, Q_structure=:full)\nresult = profile_em_ssm(spec, yields; λ_grid=0.01:0.01:0.2)\n\n# Best λ\nresult.λ_optimal\n\n# All parameters\nresult.θ  # NamedTuple with λ, T elements, H elements, Q elements\n\n# Profile likelihood plot data\nplot(result.λ_grid, result.loglik_profile)\n\n\n\n\n\n","category":"type"},{"location":"api/optimization.html#Siphon.DSL.build_transformation","page":"Estimation & Bayesian","title":"Siphon.DSL.build_transformation","text":"build_transformation(spec::SSMSpec)\n\nBuild a TransformVariables transformation from an SSMSpec.\n\nReturns a transformation that maps ℝⁿ → NamedTuple with the parameter names and appropriate constraints (positive for variances, bounded for others).\n\nExample\n\nspec = local_level()\nt = build_transformation(spec)\n# t transforms ℝ² → (σ_obs = ..., σ_level = ...)\n\nθ_nt = transform(t, randn(2))\n# θ_nt is a NamedTuple with positive values\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.DSL.transform_to_constrained","page":"Estimation & Bayesian","title":"Siphon.DSL.transform_to_constrained","text":"transform_to_constrained(spec, θ_unconstrained)\n\nTransform from unconstrained ℝⁿ to constrained parameter space. Returns (θ_constrained::NamedTuple, logjac::Real).\n\nUses TransformVariables.jl for the transformation.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.DSL.transform_to_unconstrained","page":"Estimation & Bayesian","title":"Siphon.DSL.transform_to_unconstrained","text":"transform_to_unconstrained(spec, θ_constrained::NamedTuple)\n\nTransform from constrained NamedTuple to unconstrained ℝⁿ. Inverse of transform_to_constrained.\n\n\n\n\n\ntransform_to_unconstrained(spec, θ_constrained::AbstractVector)\n\nTransform from constrained vector to unconstrained ℝⁿ. First converts vector to NamedTuple, then inverts.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.DSL.SSMLogDensity","page":"Estimation & Bayesian","title":"Siphon.DSL.SSMLogDensity","text":"SSMLogDensity(spec, y; prior=nothing, use_static=true)\n\nLog-density for a state-space model, evaluated in UNCONSTRAINED ℝⁿ space.\n\nThis integrates seamlessly with LogDensityProblems.jl for optimization and sampling. The transformation from unconstrained to constrained space is handled automatically using TransformVariables.jl.\n\nFields\n\nspec: Model specification (SSMSpec)\ntransformation: TransformVariables transformation (ℝⁿ → NamedTuple)\ny: Observation data (p × n matrix)\nprior: Optional prior on CONSTRAINED parameters (θ::NamedTuple -> log_prior)\nuse_static: Whether to use StaticArrays for small matrices (default: true)\n\nUsage\n\nspec = local_level()\ny = randn(1, 100)\n\n# Create log-density (works in unconstrained space)\nld = SSMLogDensity(spec, y)\n\n# Get initial point in unconstrained space\nθ0 = transform_to_unconstrained(spec, initial_values(spec))\n\n# Evaluate log-density\nlogdensity(ld, θ0)\n\n# For optimization (in unconstrained space, no bounds needed!)\nusing Optim\nresult = optimize(θ -> -logdensity(ld, θ), θ0, LBFGS())\n\n# Transform result back to NamedTuple\nθ_hat, _ = transform_to_constrained(spec, result.minimizer)\n# θ_hat is a NamedTuple like (σ_obs = 12.3, σ_level = 3.4)\n\n\n\n\n\n","category":"type"},{"location":"api/optimization.html#Siphon.DSL.logdensity","page":"Estimation & Bayesian","title":"Siphon.DSL.logdensity","text":"logdensity(ld::SSMLogDensity, θ_unconstrained)\n\nEvaluate the log-density at unconstrained parameters.\n\nReturns log p(y|θ) + log p(θ) + log|J| where J is the Jacobian of the transformation from unconstrained to constrained space.\n\nThe parameters are automatically transformed to a NamedTuple and passed to the likelihood function.\n\n\n\n\n\n","category":"function"},{"location":"api/optimization.html#Siphon.DSL.FlatPrior","page":"Estimation & Bayesian","title":"Siphon.DSL.FlatPrior","text":"FlatPrior()\n\nImproper flat prior (log-density = 0 everywhere). Works with both Vector and NamedTuple arguments.\n\n\n\n\n\n","category":"type"},{"location":"api/optimization.html#Siphon.DSL.NormalPrior","page":"Estimation & Bayesian","title":"Siphon.DSL.NormalPrior","text":"NormalPrior(μ::NamedTuple, σ::NamedTuple)\n\nIndependent normal priors for parameters, specified by name.\n\nExample\n\nprior = NormalPrior(\n    (σ_obs = 10.0, σ_level = 5.0),   # means\n    (σ_obs = 5.0, σ_level = 2.0)     # standard deviations\n)\n\n\n\n\n\n","category":"type"},{"location":"api/optimization.html#Siphon.DSL.InverseGammaPrior","page":"Estimation & Bayesian","title":"Siphon.DSL.InverseGammaPrior","text":"InverseGammaPrior(α, β, param_names)\n\nInverse gamma priors for variance parameters at specified parameter names. Assumes θ contains standard deviations (will square them).\n\nExample\n\nprior = InverseGammaPrior(2.0, 1.0, (:σ_obs, :σ_level))\n\n\n\n\n\n","category":"type"},{"location":"api/optimization.html#Siphon.DSL.CompositePrior","page":"Estimation & Bayesian","title":"Siphon.DSL.CompositePrior","text":"CompositePrior(priors...)\n\nCombine multiple priors by summing their log-densities.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/initial_state.html#Initial-State-Conventions-and-MARSS-Compatibility","page":"Initial State Conventions","title":"Initial State Conventions and MARSS Compatibility","text":"This page explains how Siphon.jl handles the initial state distribution and how it relates to the MARSS R package conventions.","category":"section"},{"location":"tutorials/initial_state.html#State-Space-Model-Formulation","page":"Initial State Conventions","title":"State Space Model Formulation","text":"Siphon.jl uses the standard state space formulation:\n\nObservation:  yₜ = Z αₜ + εₜ,    εₜ ~ N(0, H)\nState:        αₜ₊₁ = T αₜ + R ηₜ,  ηₜ ~ N(0, Q)\nInitial:      α₁ ~ N(a₁, P₁)\n\nThe Kalman filter processes observations starting at t=1, so it needs the initial state distribution (a₁, P₁) at time t=1.","category":"section"},{"location":"tutorials/initial_state.html#MARSS-tinitx-Parameter","page":"Initial State Conventions","title":"MARSS tinitx Parameter","text":"The MARSS R package uses a tinitx parameter to specify when the initial state is defined:","category":"section"},{"location":"tutorials/initial_state.html#tinitx0-(MARSS-Default)","page":"Initial State Conventions","title":"tinitx=0 (MARSS Default)","text":"The initial state (x₀, V₀) is specified at time t=0 (before the first observation). MARSS internally propagates it forward one time step:\n\na₁ = T × x₀\nP₁ = T × V₀ × T' + R × Q × R'\n\nThis incorporates one cycle of state dynamics into the initial covariance at t=1.","category":"section"},{"location":"tutorials/initial_state.html#tinitx1","page":"Initial State Conventions","title":"tinitx=1","text":"The initial state (x₀, V₀) is specified directly at time t=1:\n\na₁ = x₀\nP₁ = V₀\n\nNo transformation is applied.","category":"section"},{"location":"tutorials/initial_state.html#Siphon.jl-Convention","page":"Initial State Conventions","title":"Siphon.jl Convention","text":"Siphon.jl always uses the tinitx=1 convention internally. You specify (a₁, P₁) directly as the state distribution at time t=1.","category":"section"},{"location":"tutorials/initial_state.html#Mapping-MARSS-to-Siphon","page":"Initial State Conventions","title":"Mapping MARSS to Siphon","text":"To match MARSS behavior in Siphon.jl:\n\nMARSS Setting Siphon Equivalent\ntinitx=0, x0, V0 a1 = T * x0, P1 = T * V0 * T' + R * Q * R'\ntinitx=1, x0, V0 a1 = x0, P1 = V0","category":"section"},{"location":"tutorials/initial_state.html#Example:-Matching-MARSS-tinitx0","page":"Initial State Conventions","title":"Example: Matching MARSS tinitx=0","text":"using Siphon\nusing LinearAlgebra\n\n# MARSS uses tinitx=0 with these defaults:\nx0 = zeros(m)           # State mean at t=0\nV0 = 100.0 * I(m)       # State covariance at t=0\n\n# Your model parameters\nT = [0.9 0.0; 0.0 0.8]  # Transition matrix\nQ = [0.1 0.0; 0.0 0.2]  # State covariance\nR = I(m)                # Selection matrix\n\n# Convert to Siphon convention (tinitx=1):\na1 = T * x0                           # = zeros(m) if x0 = 0\nP1 = T * V0 * T' + R * Q * R'         # Incorporates dynamics\n\n# Now use a1 and P1 in Siphon\nkf = KFParms(Z, H, T, R, Q)\nll = kalman_loglik(kf, y, a1, P1)","category":"section"},{"location":"tutorials/initial_state.html#Initial-State-in-EM-Algorithm","page":"Initial State Conventions","title":"Initial State in EM Algorithm","text":"The EM algorithm can either keep the initial state fixed or update it at each iteration.","category":"section"},{"location":"tutorials/initial_state.html#Fixed-Initial-State-(Default)","page":"Initial State Conventions","title":"Fixed Initial State (Default)","text":"With update_initial_state=false (the default), (a₁, P₁) remains unchanged throughout all EM iterations:\n\nEM Iteration Initial State Parameters\n0 (start) a1, P1 from input T₀, Q₀, H₀ from input\n1 Same a1, P1 Updated T₁, Q₁, H₁\n2 Same a1, P1 Updated T₂, Q₂, H₂\n... Same a1, P1 ...\n\nUse this when:\n\nUsing a diffuse prior (large P₁)\nLong time series where t=1 has minimal effect\nNumerical stability is a concern","category":"section"},{"location":"tutorials/initial_state.html#Updated-Initial-State-(MARSS-style)","page":"Initial State Conventions","title":"Updated Initial State (MARSS-style)","text":"With update_initial_state=true, (a₁, P₁) is updated at each M-step using the smoothed state estimates:\n\na₁_new = E[α₁ | y₁:n]      (smoothed state mean at t=1)\nP₁_new = Var[α₁ | y₁:n]    (smoothed state covariance at t=1)\n\nEM Iteration Initial State Parameters\n0 (start) a1, P1 from input T₀, Q₀, H₀ from input\n1 Updated from smoother Updated T₁, Q₁, H₁\n2 Updated from smoother Updated T₂, Q₂, H₂\n... ... ...\n\nUse this when:\n\nShort time series where t=1 significantly affects the likelihood\nComparing results with MARSS\nEstimating the unconditional mean/variance of the state process","category":"section"},{"location":"tutorials/initial_state.html#Example:-EM-with-Initial-State-Updating","page":"Initial State Conventions","title":"Example: EM with Initial State Updating","text":"using Siphon\nusing Siphon.DSL: profile_em_ssm, dns_model\n\n# Create DNS model\nmaturities = [3, 12, 24, 60, 120]\nspec = dns_model(maturities)\n\n# Run EM with initial state updating\nresult = profile_em_ssm(spec, yields;\n    update_initial_state=true,\n    verbose=true\n)\n\n# Access final initial state estimates\nprintln(\"Final a1: \", result.em_result.a1)\nprintln(\"Final P1: \", result.em_result.P1)","category":"section"},{"location":"tutorials/initial_state.html#Using-tinitx-and-V0-Parameters","page":"Initial State Conventions","title":"Using tinitx and V0 Parameters","text":"Both profile_em_ssm (for DNS models) and DynamicFactorModel support the tinitx and V0 parameters for controlling initial state covariance:","category":"section"},{"location":"tutorials/initial_state.html#tinitx-Parameter","page":"Initial State Conventions","title":"tinitx Parameter","text":"tinitx=0 (default): V0 is the covariance at t=0. P1 is computed as:\nP1 = T * V0 * T' + R * Q * R'\nThis incorporates one step of state dynamics into P1.\ntinitx=1: V0 is the covariance at t=1. P1 = V0 directly (no transformation).","category":"section"},{"location":"tutorials/initial_state.html#Examples","page":"Initial State Conventions","title":"Examples","text":"# DNS models via profile_em_ssm\nresult = profile_em_ssm(spec, y; tinitx=0, V0=100.0)  # Default: MARSS-style\nresult = profile_em_ssm(spec, y; tinitx=1, V0=1e7)    # Diffuse prior at t=1\n\n# Dynamic Factor Models\nmodel = DynamicFactorModel(N, k, n; tinitx=0, V0=100.0)  # Default\nmodel = DynamicFactorModel(N, k, n; tinitx=1, V0=1e7)    # Diffuse prior at t=1\n\n# V0 can also be a matrix (for profile_em_ssm)\nV0_mat = Diagonal([100.0, 200.0, 300.0])\nresult = profile_em_ssm(spec, y; tinitx=1, V0=V0_mat)","category":"section"},{"location":"tutorials/initial_state.html#Choosing-tinitx","page":"Initial State Conventions","title":"Choosing tinitx","text":"Use Case Recommended Setting\nMatch MARSS default tinitx=0, V0=100.0\nDiffuse prior (large uncertainty) tinitx=1, V0=1e7\nInformative prior at t=1 tinitx=1, V0=<your value>\nShort time series tinitx=0 (accounts for dynamics)\n\nNote: With tinitx=0, very large V0 values (e.g., 1e7) may cause numerical instability because the transformation T * V0 * T' still produces large values. With tinitx=1, large V0 values are used directly and work well for diffuse priors.","category":"section"},{"location":"tutorials/initial_state.html#Numerical-Comparison","page":"Initial State Conventions","title":"Numerical Comparison","text":"At identical parameter values, Siphon.jl and MARSS produce matching log-likelihoods:\n\n# At MARSS converged parameters for DNS model:\n# MARSS log-likelihood:  430.032178464672\n# Siphon log-likelihood: 430.032178461962\n# Difference: ~2.7e-9 (numerical precision)","category":"section"},{"location":"tutorials/initial_state.html#Summary","page":"Initial State Conventions","title":"Summary","text":"Aspect MARSS tinitx=0 MARSS tinitx=1 Siphon tinitx=0 Siphon tinitx=1\nInitial state timing t=0 t=1 t=0 t=1\na₁ formula T × x₀ x₀ zeros zeros\nP₁ formula T × V₀ × T' + R × Q × R' V₀ T × V₀ × T' + R × Q × R' V₀\nEM update Optional Optional update_initial_state=true update_initial_state=true\n\nSiphon now supports both conventions via the tinitx parameter:\n\ntinitx=0 (default): Matches MARSS tinitx=0 behavior\ntinitx=1: Matches MARSS tinitx=1 behavior","category":"section"},{"location":"tutorials/transformations.html#Parameter-Transformations","page":"Parameter Transformations","title":"Parameter Transformations","text":"This tutorial explains how Siphon.jl handles parameter transformations for constrained optimization. Understanding this system is important for:\n\nCorrectly specifying parameter bounds\nInterpreting optimization results\nWorking with Bayesian inference\nImplementing custom models","category":"section"},{"location":"tutorials/transformations.html#Overview","page":"Parameter Transformations","title":"Overview","text":"State-space model parameters often have natural constraints:\n\nVariance parameters must be positive: sigma^2  0\nAR coefficients must satisfy stationarity: rho  1\nCorrelation parameters must be in -1 1\nProbabilities must be in 0 1\n\nSiphon.jl uses TransformVariables.jl to handle these constraints automatically. The key idea is to optimize in an unconstrained space mathbbR^n and transform to the constrained parameter space when needed.","category":"section"},{"location":"tutorials/transformations.html#How-It-Works","page":"Parameter Transformations","title":"How It Works","text":"","category":"section"},{"location":"tutorials/transformations.html#The-Two-Spaces","page":"Parameter Transformations","title":"The Two Spaces","text":"Constrained space (Theta): Where parameters have their natural interpretation\nExample: sigma^2_textobs = 2250 (a variance)\nUnconstrained space (mathbbR^n): Where optimization actually happens\nExample: theta_textunconstrained = 542 (the log of sigma^2)","category":"section"},{"location":"tutorials/transformations.html#Transformation-Flow","page":"Parameter Transformations","title":"Transformation Flow","text":"                    transform\nUnconstrained θ_u ─────────────> Constrained θ_c (NamedTuple)\n      ℝⁿ           (+ logjac)          Θ\n                                       │\n                                       ▼\n                              Build state-space matrices\n                                       │\n                                       ▼\n                              Compute log-likelihood\n\nThe optimizer works in unconstrained space. For each evaluation:\n\nTransform theta_u to theta_c (unconstrained to constrained)\nBuild state-space matrices using theta_c\nCompute log-likelihood\n(For Bayesian: add log Jacobian and log prior)","category":"section"},{"location":"tutorials/transformations.html#Specifying-Parameter-Bounds","page":"Parameter Transformations","title":"Specifying Parameter Bounds","text":"When you create a FreeParam, the bounds determine which transformation is applied:\n\n# Positive parameter (variance) - uses exp transform\nFreeParam(:var_obs, init=100.0, lower=0.0)\n\n# Bounded parameter (AR coefficient) - uses logit-like transform\nFreeParam(:ρ, init=0.8, lower=-0.99, upper=0.99)\n\n# Unbounded parameter - identity (no transform)\nFreeParam(:β, init=0.0)","category":"section"},{"location":"tutorials/transformations.html#Transformation-Rules","page":"Parameter Transformations","title":"Transformation Rules","text":"Bounds Transform Mathematical Form\n(-infty infty) Identity theta_c = theta_u\n0 infty) Exponential theta_c = exp(theta_u)\n(a b) Scaled logistic theta_c = a + (b-a) cdot textlogistic(theta_u)\n(-infty b Negative exp theta_c = b - exp(-theta_u)","category":"section"},{"location":"tutorials/transformations.html#For-Variance-Parameters","page":"Parameter Transformations","title":"For Variance Parameters","text":"Use lower=0.0 to ensure positivity:\n\n# Correct: estimate variance directly with positivity constraint\nH = [FreeParam(:var_obs, init=100.0, lower=0.0)]\n\nThe transformation automatically applied is:\n\nsigma^2 = exp(theta_u)\n\nSo if the optimizer finds theta_u = 46, you get sigma^2 = exp(46) approx 100.","category":"section"},{"location":"tutorials/transformations.html#Working-with-Transformations-Directly","page":"Parameter Transformations","title":"Working with Transformations Directly","text":"","category":"section"},{"location":"tutorials/transformations.html#Building-Transformations","page":"Parameter Transformations","title":"Building Transformations","text":"using Siphon\n\nspec = local_level()\nt = build_transformation(spec)\n\n# Transform from unconstrained to constrained\nθ_u = [4.6, 3.9]  # Unconstrained values\nθ_c = TransformVariables.transform(t, θ_u)\n# θ_c = (var_obs = 99.5, var_level = 49.4)","category":"section"},{"location":"tutorials/transformations.html#Transform-with-Jacobian","page":"Parameter Transformations","title":"Transform with Jacobian","text":"For Bayesian inference, you need the log Jacobian determinant:\n\nusing TransformVariables\n\nθ_c, logjac = transform_and_logjac(t, θ_u)\n# logjac accounts for the change of variables","category":"section"},{"location":"tutorials/transformations.html#Inverse-Transform","page":"Parameter Transformations","title":"Inverse Transform","text":"To go from constrained back to unconstrained:\n\nθ_u = transform_to_unconstrained(spec, θ_c)","category":"section"},{"location":"tutorials/transformations.html#Full-Covariance-Matrices","page":"Parameter Transformations","title":"Full Covariance Matrices","text":"For full positive-definite covariance matrices, Siphon.jl uses a special parameterization via cov_free:\n\nQ = cov_free(2, :Q)  # 2×2 PD covariance matrix\n\nThis creates:\n\nStandard deviation parameters: Q_σ_1, Q_σ_2 (positive, use exp transform)\nCorrelation parameters: Q_corr_1 (unconstrained, maps to valid correlation)\n\nThe matrix is reconstructed as:\n\nSigma = D cdot textCorr cdot D\n\nwhere D = textdiag(sigma_1 sigma_2) and textCorr is built from a Cholesky factor parameterization that guarantees positive definiteness.","category":"section"},{"location":"tutorials/transformations.html#Example:-Complete-Workflow","page":"Parameter Transformations","title":"Example: Complete Workflow","text":"using Siphon\n\n# Specify model with constrained parameters\nspec = custom_ssm(\n    Z = [1.0],\n    H = [FreeParam(:var_obs, init=100.0, lower=0.0)],  # Positive\n    T = [FreeParam(:ρ, init=0.8, lower=-0.99, upper=0.99)],  # Bounded\n    R = [1.0],\n    Q = [FreeParam(:var_state, init=50.0, lower=0.0)],  # Positive\n    a1 = [0.0],\n    P1 = [1e7]\n)\n\n# Simulate data\ny = randn(1, 200)\n\n# Optimize - all transformation handled automatically\nresult = optimize_ssm(spec, y)\n\n# Result is in constrained space\nprintln(\"var_obs = \", result.θ.var_obs)    # Positive value\nprintln(\"ρ = \", result.θ.ρ)                # In (-0.99, 0.99)\nprintln(\"var_state = \", result.θ.var_state)  # Positive value\n\n# If you need unconstrained values (e.g., for MCMC initialization)\nθ_u = transform_to_unconstrained(spec, result.θ)\nprintln(\"Unconstrained: \", θ_u)  # Values in ℝ³","category":"section"},{"location":"tutorials/transformations.html#Bayesian-Inference","page":"Parameter Transformations","title":"Bayesian Inference","text":"For Bayesian inference with MCMC, the log-density is computed in unconstrained space:\n\n# Create log-density object\nld = SSMLogDensity(spec, y)\n\n# Evaluate at unconstrained point\nθ_u = randn(n_params(spec))\nll = logdensity(ld, θ_u)\n# ll includes the log Jacobian automatically\n\nThe SSMLogDensity type implements LogDensityProblems.jl interface, so you can use it with any compatible sampler.","category":"section"},{"location":"tutorials/transformations.html#Tips-and-Best-Practices","page":"Parameter Transformations","title":"Tips and Best Practices","text":"Always use lower=0.0 for variances: This ensures the exp transform is applied.\nUse tight bounds for AR coefficients: lower=-0.99, upper=0.99 works better than (-1, 1) numerically.\nInitial values matter: Provide good initial values in the constrained space. They are automatically transformed.\nStandard errors are approximate: When using optimize_ssm_with_stderr, the standard errors are computed via the delta method and may be approximate for highly nonlinear transformations.\nFor debugging: Use transform_to_unconstrained and transform_to_constrained to verify parameter values at each stage.","category":"section"},{"location":"tutorials/transformations.html#API-Reference","page":"Parameter Transformations","title":"API Reference","text":"","category":"section"},{"location":"tutorials/transformations.html#Key-Functions","page":"Parameter Transformations","title":"Key Functions","text":"# Build transformation from spec\nt = build_transformation(spec)\n\n# Transform unconstrained → constrained\nθ_c, logjac = transform_to_constrained(spec, θ_u)\n\n# Transform constrained → unconstrained\nθ_u = transform_to_unconstrained(spec, θ_c)","category":"section"},{"location":"tutorials/transformations.html#Related-Types","page":"Parameter Transformations","title":"Related Types","text":"FreeParam: Specify a free parameter with bounds\nSSMLogDensity: Log-density in unconstrained space\nCovMatrixExpr: Expression for positive-definite covariance matrices","category":"section"},{"location":"tutorials/arma_models.html#ARMA-Models","page":"ARMA Models","title":"ARMA Models","text":"This tutorial demonstrates how to specify and estimate ARMA(p,q) models in state-space form using Siphon.jl. We cover:\n\nTheoretical background on ARMA in state-space form\nBuilding an ARMA(2,2) model from scratch using custom_ssm\nUsing the built-in arma template\nEstimation with MLE and EM\nAccessing estimated parameters and matrices\nValidation against known parameters","category":"section"},{"location":"tutorials/arma_models.html#Background:-ARMA-in-State-Space-Form","page":"ARMA Models","title":"Background: ARMA in State-Space Form","text":"An ARMA(p,q) process is defined as:\n\ny_t = phi_1 y_t-1 + cdots + phi_p y_t-p + varepsilon_t + theta_1 varepsilon_t-1 + cdots + theta_q varepsilon_t-q\n\nwhere varepsilon_t sim N(0 sigma^2) is white noise.\n\nThis can be written in innovations state-space form with state dimension r = max(p q+1):\n\nState vector:\n\nalpha_t = beginpmatrix y_t  y_t+1t  vdots  y_t+r-1t endpmatrix\n\nwhere y_t+jt = Ey_t+j  y_1 ldots y_t.\n\nObservation equation:\n\ny_t = Z alpha_t quad Z = beginpmatrix 1  0  cdots  0 endpmatrix\n\nTransition equation:\n\nalpha_t+1 = T alpha_t + R eta_t quad eta_t sim N(0 Q)\n\nwhere:\n\nT = beginpmatrix\nphi_1  1  0  cdots  0 \nphi_2  0  1  cdots  0 \nvdots  vdots  vdots  ddots  vdots \nphi_r-1  0  0  cdots  1 \nphi_r  0  0  cdots  0\nendpmatrix quad\nR = beginpmatrix 1  theta_1  theta_2  vdots  theta_r-1 endpmatrix quad\nQ = sigma^2\n\nNote: phi_i = 0 for i  p and theta_j = 0 for j  q.","category":"section"},{"location":"tutorials/arma_models.html#Building-ARMA(2,2)-from-Scratch","page":"ARMA Models","title":"Building ARMA(2,2) from Scratch","text":"Let's build an ARMA(2,2) model step by step using custom_ssm:\n\nusing Siphon\nusing LinearAlgebra\nusing Random\n\nRandom.seed!(42)\n\n# ARMA(2,2): r = max(2, 2+1) = 3\np, q = 2, 2\nr = max(p, q + 1)  # = 3\n\n# Observation matrix: Z = [1 0 0]\nZ = zeros(1, r)\nZ[1, 1] = 1.0\n\n# Observation noise: H = 0 (pure ARMA, no measurement error)\nH = zeros(1, 1)\n\n# Transition matrix: companion form\n# T = [φ₁  1  0]\n#     [φ₂  0  1]\n#     [0   0  0]  (φ₃ = 0 since p=2)\nT = zeros(r, r)\nT[1, 1] = FreeParam(:φ1, init=0.5, lower=-0.99, upper=0.99)\nT[2, 1] = FreeParam(:φ2, init=0.2, lower=-0.99, upper=0.99)\n# Subdiagonal ones\nT[1, 2] = 1.0\nT[2, 3] = 1.0\n\n# Selection matrix: R = [1; θ₁; θ₂]\nR = zeros(r, 1)\nR[1, 1] = 1.0\nR[2, 1] = FreeParam(:θ1, init=0.3)\nR[3, 1] = FreeParam(:θ2, init=0.1)\n\n# Innovation variance: Q = σ²\nQ = [FreeParam(:var, init=1.0, lower=0.0)]\n\n# Initial state (diffuse)\na1 = zeros(r)\nP1 = 1e7 * Matrix(1.0I, r, r)\n\n# Build the specification\nspec_manual = custom_ssm(\n    Z = Z,\n    H = H,\n    T = T,\n    R = R,\n    Q = Q,\n    a1 = a1,\n    P1 = P1,\n    name = :ARMA22_manual\n)\n\nprintln(\"Model: \", spec_manual.name)\nprintln(\"State dimension: \", spec_manual.n_states)\nprintln(\"Parameters: \", param_names(spec_manual))\n\nOutput:\n\nModel: ARMA22_manual\nState dimension: 3\nParameters: [:φ1, :φ2, :θ1, :θ2, :var]","category":"section"},{"location":"tutorials/arma_models.html#Using-Matrix-Helpers","page":"ARMA Models","title":"Using Matrix Helpers","text":"We can simplify the construction using matrix helpers:\n\nusing Siphon\n\np, q = 2, 2\nr = max(p, q + 1)\n\n# Using helper functions\nspec_helpers = custom_ssm(\n    Z = [1.0 0.0 0.0],                    # First state observed\n    H = zeros_mat(1, 1),                   # No observation noise\n    T = [FreeParam(:φ1, init=0.5, lower=-0.99, upper=0.99)  1.0  0.0;\n         FreeParam(:φ2, init=0.2, lower=-0.99, upper=0.99)  0.0  1.0;\n         0.0                                                 0.0  0.0],\n    R = [1.0;\n         FreeParam(:θ1, init=0.3);\n         FreeParam(:θ2, init=0.1)],\n    Q = scalar_free(:var; init=1.0),\n    a1 = [0.0, 0.0, 0.0],\n    P1 = 1e7 * identity_mat(3),\n    name = :ARMA22_helpers\n)","category":"section"},{"location":"tutorials/arma_models.html#Using-the-Built-in-ARMA-Template","page":"ARMA Models","title":"Using the Built-in ARMA Template","text":"Siphon.jl provides the arma template that handles all this automatically:\n\nusing Siphon\n\n# Create ARMA(2,2) specification\nspec_template = arma(2, 2; ar_init=[0.5, 0.2], ma_init=[0.3, 0.1], var_init=1.0)\n\nprintln(\"Model: \", spec_template.name)\nprintln(\"State dimension: \", spec_template.n_states)\nprintln(\"Parameters: \", param_names(spec_template))\n\nThe template creates parameters named φ1, φ2 (AR coefficients), θ1, θ2 (MA coefficients), and var (innovation variance).","category":"section"},{"location":"tutorials/arma_models.html#Simulating-and-Estimating","page":"ARMA Models","title":"Simulating and Estimating","text":"Let's simulate data from a known ARMA(2,2) process and estimate the parameters:\n\nusing Siphon\nusing Random\nusing Statistics\n\nRandom.seed!(123)\n\n# True parameters\nφ1_true, φ2_true = 0.7, -0.2\nθ1_true, θ2_true = 0.4, 0.1\nvar_true = 1.5\n\n# Simulate ARMA(2,2) data\nfunction simulate_arma22(n, φ1, φ2, θ1, θ2, σ²)\n    y = zeros(n)\n    ε = sqrt(σ²) * randn(n + 100)  # Pre-sample for burn-in\n\n    for t in 3:(n + 100)\n        y_idx = t - 100\n        if y_idx >= 1\n            y[y_idx] = (t >= 3 ? φ1 * y[max(1, y_idx-1)] : 0.0) +\n                       (t >= 4 ? φ2 * y[max(1, y_idx-2)] : 0.0) +\n                       ε[t] + θ1 * ε[t-1] + θ2 * ε[t-2]\n        end\n    end\n    return y\nend\n\nn = 500\ny_sim = simulate_arma22(n, φ1_true, φ2_true, θ1_true, θ2_true, var_true)\n\n# Reshape for Siphon (expects p × n matrix)\ny = reshape(y_sim, 1, n)\n\n# Create and fit model with MLE\nspec = arma(2, 2; ar_init=[0.5, 0.0], ma_init=[0.0, 0.0], var_init=1.0)\nmodel = StateSpaceModel(spec, n)\nfit!(MLE(), model, y)\n\n# Access estimated parameters\nparams = parameters(model)\nprintln(\"\\nEstimated parameters (MLE):\")\nprintln(\"  φ1: \", round(params.φ1, digits=4), \" (true: $φ1_true)\")\nprintln(\"  φ2: \", round(params.φ2, digits=4), \" (true: $φ2_true)\")\nprintln(\"  θ1: \", round(params.θ1, digits=4), \" (true: $θ1_true)\")\nprintln(\"  θ2: \", round(params.θ2, digits=4), \" (true: $θ2_true)\")\nprintln(\"  var: \", round(params.var, digits=4), \" (true: $var_true)\")\nprintln(\"\\nLog-likelihood: \", round(loglikelihood(model), digits=2))","category":"section"},{"location":"tutorials/arma_models.html#Accessing-Fitted-Matrices","page":"ARMA Models","title":"Accessing Fitted Matrices","text":"After fitting, you can access the system matrices:\n\n# Get all matrices at once (efficient)\nmats = system_matrices(model)\n\nprintln(\"\\nFitted matrices:\")\nprintln(\"Z (observation):\")\ndisplay(mats.Z)\n\nprintln(\"\\nT (transition):\")\ndisplay(mats.T)\n\nprintln(\"\\nR (selection):\")\ndisplay(mats.R)\n\nprintln(\"\\nQ (state covariance):\")\ndisplay(mats.Q)\n\n# Or access individually\nZ = obs_matrix(model)\nH = obs_cov(model)\nT_mat = transition_matrix(model)\nR_mat = selection_matrix(model)\nQ_mat = state_cov(model)","category":"section"},{"location":"tutorials/arma_models.html#Parameter-Vector-vs-NamedTuple","page":"ARMA Models","title":"Parameter Vector vs NamedTuple","text":"After estimation, parameters are available in two forms:\n\n# As NamedTuple (recommended - clear names)\nparams = parameters(model)\nprintln(\"φ1 = \", params.φ1)\nprintln(\"θ1 = \", params.θ1)\n\n# As Vector (internal representation, same order as param_names(spec))\nθ_vec = model.theta_values\nnames = param_names(spec)\nfor (name, val) in zip(names, θ_vec)\n    println(\"$name = $val\")\nend","category":"section"},{"location":"tutorials/arma_models.html#Comparison:-MLE-vs-EM","page":"ARMA Models","title":"Comparison: MLE vs EM","text":"For ARMA models, both MLE and EM estimation are available:\n\n# MLE estimation (gradient-based optimization)\nmodel_mle = StateSpaceModel(spec, n)\nfit!(MLE(), model_mle, y)\n\n# EM estimation (iterative)\nmodel_em = StateSpaceModel(spec, n)\nfit!(EM(), model_em, y; maxiter=200, verbose=false)\n\nprintln(\"\\nComparison (MLE vs EM):\")\nparams_mle = parameters(model_mle)\nparams_em = parameters(model_em)\n\nfor name in param_names(spec)\n    mle_val = getproperty(params_mle, name)\n    em_val = getproperty(params_em, name)\n    println(\"  $name: MLE=$(round(mle_val, digits=4)), EM=$(round(em_val, digits=4))\")\nend\n\nprintln(\"\\nLog-likelihoods:\")\nprintln(\"  MLE: \", round(loglikelihood(model_mle), digits=2))\nprintln(\"  EM:  \", round(loglikelihood(model_em), digits=2))","category":"section"},{"location":"tutorials/arma_models.html#Complete-Working-Example","page":"ARMA Models","title":"Complete Working Example","text":"Here is a complete, self-contained example:\n\nusing Siphon\nusing Random\nusing LinearAlgebra\n\nRandom.seed!(42)\n\n# 1. Generate synthetic ARMA(2,2) data\nn = 300\nε = randn(n + 10)\ny = zeros(n)\nφ1, φ2 = 0.6, -0.15\nθ1, θ2 = 0.3, 0.1\nσ² = 2.0\n\nfor t in 3:n\n    y[t] = φ1 * y[t-1] + φ2 * y[t-2] + sqrt(σ²) * ε[t+10] +\n           θ1 * sqrt(σ²) * ε[t+9] + θ2 * sqrt(σ²) * ε[t+8]\nend\ny_mat = reshape(y, 1, n)\n\n# 2. Create model specification\nspec = arma(2, 2)\n\n# 3. Create and fit model\nmodel = StateSpaceModel(spec, n)\nfit!(MLE(), model, y_mat)\n\n# 4. Results\nprintln(\"Fitted ARMA(2,2) parameters:\")\nparams = parameters(model)\nprintln(\"  φ1 = \", round(params.φ1, digits=4))\nprintln(\"  φ2 = \", round(params.φ2, digits=4))\nprintln(\"  θ1 = \", round(params.θ1, digits=4))\nprintln(\"  θ2 = \", round(params.θ2, digits=4))\nprintln(\"  var = \", round(params.var, digits=4))\nprintln(\"\\nLog-likelihood: \", round(loglikelihood(model), digits=2))\n\n# 5. Get smoothed states\nα = smoothed_states(model)\nprintln(\"\\nSmoothed state dimension: \", size(α))\n\n# 6. Get system matrices\nmats = system_matrices(model)\nprintln(\"\\nTransition matrix T:\")\ndisplay(round.(mats.T, digits=4))","category":"section"},{"location":"tutorials/arma_models.html#Tips-and-Best-Practices","page":"ARMA Models","title":"Tips and Best Practices","text":"Initial values matter: ARMA models can have multiple local optima. Good initial values help find the global optimum.\nStationarity: The arma template does not enforce stationarity constraints on AR coefficients. For guaranteed stationarity, use bounds like lower=-0.99, upper=0.99 for AR coefficients.\nInvertibility: Similarly, MA coefficients are not constrained for invertibility. Check your fitted coefficients satisfy the invertibility conditions.\nModel order selection: Use information criteria (AIC, BIC) computed from the log-likelihood to select appropriate (p,q) values.\nEM vs MLE: MLE is generally faster for ARMA models. EM can be useful when there are many missing observations.","category":"section"},{"location":"tutorials/arma_models.html#Next-Steps","page":"ARMA Models","title":"Next Steps","text":"Learn about Custom Models for more complex specifications\nSee Parameter Transformations for constrained optimization\nExplore Dynamic Factor Models for multivariate time series","category":"section"},{"location":"api/dsl.html#DSL-and-Templates","page":"DSL & Templates","title":"DSL & Templates","text":"This page documents the domain-specific language for model specification and pre-built templates.","category":"section"},{"location":"api/dsl.html#Core-Types","page":"DSL & Templates","title":"Core Types","text":"","category":"section"},{"location":"api/dsl.html#Model-Introspection","page":"DSL & Templates","title":"Model Introspection","text":"","category":"section"},{"location":"api/dsl.html#Model-Building","page":"DSL & Templates","title":"Model Building","text":"","category":"section"},{"location":"api/dsl.html#Pre-Built-Templates","page":"DSL & Templates","title":"Pre-Built Templates","text":"","category":"section"},{"location":"api/dsl.html#Local-Level-Model","page":"DSL & Templates","title":"Local Level Model","text":"","category":"section"},{"location":"api/dsl.html#Local-Linear-Trend","page":"DSL & Templates","title":"Local Linear Trend","text":"","category":"section"},{"location":"api/dsl.html#AR(1)-Model","page":"DSL & Templates","title":"AR(1) Model","text":"","category":"section"},{"location":"api/dsl.html#ARMA-Model","page":"DSL & Templates","title":"ARMA Model","text":"","category":"section"},{"location":"api/dsl.html#Dynamic-Factor-Model","page":"DSL & Templates","title":"Dynamic Factor Model","text":"","category":"section"},{"location":"api/dsl.html#Custom-Model-Specification","page":"DSL & Templates","title":"Custom Model Specification","text":"","category":"section"},{"location":"api/dsl.html#Parameter-Expressions","page":"DSL & Templates","title":"Parameter Expressions","text":"","category":"section"},{"location":"api/dsl.html#DNS/Svensson-Yield-Curve-Helpers","page":"DSL & Templates","title":"DNS/Svensson Yield Curve Helpers","text":"","category":"section"},{"location":"api/dsl.html#Siphon.DSL.SSMParameter","page":"DSL & Templates","title":"Siphon.DSL.SSMParameter","text":"SSMParameter{T<:Real}\n\nRepresents a single estimable parameter in a state-space model.\n\nFields\n\nname::Symbol: Parameter name for identification\nlower::T: Lower bound for optimization (use -Inf for unbounded)\nupper::T: Upper bound for optimization (use Inf for unbounded)\ninit::T: Initial value for optimization\n\n\n\n\n\n","category":"type"},{"location":"api/dsl.html#Siphon.DSL.SSMSpec","page":"DSL & Templates","title":"Siphon.DSL.SSMSpec","text":"SSMSpec\n\nComplete specification of a state-space model.\n\nFields\n\nname::Symbol: Model name\nn_states::Int: Number of state variables\nn_obs::Int: Number of observables\nn_shocks::Int: Number of shocks\nparams::Vector{SSMParameter}: Free parameters to estimate\nZ::SSMMatrixSpec: Observation matrix specification\nH::SSMMatrixSpec: Observation covariance specification\nT::SSMMatrixSpec: Transition matrix specification\nR::SSMMatrixSpec: Selection matrix specification\nQ::SSMMatrixSpec: State covariance specification\na1::Vector{MatrixElement}: Initial state mean\nP1::SSMMatrixSpec: Initial state covariance (finite part for exact diffuse)\nP1_inf::Union{Nothing,SSMMatrixSpec}: Diffuse covariance (nothing = approximate diffuse)\nmatrix_exprs::Dict{Symbol,Any}: Expression-based matrices (for DNS, etc.)\n\nWhen P1_inf is not nothing, exact diffuse initialization is used:\n\nP1 becomes P1_star (finite part)\nP1_inf is the diffuse/infinite part\n\nWhen P1_inf is nothing, approximate diffuse is used (P1 is the full initial covariance).\n\n\n\n\n\n","category":"type"},{"location":"api/dsl.html#Siphon.DSL.FixedValue","page":"DSL & Templates","title":"Siphon.DSL.FixedValue","text":"FixedValue{T}\n\nRepresents a fixed (non-estimable) value in a state-space model.\n\n\n\n\n\n","category":"type"},{"location":"api/dsl.html#Siphon.DSL.ParameterRef","page":"DSL & Templates","title":"Siphon.DSL.ParameterRef","text":"ParameterRef\n\nReference to a parameter by name, used for building matrix mappings.\n\n\n\n\n\n","category":"type"},{"location":"api/dsl.html#Siphon.DSL.param_names","page":"DSL & Templates","title":"Siphon.DSL.param_names","text":"param_names(spec::SSMSpec)\n\nReturn the names of all free parameters.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.n_params","page":"DSL & Templates","title":"Siphon.DSL.n_params","text":"n_params(spec::SSMSpec)\n\nReturn the number of free parameters.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.initial_values","page":"DSL & Templates","title":"Siphon.DSL.initial_values","text":"initial_values(spec::SSMSpec)\n\nReturn a vector of initial parameter values.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.param_bounds","page":"DSL & Templates","title":"Siphon.DSL.param_bounds","text":"param_bounds(spec::SSMSpec)\n\nReturn lower and upper bound vectors.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.build_linear_state_space","page":"DSL & Templates","title":"Siphon.DSL.build_linear_state_space","text":"build_linear_state_space(spec::SSMSpec, θ, y; use_static=true)\n\nwarning: Deprecated\nThis function is deprecated. Use StateSpaceModel(spec, θ, n) instead for a unified API:# Old way (deprecated)\nss = build_linear_state_space(spec, θ, y)\nll = kalman_loglik(ss.p, y, ss.a1, ss.P1)\n\n# New way (recommended)\nmodel = StateSpaceModel(spec, θ, size(y, 2))\nll = kalman_loglik(model, y)\n\nBuild state-space model components from specification.\n\nAccepts either a Vector or NamedTuple of parameters.\n\nArguments\n\nspec::SSMSpec: Model specification\nθ: Parameters (Vector or NamedTuple)\ny: Observations (used for dimension inference in some models)\nuse_static::Bool=true: If true, automatically convert small matrices (dimensions ≤ 13) to StaticArrays for better performance\n\nReturns a NamedTuple with:\n\np: KFParms struct with system matrices (Z, H, T, R, Q)\na1: Initial state mean vector\nP1: Initial state covariance matrix\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.ssm_loglik","page":"DSL & Templates","title":"Siphon.DSL.ssm_loglik","text":"ssm_loglik(spec::SSMSpec, θ::NamedTuple, y::AbstractMatrix)\n\nCompute log-likelihood directly from spec and NamedTuple parameters.\n\nThis is the recommended high-level API for likelihood evaluation.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.objective_function","page":"DSL & Templates","title":"Siphon.DSL.objective_function","text":"objective_function(spec::SSMSpec, y)\n\nCreate a negative log-likelihood function for optimization.\n\nReturns a callable that computes -loglik for a given parameter vector theta. Uses the AD-compatible kalman_loglik function.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.local_level","page":"DSL & Templates","title":"Siphon.DSL.local_level","text":"local_level(; var_obs=:free, var_level=:free, diffuse=true)\n\nCreate specification for local level (random walk + noise) model.\n\nModel:     yₜ = μₜ + εₜ,  εₜ ~ N(0, varobs)     μₜ₊₁ = μₜ + ηₜ,  ηₜ ~ N(0, varlevel)\n\nArguments\n\nvar_obs: Observation noise variance. Use :free to estimate, a number to fix,            or (init=, lower=, upper=) for custom estimation settings.\nvar_level: Level noise variance. Same options as var_obs.\ndiffuse: Initialization mode:\ntrue (default): Approximate diffuse (P1 = 1e7)\nfalse: Non-diffuse (P1 = small value)\n:exact: Exact diffuse initialization (P1star=0, P1inf=I)\n\nExamples\n\n# Estimate both (default)\nspec = local_level()\n\n# Fix observation variance, estimate level variance\nspec = local_level(var_obs=225.0, var_level=:free)\n\n# Use exact diffuse initialization\nspec = local_level(diffuse=:exact)\n\n# Custom initial value and bounds\nspec = local_level(var_obs=(init=225.0, lower=1.0, upper=10000.0))\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.local_linear_trend","page":"DSL & Templates","title":"Siphon.DSL.local_linear_trend","text":"local_linear_trend(; var_obs=:free, var_level=:free, var_slope=:free, diffuse=true)\n\nCreate specification for local linear trend model.\n\nModel:     yₜ = μₜ + εₜ,  εₜ ~ N(0, varobs)     μₜ₊₁ = μₜ + νₜ + ηₜ,  ηₜ ~ N(0, varlevel)     νₜ₊₁ = νₜ + ζₜ,  ζₜ ~ N(0, var_slope)\n\nState: [μₜ, νₜ]\n\nArguments\n\nvar_obs, var_level, var_slope: Use :free to estimate, a number to fix, or (init=, lower=, upper=) for custom settings.\ndiffuse: Initialization mode:\ntrue (default): Approximate diffuse (P1 = 1e7*I)\nfalse: Non-diffuse (P1 = 1e4*I)\n:exact: Exact diffuse initialization (P1star=0, P1inf=I)\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.ar1","page":"DSL & Templates","title":"Siphon.DSL.ar1","text":"ar1(; ρ=:free, var_obs=:free, var_state=:free, diffuse=true)\n\nCreate specification for AR(1) plus noise model.\n\nModel:     yₜ = xₜ + εₜ,  εₜ ~ N(0, varobs)     xₜ₊₁ = ρ xₜ + ηₜ,  ηₜ ~ N(0, varstate)\n\nArguments\n\nρ: AR coefficient (|ρ| < 1). Use :free, a number, or (init=, lower=, upper=).\nvar_obs, var_state: Noise variances. Same options.\ndiffuse: Initialization mode:\ntrue (default): Approximate diffuse (P1 = 1e7)\nfalse: Non-diffuse (P1 = 1e4)\n:exact: Exact diffuse initialization (P1star=0, P1inf=I)\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.arma","page":"DSL & Templates","title":"Siphon.DSL.arma","text":"arma(p::Int, q::Int; ar_init=nothing, ma_init=nothing, var_init=1.0, diffuse=true)\n\nCreate specification for ARMA(p,q) model in state-space form.\n\nUses the innovations state-space representation.\n\nParameters: ar coefficients φ₁...φₚ, ma coefficients θ₁...θᵧ, var (innovation variance)\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.dynamic_factor","page":"DSL & Templates","title":"Siphon.DSL.dynamic_factor","text":"dynamic_factor(n_obs, n_factors; factor_lags=1, obs_lags=0, correlated_errors=false,\n               loadings_init=0.5, ar_init=0.5, var_obs_init=1.0,\n               var_factor_init=1.0, diffuse=true)\n\nCreate specification for a dynamic factor model with VAR factor dynamics and optional lagged factor loadings.\n\nModel:     yₜ = Λ₀ fₜ + Λ₁ fₜ₋₁ + ... + Λₛ fₜ₋ₛ + εₜ,  εₜ ~ N(0, H)     fₜ = Φ₁ fₜ₋₁ + Φ₂ fₜ₋₂ + ... + Φₚ fₜ₋ₚ + ηₜ,  ηₜ ~ N(0, Q)\n\nwhere:\n\nyₜ is n_obs × 1 vector of observations\nfₜ is n_factors × 1 vector of latent factors\nΛₗ is nobs × nfactors factor loadings matrix for lag l (l = 0, ..., s)\nΦₗ is nfactors × nfactors AR coefficient matrix for lag l (diagonal)\nH is observation error covariance (diagonal or full if correlated_errors=true)\nQ is factor innovation covariance (diagonal)\ns = obslags, p = factorlags\n\nThe model is cast in companion form with state vector:     αₜ = [fₜ', fₜ₋₁', ..., fₜ₋ₘ₊₁']'  where m = max(p, s+1)\n\nIdentification\n\nFor identification, the first n_factors rows of Λ₀ (contemporaneous loadings) are set to an identity matrix:\n\nλ₀{i,i} = 1 for i ≤ nfactors\nλ₀{i,j} = 0 for i < j ≤ nfactors\n\nAll other loadings (including lagged) are free parameters.\n\nArguments\n\nn_obs::Int: Number of observable variables\nn_factors::Int: Number of latent factors (must be < n_obs)\nfactor_lags::Int=1: Number of lags in factor VAR dynamics (p)\nobs_lags::Int=0: Number of lagged factors that load onto observations (s).                    If s > 0, observations depend on fₜ, fₜ₋₁, ..., fₜ₋ₛ.\ncorrelated_errors::Bool=false: If true, H is a full covariance matrix (inexact DFM).                                  If false, H is diagonal (exact DFM).\nloadings_init::Real=0.5: Initial value for free factor loadings\nar_init::Real=0.5: Initial value for AR coefficients (divided by lag number)\nvar_obs_init::Real=1.0: Initial value for observation error variances\nvar_factor_init::Real=1.0: Initial value for factor innovation variances\ndiffuse::Bool=true: Use diffuse initialization for factors\n\nState Space Representation\n\nState dimension is n_factors * max(factor_lags, obs_lags + 1).\n\nState vector: αₜ = [fₜ', fₜ₋₁', ..., fₜ₋ₘ₊₁']'\n\nTransition matrix (companion form):     T = [Φ₁  Φ₂  ... Φₚ  0  ... 0]   (padded with zeros if m > p)         [I   0   ... 0   0  ... 0]         [0   I   ... 0   0  ... 0]         [⋮   ⋮   ⋱   ⋮   ⋮  ⋱   ⋮]         [0   0   ... I   0  ... 0]\n\nObservation matrix: Z = [Λ₀  Λ₁  ...  Λₛ  0  ...  0]\n\nExample\n\n# Standard 1-factor model (no lagged loadings)\nspec = dynamic_factor(4, 1)\n\n# 1-factor model with 2 factor lags, no lagged loadings\nspec = dynamic_factor(4, 1; factor_lags=2)\n# yₜ = Λ₀ fₜ + εₜ\n# fₜ = φ₁ fₜ₋₁ + φ₂ fₜ₋₂ + ηₜ\n\n# 1-factor model with lagged loadings (s=1)\nspec = dynamic_factor(4, 1; factor_lags=2, obs_lags=1)\n# yₜ = Λ₀ fₜ + Λ₁ fₜ₋₁ + εₜ\n# fₜ = φ₁ fₜ₋₁ + φ₂ fₜ₋₂ + ηₜ\n# Parameters: λ0_i_j (contemporaneous), λ1_i_j (lag 1 loadings)\n\n# 2-factor model with obs_lags=2, factor_lags=1\nspec = dynamic_factor(5, 2; factor_lags=1, obs_lags=2)\n# yₜ = Λ₀ fₜ + Λ₁ fₜ₋₁ + Λ₂ fₜ₋₂ + εₜ\n# State: [f₁ₜ, f₂ₜ, f₁ₜ₋₁, f₂ₜ₋₁, f₁ₜ₋₂, f₂ₜ₋₂]\n\nParameters Created\n\nλ0_i_j: Contemporaneous loadings (i > j for identification in first k rows)\nλl_i_j: Loadings for lag l (l = 1, ..., obs_lags), all free\nφ_i_l: AR coefficient for factor i at lag l (l = 1, ..., factor_lags)\nvar_obs_i: Observation error variances (if correlated_errors=false)\nH_σ_i, H_corr_i: Covariance parameters (if correlated_errors=true, σ for DCorrD)\nvar_factor_i: Factor innovation variances\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.custom_ssm","page":"DSL & Templates","title":"Siphon.DSL.custom_ssm","text":"custom_ssm(; Z, H, T, R, Q, a1, P1, name=:CustomSSM) -> SSMSpec\n\nCreate a state-space model specification from explicit matrices.\n\nUse FreeParam(:name, init=val, lower=lb, upper=ub) to mark elements for estimation. Use regular numbers for fixed values.\n\nArguments\n\nZ: Observation matrix (p × m)\nH: Observation covariance (p × p)\nT: Transition matrix (m × m)\nR: Selection matrix (m × r)\nQ: State covariance (r × r)\na1: Initial state mean (m-vector)\nP1: Initial state covariance (m × m)\nname: Model name (default: :CustomSSM)\n\nExample\n\n# Local level model\nspec = custom_ssm(\n    Z = [1.0],\n    H = [FreeParam(:var_obs, init=225.0, lower=0.0)],\n    T = [1.0],\n    R = [1.0],\n    Q = [FreeParam(:var_level, init=100.0, lower=0.0)],\n    a1 = [0.0],\n    P1 = [1e7]\n)\nparam_names(spec)  # [:var_obs, :var_level]\n\nSee also: local_level, ar1, arma, dynamic_factor for pre-built templates.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.FreeParam","page":"DSL & Templates","title":"Siphon.DSL.FreeParam","text":"FreeParam(name; init=0.0, lower=-Inf, upper=Inf)\n\nMark a matrix element as a free parameter to be estimated.\n\nArguments\n\nname::Symbol: Parameter name\ninit: Initial value for optimization\nlower, upper: Bounds for optimization (TransformVariables.jl handles transformations)\n\nExample\n\n# A variance parameter (estimate variance directly, lower=0 triggers asℝ₊)\nH = [FreeParam(:var_obs, init=100.0, lower=0.0)]\n\n# A coefficient with bounds\nT = [FreeParam(:ρ, init=0.8, lower=-0.99, upper=0.99)]\n\n\n\n\n\n","category":"type"},{"location":"api/dsl.html#Siphon.DSL.ParamExpr","page":"DSL & Templates","title":"Siphon.DSL.ParamExpr","text":"ParamExpr(params, data, expr)\n\nA matrix element that is a function of parameters and external data.\n\nArguments\n\nparams: Symbol or tuple of Symbols for parameter names\ndata: NamedTuple of external data used in the expression\nexpr: Function (param_values..., data...) -> scalar\n\nExample: Nelson-Siegel loading\n\nτ = 30  # maturity in months\n# Loading: (1 - exp(-τ*λ)) / (τ*λ)\nelem = ParamExpr(:λ, (τ=τ,), (λ, τ) -> (1 - exp(-τ*λ)) / (τ*λ))\n\n\n\n\n\n","category":"type"},{"location":"api/dsl.html#Siphon.DSL.MatrixExpr","page":"DSL & Templates","title":"Siphon.DSL.MatrixExpr","text":"MatrixExpr(params, data, builder)\n\nA full matrix that is built from parameters and external data.\n\nArguments\n\nparams: Vector of SSMParameter specs for parameters used\ndata: NamedTuple of external data\nbuilder: Function (θ_dict, data) -> Matrix where θ_dict maps param names to values\n\nExample: Dynamic Nelson-Siegel factor loadings\n\nmaturities = [3, 6, 12, 24, 60, 120]  # months\n\nfunction dns_loadings(θ, data)\n    λ = θ[:λ]\n    τ = data.maturities\n    p = length(τ)\n    Z = ones(p, 3)\n    for i in 1:p\n        x = τ[i] * λ\n        Z[i, 2] = (1 - exp(-x)) / x\n        Z[i, 3] = Z[i, 2] - exp(-x)\n    end\n    Z\nend\n\nZ = MatrixExpr(\n    [SSMParameter(:λ, init=0.0609, lower=0.001, upper=1.0)],\n    (maturities=maturities,),\n    dns_loadings\n)\n\n\n\n\n\n","category":"type"},{"location":"api/dsl.html#Siphon.DSL.build_dns_loadings","page":"DSL & Templates","title":"Siphon.DSL.build_dns_loadings","text":"build_dns_loadings(maturities; λ_init=0.0609, λ_lower=0.001, λ_upper=1.0)\n\nCreate a MatrixExpr for Dynamic Nelson-Siegel factor loadings.\n\nArguments\n\nmaturities: Vector of maturities (e.g., in months)\nλ_init, λ_lower, λ_upper: Parameter settings for decay rate λ\n\nReturns\n\nA MatrixExpr that builds the p×3 loading matrix Z where:\n\nColumn 1: Level factor (all 1s)\nColumn 2: Slope factor (1 - exp(-λτ)) / (λτ)\nColumn 3: Curvature factor (1 - exp(-λτ)) / (λτ) - exp(-λτ)\n\nExample\n\nZ = build_dns_loadings([3, 6, 12, 24, 60, 120])\n\nspec = custom_ssm(\n    Z = Z,\n    H = diag_free(6, :var_obs, init=0.01),\n    T = [FreeParam(:φ_L, init=0.99, lower=0.0, upper=0.9999) 0.0 0.0;\n         0.0 FreeParam(:φ_S, init=0.99, lower=0.0, upper=0.9999) 0.0;\n         0.0 0.0 FreeParam(:φ_C, init=0.99, lower=0.0, upper=0.9999)],\n    R = identity_mat(3),\n    Q = diag_free([:var_L, :var_S, :var_C], init=0.01),\n    a1 = [0.0, 0.0, 0.0],\n    P1 = 1e4 * identity_mat(3)\n)\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.build_svensson_loadings","page":"DSL & Templates","title":"Siphon.DSL.build_svensson_loadings","text":"build_svensson_loadings(maturities; λ1_init=0.0609, λ2_init=0.03, ...)\n\nCreate a MatrixExpr for Svensson (4-factor) yield curve loadings.\n\nAdds a second curvature factor with separate decay rate λ2.\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.dns_loading1","page":"DSL & Templates","title":"Siphon.DSL.dns_loading1","text":"dns_loading1(λ, τ)\n\nFirst slope loading for Dynamic Nelson-Siegel: (1 - exp(-λτ)) / (λτ)\n\n\n\n\n\n","category":"function"},{"location":"api/dsl.html#Siphon.DSL.dns_loading2","page":"DSL & Templates","title":"Siphon.DSL.dns_loading2","text":"dns_loading2(λ, τ)\n\nCurvature loading for Dynamic Nelson-Siegel: (1 - exp(-λτ)) / (λτ) - exp(-λτ)\n\n\n\n\n\n","category":"function"},{"location":"tutorials/custom_models.html#Custom-Models","page":"Custom Models","title":"Custom Models","text":"This tutorial covers how to specify custom state space models using Siphon.jl's domain-specific language (DSL). You'll learn:\n\nThe custom_ssm function for specifying arbitrary models\nUsing FreeParam to mark estimated parameters\nMatrix helper functions for common patterns\nParameter-dependent matrices with MatrixExpr\nFull positive-definite covariance matrices with cov_free","category":"section"},{"location":"tutorials/custom_models.html#The-custom_ssm-Function","page":"Custom Models","title":"The custom_ssm Function","text":"The custom_ssm function lets you specify any linear state space model by providing the system matrices directly:\n\nusing Siphon\nusing LinearAlgebra\n\n# A simple local level model specified manually\nspec = custom_ssm(\n    Z = [1.0],                                    # Observation matrix\n    H = [FreeParam(:var_obs, init=100.0, lower=0.0)],  # Obs variance\n    T = [1.0],                                    # Transition matrix\n    R = [1.0],                                    # Selection matrix\n    Q = [FreeParam(:var_level, init=25.0, lower=0.0)], # State variance\n    a1 = [0.0],                                   # Initial state mean\n    P1 = [1e7],                                   # Initial state variance (diffuse)\n    name = :MyLocalLevel\n)\n\nprintln(\"Parameters: \", param_names(spec))\n# [:var_obs, :var_level]","category":"section"},{"location":"tutorials/custom_models.html#Key-Points","page":"Custom Models","title":"Key Points","text":"Fixed values: Use regular numbers for fixed matrix elements\nFree parameters: Use FreeParam(...) for parameters to be estimated\nBounds: Use lower=0.0 for variance parameters. TransformVariables.jl automatically applies appropriate transformations for constrained parameters.\nDimensions: Inferred automatically from the provided matrices","category":"section"},{"location":"tutorials/custom_models.html#The-FreeParam-Type","page":"Custom Models","title":"The FreeParam Type","text":"FreeParam marks a matrix element as an estimated parameter:\n\nFreeParam(name::Symbol;\n    init = 0.0,           # Initial value for optimization\n    lower = -Inf,         # Lower bound\n    upper = Inf           # Upper bound\n)","category":"section"},{"location":"tutorials/custom_models.html#Examples","page":"Custom Models","title":"Examples","text":"# Variance parameter (use lower=0.0 for positivity constraint)\nFreeParam(:var_obs, init=100.0, lower=0.0)\n\n# Bounded coefficient (e.g., AR coefficient)\nFreeParam(:ρ, init=0.8, lower=-0.99, upper=0.99)\n\n# Unbounded coefficient\nFreeParam(:β, init=0.0)","category":"section"},{"location":"tutorials/custom_models.html#The-@P-Macro","page":"Custom Models","title":"The @P Macro","text":"For quick parameter specification:\n\n@P(:σ, 1.0)  # Equivalent to FreeParam(:σ, init=1.0)","category":"section"},{"location":"tutorials/custom_models.html#Example:-Local-Linear-Trend-Model","page":"Custom Models","title":"Example: Local Linear Trend Model","text":"The local linear trend model has two states: level and slope.\n\nbeginaligned\ny_t = mu_t + varepsilon_t \nmu_t+1 = mu_t + nu_t + eta^mu_t \nnu_t+1 = nu_t + eta^nu_t\nendaligned\n\nspec = custom_ssm(\n    Z = [1.0 0.0],  # Only level is observed\n    H = [FreeParam(:var_obs, init=1.0, lower=0.0)],\n    T = [1.0 1.0;   # Level depends on previous level + slope\n         0.0 1.0],  # Slope is a random walk\n    R = Matrix(1.0I, 2, 2),  # Both states receive shocks\n    Q = [FreeParam(:var_level, init=0.01, lower=0.0)  0.0;\n         0.0  FreeParam(:var_slope, init=0.0001, lower=0.0)],\n    a1 = [0.0, 0.0],\n    P1 = 1e7 * Matrix(1.0I, 2, 2),\n    name = :LocalLinearTrend\n)\n\nprintln(\"States: \", spec.n_states)  # 2\nprintln(\"Parameters: \", param_names(spec))  # [:var_obs, :var_level, :var_slope]","category":"section"},{"location":"tutorials/custom_models.html#Matrix-Helper-Functions","page":"Custom Models","title":"Matrix Helper Functions","text":"Siphon.jl provides helper functions for common matrix patterns:","category":"section"},{"location":"tutorials/custom_models.html#Diagonal-Matrices-with-Free-Parameters","page":"Custom Models","title":"Diagonal Matrices with Free Parameters","text":"# Diagonal matrix with n free variance parameters (lower=0.0 by default)\nH = diag_free(3, :var_obs)\n# Creates parameters: var_obs_1, var_obs_2, var_obs_3\n\n# With custom initial value\nH = diag_free(3, :var_obs; init=2.0)","category":"section"},{"location":"tutorials/custom_models.html#Scalar-Matrices","page":"Custom Models","title":"Scalar Matrices","text":"# Scalar times identity\nH = scalar_free(3, :var_obs; init=1.0)\n# One parameter var_obs, applied to all diagonal elements","category":"section"},{"location":"tutorials/custom_models.html#Fixed-Diagonal-Matrices","page":"Custom Models","title":"Fixed Diagonal Matrices","text":"# Fixed diagonal values\nH = diag_fixed(3, [1.0, 2.0, 3.0])","category":"section"},{"location":"tutorials/custom_models.html#Identity-and-Zero-Matrices","page":"Custom Models","title":"Identity and Zero Matrices","text":"Z = identity_mat(3)     # 3×3 identity\nR = zeros_mat(3, 2)     # 3×2 zeros\nO = ones_mat(2, 2)      # 2×2 ones","category":"section"},{"location":"tutorials/custom_models.html#Selection-Matrix","page":"Custom Models","title":"Selection Matrix","text":"# Select specific states for observation\n# If n_states=4 and we observe states 1 and 3:\nZ = selection_mat([1, 3], 4)  # 2×4 matrix","category":"section"},{"location":"tutorials/custom_models.html#Companion-Matrix","page":"Custom Models","title":"Companion Matrix","text":"For VAR/ARMA models in companion form:\n\n# AR(2) companion matrix with free coefficients\nT = companion_mat(2, :φ; init=[0.5, 0.3])\n# Creates parameters φ_1, φ_2\n# Matrix: [φ_1  φ_2]\n#         [1    0  ]","category":"section"},{"location":"tutorials/custom_models.html#Lower-Triangular-Free","page":"Custom Models","title":"Lower Triangular Free","text":"# Lower triangular matrix with free parameters\nL = lower_triangular_free(3, :L)\n# Creates parameters for lower triangle: L_1_1, L_2_1, L_2_2, L_3_1, L_3_2, L_3_3","category":"section"},{"location":"tutorials/custom_models.html#Symmetric-Free","page":"Custom Models","title":"Symmetric Free","text":"# Symmetric matrix with free parameters\nS = symmetric_free(2, :S)\n# Creates parameters: S_1_1, S_2_1, S_2_2\n# The matrix is symmetric by construction","category":"section"},{"location":"tutorials/custom_models.html#Example:-Bivariate-VAR(1)","page":"Custom Models","title":"Example: Bivariate VAR(1)","text":"A bivariate VAR(1) model:\n\ny_t = Phi y_t-1 + varepsilon_t quad varepsilon_t sim N(0 Sigma)\n\nspec = custom_ssm(\n    Z = identity_mat(2),          # Observe both states\n    H = zeros_mat(2, 2),          # No measurement error (VAR is exact)\n    T = [FreeParam(:φ_11, init=0.5)  FreeParam(:φ_12, init=0.1);\n         FreeParam(:φ_21, init=0.1)  FreeParam(:φ_22, init=0.5)],\n    R = identity_mat(2),\n    Q = diag_free(2, :var_innov),  # Diagonal innovation covariance\n    a1 = [0.0, 0.0],\n    P1 = 1e4 * Matrix(1.0I, 2, 2),\n    name = :BivariateVAR1\n)","category":"section"},{"location":"tutorials/custom_models.html#Full-Covariance-Matrices-with-cov_free","page":"Custom Models","title":"Full Covariance Matrices with cov_free","text":"For models with correlated errors, use cov_free to specify a full positive-definite covariance matrix:\n\n# 3×3 positive definite covariance matrix\nQ = cov_free(3, :Q)\n\nThis uses the decomposition Sigma = D cdot textCorr cdot D where:\n\nD = textdiag(sigma_1 ldots sigma_n) contains standard deviations\ntextCorr is a correlation matrix (constructed via Cholesky factor)","category":"section"},{"location":"tutorials/custom_models.html#Parameters-Created","page":"Custom Models","title":"Parameters Created","text":"For cov_free(n, :prefix):\n\nn standard deviation parameters: prefix_σ_1, ..., prefix_σ_n\nn(n-1)/2 correlation parameters: prefix_corr_1, ...","category":"section"},{"location":"tutorials/custom_models.html#Example:-VAR-with-Correlated-Innovations","page":"Custom Models","title":"Example: VAR with Correlated Innovations","text":"spec = custom_ssm(\n    Z = identity_mat(2),\n    H = zeros_mat(2, 2),\n    T = [FreeParam(:φ_11, init=0.5)  FreeParam(:φ_12, init=0.0);\n         FreeParam(:φ_21, init=0.0)  FreeParam(:φ_22, init=0.5)],\n    R = identity_mat(2),\n    Q = cov_free(2, :Q),  # Full 2×2 covariance\n    a1 = [0.0, 0.0],\n    P1 = 1e4 * Matrix(1.0I, 2, 2),\n    name = :VAR1_Correlated\n)\n\nprintln(\"Parameters: \", param_names(spec))\n# [:φ_11, :φ_12, :φ_21, :φ_22, :Q_σ_1, :Q_σ_2, :Q_corr_1]","category":"section"},{"location":"tutorials/custom_models.html#Parameter-Dependent-Matrices-with-MatrixExpr","page":"Custom Models","title":"Parameter-Dependent Matrices with MatrixExpr","text":"For advanced models where matrix elements depend on parameters in complex ways (e.g., yield curve models), use MatrixExpr:\n\nstruct MatrixExpr\n    params::Vector{SSMParameter}  # Parameters used\n    data::NamedTuple              # Static data (e.g., maturities)\n    builder::Function             # Function to build the matrix\n    dims::Tuple{Int,Int}          # Matrix dimensions\nend","category":"section"},{"location":"tutorials/custom_models.html#Example:-Nelson-Siegel-Yield-Curve-Model","page":"Custom Models","title":"Example: Nelson-Siegel Yield Curve Model","text":"The Dynamic Nelson-Siegel model has loadings that depend on a decay parameter λ:\n\nZ_ij = begincases\n1  j=1 \nfrac1-e^-lambda tau_ilambda tau_i  j=2 \nfrac1-e^-lambda tau_ilambda tau_i - e^-lambda tau_i  j=3\nendcases\n\nusing Siphon\n\n# Builder function for DNS loadings\nfunction dns_builder(θ::Dict, data)\n    λ = θ[:λ]\n    τ = data.maturities\n    n = length(τ)\n    T = eltype(λ)\n\n    Z = zeros(T, n, 3)\n    for i in 1:n\n        Z[i, 1] = one(T)\n        Z[i, 2] = dns_loading1(λ, τ[i])\n        Z[i, 3] = dns_loading2(λ, τ[i])\n    end\n    return Z\nend\n\n# Create the MatrixExpr\nmaturities = [3, 6, 12, 24, 60, 120]  # in months\nZ_expr = MatrixExpr(\n    [SSMParameter(:λ, 0.001, 0.5, 0.0609)],  # name, lower, upper, init\n    (maturities = maturities,),\n    dns_builder,\n    (length(maturities), 3)\n)\n\n# Use in custom_ssm\nspec = custom_ssm(\n    Z = Z_expr,\n    H = diag_free(6, :var_obs),\n    T = diag_free(3, :φ; lower=-0.999, upper=0.999, init=0.9),\n    R = identity_mat(3),\n    Q = diag_free(3, :var_factor),\n    a1 = [0.0, 0.0, 0.0],\n    P1 = 1e4 * Matrix(1.0I, 3, 3),\n    name = :DynamicNelsonSiegel\n)","category":"section"},{"location":"tutorials/custom_models.html#Built-in-DNS-Helpers","page":"Custom Models","title":"Built-in DNS Helpers","text":"Siphon.jl provides convenience functions for DNS models:\n\n# Build DNS Z matrix directly\nZ = build_dns_loadings(maturities, :λ; λ_init=0.0609)\n\n# Or Svensson (4-factor) loadings\nZ = build_svensson_loadings(maturities, :λ1, :λ2; λ1_init=0.05, λ2_init=0.10)","category":"section"},{"location":"tutorials/custom_models.html#Putting-It-All-Together:-A-Complete-Example","page":"Custom Models","title":"Putting It All Together: A Complete Example","text":"Here's a complete workflow for a custom bivariate model:\n\nusing Siphon\nusing LinearAlgebra\nusing Random\n\nRandom.seed!(123)\n\n# Specify the model\nspec = custom_ssm(\n    Z = [1.0 0.0;    # First obs loads on first state\n         0.0 1.0],   # Second obs loads on second state\n    H = diag_free(2, :var_obs; init=0.5),\n    T = [FreeParam(:φ_1, init=0.8, lower=-0.99, upper=0.99)  0.0;\n         0.0  FreeParam(:φ_2, init=0.6, lower=-0.99, upper=0.99)],\n    R = identity_mat(2),\n    Q = cov_free(2, :Q; init_σ=1.0),\n    a1 = [0.0, 0.0],\n    P1 = 10.0 * Matrix(1.0I, 2, 2),\n    name = :BivariateAR1\n)\n\n# Check the specification\nprintln(\"Model: \", spec.name)\nprintln(\"States: \", spec.n_states)\nprintln(\"Parameters: \", param_names(spec))\nprintln(\"Number of parameters: \", n_params(spec))\n\n# Simulate some data\nT = 200\ny = randn(2, T)\n\n# Estimate parameters\nresult = optimize_ssm(spec, y)\nprintln(\"\\nEstimated parameters:\")\nfor (name, val) in pairs(result.θ)\n    println(\"  $name = $val\")\nend\n\n# Get smoothed states\nss = build_linear_state_space(spec, result.θ, y)\nfilt = kalman_filter(ss.p, y, ss.a1, ss.P1)\nsmooth = kalman_smoother(ss.p.Z, ss.p.T, filt.at, filt.Pt, filt.vt, filt.Ft)\nprintln(\"\\nSmoothed state at t=100: \", smooth.alpha[:, 100])","category":"section"},{"location":"tutorials/custom_models.html#Tips-and-Best-Practices","page":"Custom Models","title":"Tips and Best Practices","text":"Parameter naming: Use descriptive names with prefixes (e.g., var_obs, var_level) for clarity.\nInitial values: Good initial values help optimization converge. Use domain knowledge when possible.\nVariance parameters: Use lower=0.0 for variance parameters. TransformVariables.jl automatically applies asℝ₊ transformation to enforce positivity.\nBounds: Use bounds for constrained parameters (e.g., AR coefficients in (-1, 1) for stationarity).\nDiffuse initialization: Use large values in P1 (e.g., 1e7) for diffuse priors on initial states.\nDimension checks: custom_ssm validates dimensions automatically–let it catch errors early.","category":"section"},{"location":"tutorials/custom_models.html#Next-Steps","page":"Custom Models","title":"Next Steps","text":"Learn about Parameter Transformations for understanding how bounds are handled\nLearn about Dynamic Factor Models for multivariate factor analysis\nSee the Core Functions and DSL & Templates for complete API documentation","category":"section"},{"location":"api/matrix_helpers.html#Matrix-Helpers","page":"Matrix Helpers","title":"Matrix Helpers","text":"This page documents helper functions for constructing state space matrices.","category":"section"},{"location":"api/matrix_helpers.html#Diagonal-Matrices","page":"Matrix Helpers","title":"Diagonal Matrices","text":"","category":"section"},{"location":"api/matrix_helpers.html#Identity-and-Zero-Matrices","page":"Matrix Helpers","title":"Identity and Zero Matrices","text":"","category":"section"},{"location":"api/matrix_helpers.html#Covariance-Matrices","page":"Matrix Helpers","title":"Covariance Matrices","text":"","category":"section"},{"location":"api/matrix_helpers.html#Structured-Matrices","page":"Matrix Helpers","title":"Structured Matrices","text":"","category":"section"},{"location":"api/matrix_helpers.html#Selection-and-Companion-Matrices","page":"Matrix Helpers","title":"Selection and Companion Matrices","text":"","category":"section"},{"location":"api/matrix_helpers.html#Siphon.DSL.diag_free","page":"Matrix Helpers","title":"Siphon.DSL.diag_free","text":"diag_free(names; init=1.0, lower=0.0, upper=Inf)\ndiag_free(n, prefix; init=1.0, lower=0.0, upper=Inf)\n\nCreate a diagonal matrix with free parameters on the diagonal.\n\nArguments\n\nnames: Vector of parameter names, e.g., [:var_level, :var_slope]\nOR n, prefix: Number of elements and prefix, creates :prefix_1, :prefix_2, etc.\ninit: Initial value(s) - scalar or vector (variance values)\nlower, upper: Bounds - scalar or vector (lower=0.0 triggers asℝ₊ transform)\n\nExamples\n\n# 2×2 diagonal with named parameters (variance values)\nQ = diag_free([:var_level, :var_slope], init=[0.01, 0.0001])\n\n# 3×3 diagonal with auto-generated names\nQ = diag_free(3, :var, init=1.0)  # Creates :var_1, :var_2, :var_3\n\n# 1×1 scalar variance\nH = diag_free([:var_obs], init=100.0)\n# Or equivalently:\nH = scalar_free(:var_obs, init=100.0)\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Siphon.DSL.scalar_free","page":"Matrix Helpers","title":"Siphon.DSL.scalar_free","text":"scalar_free(name; init=1.0, lower=0.0, upper=Inf)\n\nCreate a 1×1 matrix with a single free parameter. Convenience wrapper for diag_free.\n\nExample\n\nH = scalar_free(:var_obs, init=225.0)\n# Equivalent to: H = [FreeParam(:var_obs, init=225.0, lower=0.0)]\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Siphon.DSL.diag_fixed","page":"Matrix Helpers","title":"Siphon.DSL.diag_fixed","text":"diag_fixed(values)\n\nCreate a diagonal matrix with fixed values on the diagonal.\n\nExample\n\nQ = diag_fixed([0.1, 0.01, 0.001])  # Fixed 3×3 diagonal\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Siphon.DSL.identity_mat","page":"Matrix Helpers","title":"Siphon.DSL.identity_mat","text":"identity_mat(n)\n\nCreate an n×n identity matrix (fixed values).\n\nExample\n\nR = identity_mat(3)  # 3×3 identity\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Siphon.DSL.zeros_mat","page":"Matrix Helpers","title":"Siphon.DSL.zeros_mat","text":"zeros_mat(m, n=m)\n\nCreate an m×n zero matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Siphon.DSL.ones_mat","page":"Matrix Helpers","title":"Siphon.DSL.ones_mat","text":"ones_mat(m, n=m)\n\nCreate an m×n matrix of ones.\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Siphon.DSL.cov_free","page":"Matrix Helpers","title":"Siphon.DSL.cov_free","text":"cov_free(n, prefix; init_σ=1.0)\n\nCreate a full n×n positive definite covariance matrix specification.\n\nUses Σ = D * Corr * D decomposition where:\n\nD = Diagonal(σ) with n standard deviation parameters (positive, named prefix_σ_1, etc.)\nCorr = L'L from corr_cholesky_factor(n) with n(n-1)/2 correlation parameters (unconstrained)\n\nTotal parameters: n + n(n-1)/2 = n(n+1)/2\n\nArguments\n\nn::Int: Matrix dimension\nprefix::Symbol: Prefix for parameter names\ninit_σ::Float64=1.0: Initial value for standard deviation parameters\n\nExample\n\n# 2×2 covariance with 3 parameters: Q_σ_1, Q_σ_2, Q_corr_1\nQ = cov_free(2, :Q)\n\n# 3×3 covariance with 6 parameters\nQ = cov_free(3, :Q, init_σ=0.5)\n\nSee also\n\ndiag_free: For diagonal covariance matrices (no correlation)\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Siphon.DSL.CovFree","page":"Matrix Helpers","title":"Siphon.DSL.CovFree","text":"CovFree\n\nMarker type for a full positive definite covariance matrix with free parameters.\n\nUsed in custom_ssm to specify that a covariance matrix (Q or H) should be parameterized as Σ = D * Corr * D where:\n\nD = Diagonal(σ) with n positive standard deviation parameters\nCorr = L'L where L comes from corr_cholesky_factor(n) with n(n-1)/2 correlation parameters\n\nTotal parameters: n + n(n-1)/2 = n(n+1)/2\n\nFields\n\nn::Int: Matrix dimension\nprefix::Symbol: Prefix for parameter names (e.g., :Q creates :Qσ1, :Qcorr1, etc.)\ninit_σ::Float64: Initial value for standard deviation parameters (default: 1.0)\n\nExample\n\nQ = cov_free(3, :Q)  # Creates 3×3 covariance with 6 parameters\n\n\n\n\n\n","category":"type"},{"location":"api/matrix_helpers.html#Siphon.DSL.lower_triangular_free","page":"Matrix Helpers","title":"Siphon.DSL.lower_triangular_free","text":"lower_triangular_free(n, prefix; init=0.0, lower=-Inf, upper=Inf)\n\nCreate a lower triangular matrix with free parameters below and on the diagonal. Useful for Cholesky factors.\n\nExample\n\n# Cholesky factor of covariance matrix\nL = lower_triangular_free(2, :L)\n# Creates:\n# [FreeParam(:L_1_1)  0.0            ]\n# [FreeParam(:L_2_1)  FreeParam(:L_2_2)]\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Siphon.DSL.symmetric_free","page":"Matrix Helpers","title":"Siphon.DSL.symmetric_free","text":"symmetric_free(n, prefix; init_diag=1.0, init_offdiag=0.0,\n               lower_diag=0.0, lower_offdiag=-Inf,\n               upper_diag=Inf, upper_offdiag=Inf)\n\nCreate a symmetric matrix with free parameters. Diagonal and off-diagonal elements can have different settings.\n\nExample\n\n# Symmetric covariance with variance on diagonal, covariances off-diagonal\nΣ = symmetric_free(2, :Σ, init_diag=1.0, init_offdiag=0.1)\n# Creates a symmetric matrix where Σ[1,2] = Σ[2,1] (same parameter)\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Siphon.DSL.selection_mat","page":"Matrix Helpers","title":"Siphon.DSL.selection_mat","text":"selection_mat(m, r)\n\nCreate a selection matrix R of size m×r. Common patterns:\n\nselection_mat(m, m) → Identity (all states have shocks)\nselection_mat(m, r) where r < m → First r states have shocks\n\nExample\n\n# 3 states, 2 shocks (first 2 states)\nR = selection_mat(3, 2)\n# [1 0]\n# [0 1]\n# [0 0]\n\n\n\n\n\n","category":"function"},{"location":"api/matrix_helpers.html#Siphon.DSL.companion_mat","page":"Matrix Helpers","title":"Siphon.DSL.companion_mat","text":"companion_mat(n)\n\nCreate an n×n companion matrix structure (for AR(n) models). Returns a matrix with 1s on the superdiagonal and FreeParams in the first column.\n\nExample\n\nT = companion_mat(2, :φ)  # AR(2) transition\n# [FreeParam(:φ_1)  1.0]\n# [FreeParam(:φ_2)  0.0]\n\n\n\n\n\n","category":"function"},{"location":"tutorials/dynamic_factor.html#Dynamic-Factor-Models","page":"Dynamic Factor Models","title":"Dynamic Factor Models","text":"This tutorial covers dynamic factor models (DFMs) in Siphon.jl. DFMs are widely used in macroeconomics, finance, and other fields to extract common latent factors from a large panel of observed time series.\n\nSiphon.jl provides two approaches:\n\nDSL approach (dynamic_factor template): Flexible specification with MLE estimation\nHigh-level API (DynamicFactorModel): Specialized type with EM estimation for large panels","category":"section"},{"location":"tutorials/dynamic_factor.html#Model-Overview","page":"Dynamic Factor Models","title":"Model Overview","text":"","category":"section"},{"location":"tutorials/dynamic_factor.html#Mathematical-Formulation","page":"Dynamic Factor Models","title":"Mathematical Formulation","text":"The general dynamic factor model is:\n\nbeginaligned\nX_t = Lambda(L) f_t + e_t \nf_t = Psi(L) f_t-1 + eta_t quad eta_t sim N(0 Sigma_eta) \ne_it = delta(L) e_it-1 + v_it quad v_it sim N(0 sigma^2_i)\nendaligned\n\nWhere:\n\nX_t is the N times 1 vector of observables\nf_t is the k times 1 vector of latent factors\nLambda(L) = Lambda_0 + Lambda_1 L + cdots are dynamic factor loadings\nPsi(L) is the factor VAR polynomial\ndelta(L) captures AR dynamics in idiosyncratic errors","category":"section"},{"location":"tutorials/dynamic_factor.html#High-Level-API:-DynamicFactorModel","page":"Dynamic Factor Models","title":"High-Level API: DynamicFactorModel","text":"For large-scale applications, use the DynamicFactorModel type with EM estimation:\n\nusing Siphon\ninclude(\"src/inplace.jl\")\n\n# Create model: 100 observables, 6 factors, 200 time periods\nmodel = DynamicFactorModel(\n    100,              # N: number of observables\n    6,                # k: number of factors\n    200;              # n: number of time periods\n    factor_lags = 3,  # VAR(3) factor dynamics\n    error_lags = 1    # AR(1) idiosyncratic errors\n)\n\n# Fit with EM algorithm\nfit!(EM(), model, y; maxiter=500, tol=1e-6, verbose=true)\n\n# Check convergence\nprintln(\"Converged: \", isconverged(model))\nprintln(\"Log-likelihood: \", loglikelihood(model))\n\n# Access results\nf = factors(model)              # k × n smoothed factors\nΛ = loadings(model)             # Factor loadings [Λ₀, Λ₁, ...]\nΦ = var_coefficients(model)     # VAR coefficients [Φ₁, ..., Φ_q]\nδ = ar_coefficients(model)      # AR error coefficients\nΣ_η = innovation_cov(model)     # Factor innovation covariance\nσ²_v = idiosyncratic_variances(model)  # Idiosyncratic variances","category":"section"},{"location":"tutorials/dynamic_factor.html#Model-Configurations","page":"Dynamic Factor Models","title":"Model Configurations","text":"Configuration loading_lags factor_lags error_lags Description\nSimple DFM 0 q 0 Static loadings, VAR(q) factors\nAR errors 0 q r Static loadings, AR(r) errors\nDynamic loadings p q 0 Dynamic loadings with p lags\nFull DFM p q r Dynamic loadings + AR errors","category":"section"},{"location":"tutorials/dynamic_factor.html#State-Dimension","page":"Dynamic Factor Models","title":"State Dimension","text":"The state vector stacks current and lagged factors (and errors if r > 0):\n\nalpha_t = f_t f_t-1 ldots f_t-s+1 e_t e_t-1 ldots e_t-r+1\n\nwhere s = max(q p+1). State dimension: m = k times s + N times r.","category":"section"},{"location":"tutorials/dynamic_factor.html#DSL-Approach:-dynamic_factor-Template","page":"Dynamic Factor Models","title":"DSL Approach: dynamic_factor Template","text":"For smaller models or when you need MLE with AD:\n\nusing Siphon\n\n# 8 observables, 2 factors, AR(1) dynamics\nspec = dynamic_factor(8, 2)\n\nprintln(\"Parameters: \", param_names(spec))\nprintln(\"Number of states: \", spec.n_states)\n\n# Estimate via MLE\nresult = optimize_ssm(spec, y)","category":"section"},{"location":"tutorials/dynamic_factor.html#Identification-and-Normalization","page":"Dynamic Factor Models","title":"Identification and Normalization","text":"","category":"section"},{"location":"tutorials/dynamic_factor.html#Why-Identification-is-Needed","page":"Dynamic Factor Models","title":"Why Identification is Needed","text":"Factor models have a fundamental rotation indeterminacy: for any orthogonal matrix P, replacing f_t with P f_t and Lambda with Lambda P yields the same observed distribution:\n\nX_t = Lambda f_t + e_t = (Lambda P)(P f_t) + e_t = hatLambda hatf_t + e_t\n\nThis means factors are only identified up to rotation without additional constraints. The likelihood is identical for infinitely many rotations of the factors.\n\nTo uniquely identify k factors, we need exactly k^2 restrictions. These can be placed on:\n\nFactor loadings Lambda (constrain certain elements)\nFactor covariance Sigma_eta (constrain the innovation covariance)\n\nSiphon.jl provides two identification schemes that achieve k^2 restrictions through different combinations.","category":"section"},{"location":"tutorials/dynamic_factor.html#Identification-Schemes","page":"Dynamic Factor Models","title":"Identification Schemes","text":"","category":"section"},{"location":"tutorials/dynamic_factor.html#Named-Factor-(:named_factor,-default)","page":"Dynamic Factor Models","title":"Named Factor (:named_factor, default)","text":"The named factor identification (Stock & Watson 2011) constrains the first k rows of Lambda_0 to form an identity block:\n\nLambda_0 = beginbmatrix\n1  0  cdots  0 \nlambda_21  1  cdots  0 \nlambda_31  lambda_32  ddots  vdots \nvdots  vdots  ddots  1 \nlambda_k+11  lambda_k+12  cdots  lambda_k+1k \nvdots  vdots   vdots\nendbmatrix\n\nConstraints:\n\nDiagonal: lambda_ii = 1 for i = 1 ldots k (k restrictions)\nUpper triangle: lambda_ij = 0 for i  j leq k (k(k-1)2 restrictions)\nTotal from loadings: k + k(k-1)2 = k(k+1)2 restrictions\n\nThe factor innovation covariance Sigma_eta is freely estimated, providing no additional restrictions.\n\nInterpretation: The first k observables directly \"name\" the factors. Observable 1 has unit loading on factor 1 and zero loading on factors 2 through k. Observable 2 has unit loading on factor 2 and zero loading on factors 3 through k, etc.\n\nnote: Note\nWith :named_factor, you get exactly k(k+1)2 restrictions from the loadings. The remaining k(k-1)2 restrictions needed for full identification come implicitly from the model structure (the identity block pins down the scale and ordering of factors).","category":"section"},{"location":"tutorials/dynamic_factor.html#Lower-Triangular-(:lower_triangular)","page":"Dynamic Factor Models","title":"Lower Triangular (:lower_triangular)","text":"The lower triangular identification (Harvey 1989) uses a different combination:\n\nLoading constraints:\n\nLambda_0 = beginbmatrix\nlambda_11  0  cdots  0 \nlambda_21  lambda_22  cdots  0 \nvdots  vdots  ddots  vdots \nlambda_k1  lambda_k2  cdots  lambda_kk \nlambda_k+11  lambda_k+12  cdots  lambda_k+1k \nvdots  vdots   vdots\nendbmatrix\n\nUpper triangle of Lambda_0 is zero: k(k-1)2 restrictions\nDiagonal is FREE (unlike :named_factor)\n\nCovariance constraint:\n\nSigma_eta = I_k quad text(fixed to identity)\n\nThis provides k(k+1)2 restrictions (the k diagonal elements and k(k-1)2 off-diagonal elements are all fixed).\n\nTotal restrictions: k(k-1)2 + k(k+1)2 = k^2 ✓\n\nInterpretation: Factors have unit innovation variance and are uncorrelated in innovations. The loadings diagonal captures the \"scale\" of each factor's influence.","category":"section"},{"location":"tutorials/dynamic_factor.html#Choosing-an-Identification-Scheme","page":"Dynamic Factor Models","title":"Choosing an Identification Scheme","text":"Aspect :named_factor :lower_triangular\nLambda_0 diagonal Fixed at 1 Free\nLambda_0 upper triangle Fixed at 0 Fixed at 0\nSigma_eta (factor innovation cov) Free Fixed at I_k\nInterpretation First k obs directly observe factors Factors have unit innovation variance\nReference Stock & Watson (2011) Harvey (1989)\n\nWhen to use each:\n\n:named_factor (default): Good when you want interpretable factors tied to specific observables. Natural when certain series are known to load primarily on one factor.\n:lower_triangular: Good when you want the factor innovation covariance to absorb all scale information. Useful when comparing across datasets with different observables.\n\n# Use named factor identification (default)\nmodel = DynamicFactorModel(100, 6, 200; identification=:named_factor)\n\n# Use lower triangular identification\nmodel = DynamicFactorModel(100, 6, 200; identification=:lower_triangular)\n\n# No identification (for forecasting only - factors not uniquely identified)\nmodel = DynamicFactorModel(100, 6, 200; identification=:none)","category":"section"},{"location":"tutorials/dynamic_factor.html#How-Constraints-Are-Enforced","page":"Dynamic Factor Models","title":"How Constraints Are Enforced","text":"Siphon.jl enforces identification constraints through two mechanisms:\n\nZ_free BitMatrix: Tracks which loading elements can be updated during EM iterations\nElements where Z_free[i,j] = false are never modified\nSet at initialization via _apply_identification!\nQ_free BitMatrix: Controls which factor covariance elements are estimated\nFor :lower_triangular, factor covariance block is fixed to identity\nFor :named_factor, factor covariance is freely estimated\n\nDuring each M-step, update_Z! only modifies loadings where Z_free[i,j] = true:\n\n# Simplified view of constraint enforcement\nfor i in 1:N, j in 1:k\n    if Z_free[i, j]\n        Z[i, j] = new_value  # Update only free elements\n    end\n    # Fixed elements (diagonal=1, upper=0 for :named_factor) unchanged\nend","category":"section"},{"location":"tutorials/dynamic_factor.html#Factor-Extraction","page":"Dynamic Factor Models","title":"Factor Extraction","text":"After fitting, extracted factors are already normalized according to the chosen identification scheme:\n\nmodel = DynamicFactorModel(N, k, n; identification=:named_factor)\nfit!(EM(), model, y)\n\n# Factors are normalized - no post-processing needed\nf = factors(model)  # k × n smoothed factors E[fₜ | y₁:ₙ]\n\nWith :named_factor: Factors have the scale implied by unit loadings on the first k observables\nWith :lower_triangular: Factors have the scale implied by unit innovation variance\n\nNo additional normalization or rotation is required after extraction. The constraints are maintained throughout estimation, so the extracted factors are directly interpretable under the chosen scheme.","category":"section"},{"location":"tutorials/dynamic_factor.html#Multiple-Lags","page":"Dynamic Factor Models","title":"Multiple Lags","text":"# 5 observables, 2 factors, VAR(3) factor dynamics, 1 lag in loadings\nspec = dynamic_factor(5, 2; factor_lags=3, obs_lags=1)\n\nprintln(\"States: \", spec.n_states)  # 6 = 2 factors × max(3, 1+1)","category":"section"},{"location":"tutorials/dynamic_factor.html#Complete-Example:-Macroeconomic-Factor-Model","page":"Dynamic Factor Models","title":"Complete Example: Macroeconomic Factor Model","text":"","category":"section"},{"location":"tutorials/dynamic_factor.html#Using-DynamicFactorModel-(Recommended-for-Large-Panels)","page":"Dynamic Factor Models","title":"Using DynamicFactorModel (Recommended for Large Panels)","text":"using Siphon\nusing LinearAlgebra\nusing Statistics\ninclude(\"src/inplace.jl\")\n\n# Load FRED-QD or similar macro data\n# y should be N × n (observables × time)\n\nN, n = 100, 200  # 100 series, 200 quarters\nk = 6            # 6 factors\n\n# Create and fit model\nmodel = DynamicFactorModel(N, k, n;\n    factor_lags = 3,   # VAR(3)\n    error_lags = 1     # AR(1) errors\n)\n\nfit!(EM(), model, y; maxiter=200, verbose=true)\n\n# Variance decomposition\nf = factors(model)\nΛ = loadings(model)[1]  # Contemporaneous loadings\nfactor_cov = (f * f') / n\n\ncommunalities = zeros(N)\nfor i in 1:N\n    λ_i = Λ[i, :]\n    communalities[i] = λ_i' * factor_cov * λ_i\nend\n\nidio_var = idiosyncratic_variances(model)\nvar_explained = communalities ./ (communalities .+ idio_var)\n\nprintln(\"Average variance explained: \", mean(var_explained) * 100, \"%\")","category":"section"},{"location":"tutorials/dynamic_factor.html#Using-DSL-(For-Smaller-Models-with-MLE)","page":"Dynamic Factor Models","title":"Using DSL (For Smaller Models with MLE)","text":"using Siphon\nusing Random\nusing LinearAlgebra\n\nRandom.seed!(42)\n\n# Simulate data from a 2-factor model\nn_obs, T = 8, 200\n\ntrue_loadings = [1.0 0.0; 0.5 1.0; 0.8 0.3; 0.3 0.7;\n                 0.6 0.4; 0.4 0.6; 0.7 0.2; 0.2 0.8]\ntrue_φ = [0.9, 0.7]\ntrue_σ_factor = [0.5, 0.4]\ntrue_σ_obs = fill(0.3, n_obs)\n\n# Simulate factors\nf = zeros(2, T)\nfor t in 2:T\n    f[:, t] = Diagonal(true_φ) * f[:, t-1] + Diagonal(true_σ_factor) * randn(2)\nend\n\n# Generate observations\ny = zeros(n_obs, T)\nfor t in 1:T\n    y[:, t] = true_loadings * f[:, t] + Diagonal(true_σ_obs) * randn(n_obs)\nend\n\n# Specify and estimate model\nspec = dynamic_factor(n_obs, 2;\n    loadings_init = 0.5,\n    ar_init = 0.7,\n    σ_obs_init = 0.5,\n    σ_factor_init = 0.5\n)\n\nresult = optimize_ssm(spec, y)\n\nprintln(\"Estimated AR coefficients:\")\nprintln(\"  φ_1_1 = \", round(result.θ.φ_1_1, digits=3), \" (true: 0.9)\")\nprintln(\"  φ_2_1 = \", round(result.θ.φ_2_1, digits=3), \" (true: 0.7)\")\n\n# Extract smoothed factors\nss = build_linear_state_space(spec, result.θ, y)\nfilt = kalman_filter(ss.p, y, ss.a1, ss.P1)\nsmooth = kalman_smoother(ss.p.Z, ss.p.T, filt.at, filt.Pt, filt.vt, filt.Ft)\n\nprintln(\"\\nCorrelation with true factors:\")\nprintln(\"  Factor 1: \", round(cor(smooth.alpha[1, :], f[1, :]), digits=3))\nprintln(\"  Factor 2: \", round(cor(smooth.alpha[2, :], f[2, :]), digits=3))","category":"section"},{"location":"tutorials/dynamic_factor.html#Forecasting","page":"Dynamic Factor Models","title":"Forecasting","text":"# Forecast h steps ahead\nfc = forecast(model, 12)  # 12-step forecast\n\n# Access forecast components\nfc.obs_mean      # N × h forecasted observations\nfc.obs_cov       # N × N × h forecast covariances\nfc.factor_mean   # k × h forecasted factors\nfc.factor_cov    # k × k × h factor forecast covariances","category":"section"},{"location":"tutorials/dynamic_factor.html#Tips-and-Best-Practices","page":"Dynamic Factor Models","title":"Tips and Best Practices","text":"Number of factors: Start with fewer factors. Use information criteria or scree plots to select k.\nInitialization: The EM algorithm uses PCA for initialization. For MLE, good initial values help.\nLarge panels: Use DynamicFactorModel with EM for N > 20. The in-place implementation is memory-efficient.\nMissing data: Both approaches handle missing values (NaN) automatically.\nAR errors: Include AR errors (error_lags > 0) when idiosyncratic components are persistent.\nDynamic loadings: Use loading_lags > 0 when factors have delayed effects on observables.\nConvergence: Check isconverged(model) and examine the log-likelihood history.","category":"section"},{"location":"tutorials/dynamic_factor.html#Function-Reference","page":"Dynamic Factor Models","title":"Function Reference","text":"","category":"section"},{"location":"tutorials/dynamic_factor.html#DynamicFactorModel-Constructor","page":"Dynamic Factor Models","title":"DynamicFactorModel Constructor","text":"DynamicFactorModel(n_obs, n_factors, n_times;\n    loading_lags = 0,   # Lags in λ(L), 0 = static loadings\n    factor_lags = 1,    # Lags in factor VAR\n    error_lags = 0,     # Lags in AR errors, 0 = white noise\n    T = Float64         # Element type\n)","category":"section"},{"location":"tutorials/dynamic_factor.html#dynamic_factor-Template","page":"Dynamic Factor Models","title":"dynamic_factor Template","text":"dynamic_factor(n_obs, n_factors;\n    factor_lags = 1,           # AR lags in factor dynamics\n    obs_lags = 0,              # Lagged factor loadings\n    correlated_errors = false, # Full H matrix if true\n    loadings_init = 0.5,       # Initial loading values\n    ar_init = 0.5,             # Initial AR coefficient\n    σ_obs_init = 1.0,          # Initial obs error std dev\n    σ_factor_init = 1.0,       # Initial factor std dev\n    diffuse = true             # Diffuse initialization\n)","category":"section"},{"location":"tutorials/dynamic_factor.html#Next-Steps","page":"Dynamic Factor Models","title":"Next Steps","text":"Learn about Custom Models for more flexible specifications\nCheck the Core Functions API reference","category":"section"},{"location":"tutorials/getting_started.html#Getting-Started","page":"Getting Started","title":"Getting Started","text":"This tutorial introduces the basic workflow for working with state space models in Siphon.jl. We'll cover:\n\nCreating a model specification using templates\nComputing the log-likelihood\nRunning the Kalman filter\nRunning the Kalman smoother\nEstimating parameters via maximum likelihood","category":"section"},{"location":"tutorials/getting_started.html#The-Local-Level-Model","page":"Getting Started","title":"The Local Level Model","text":"We'll use the classic local level model (also known as the random walk plus noise model) as our running example. This model decomposes a time series into a slowly-evolving level plus observation noise:\n\nbeginaligned\ny_t = mu_t + varepsilon_t quad varepsilon_t sim N(0 sigma^2_varepsilon) \nmu_t+1 = mu_t + eta_t quad eta_t sim N(0 sigma^2_eta)\nendaligned\n\nIn state space form:\n\nZ = 1 (the observation loads directly on the state)\nH = sigma^2_varepsilon (observation variance)\nT = 1 (random walk dynamics)\nR = 1 (state receives the shock directly)\nQ = sigma^2_eta (state innovation variance)","category":"section"},{"location":"tutorials/getting_started.html#Creating-a-Model-Specification","page":"Getting Started","title":"Creating a Model Specification","text":"using Siphon\n\n# Create a local level model with initial parameter values\nspec = local_level()\n\n# Inspect the specification\nprintln(\"Parameters: \", param_names(spec))\nprintln(\"Initial values: \", initial_values(spec))\n\nOutput:\n\nParameters: (:var_obs, :var_level)\nInitial values: [225.0, 100.0]","category":"section"},{"location":"tutorials/getting_started.html#Loading-Data","page":"Getting Started","title":"Loading Data","text":"Let's work with the classic Nile River dataset:\n\nusing DelimitedFiles\n\n# Load the Nile data\nnile = readdlm(\"test/Nile.csv\", ',', Float64)\ny = nile'  # Convert to 1×n matrix (p × T format)\n\nprintln(\"Number of observations: \", size(y, 2))","category":"section"},{"location":"tutorials/getting_started.html#The-Unified-StateSpaceModel-API","page":"Getting Started","title":"The Unified StateSpaceModel API","text":"Siphon.jl provides a unified API centered around StateSpaceModel. There are two main ways to create a model:","category":"section"},{"location":"tutorials/getting_started.html#Option-1:-Create-Model-with-Known-Parameters","page":"Getting Started","title":"Option 1: Create Model with Known Parameters","text":"If you already have parameter values (from prior knowledge, simulation, or external estimation):\n\n# Define known parameters\nθ = (var_obs=15099.0, var_level=1469.0)\n\n# Create model with these parameters\nmodel = StateSpaceModel(spec, θ, size(y, 2))\n\n# Run filter and smoother\nll = kalman_filter!(model, y)\nkalman_smoother!(model)\n\n# Access results\nprintln(\"Log-likelihood: \", ll)\nprintln(\"Filtered states: \", filtered_states(model))\nprintln(\"Smoothed states: \", smoothed_states(model))","category":"section"},{"location":"tutorials/getting_started.html#Option-2:-Estimate-Parameters","page":"Getting Started","title":"Option 2: Estimate Parameters","text":"If you want to estimate parameters from data:\n\n# Create an unfitted model\nmodel = StateSpaceModel(spec, size(y, 2))\n\n# Estimate via MLE\nfit!(MLE(), model, y)\n\n# Or estimate via EM\n# fit!(EM(), model, y; maxiter=200)\n\n# Access results\nprintln(\"Log-likelihood: \", loglikelihood(model))\nprintln(\"Parameters: \", parameters(model))\nprintln(\"Converged: \", isconverged(model))","category":"section"},{"location":"tutorials/getting_started.html#Computing-the-Log-Likelihood","page":"Getting Started","title":"Computing the Log-Likelihood","text":"With the unified API, computing the log-likelihood is straightforward:\n\n# With known parameters\nθ = (var_obs=15099.0, var_level=1469.0)\nmodel = StateSpaceModel(spec, θ, size(y, 2))\n\n# Method 1: kalman_loglik (doesn't store filter results)\nll = kalman_loglik(model, y)\nprintln(\"Log-likelihood: \", ll)\n\n# Method 2: kalman_filter! (stores filter results for later use)\nll = kalman_filter!(model, y)\nprintln(\"Log-likelihood: \", ll)","category":"section"},{"location":"tutorials/getting_started.html#Kalman-Filter","page":"Getting Started","title":"Kalman Filter","text":"The Kalman filter computes the sequence of filtered state estimates:\n\nθ = (var_obs=15099.0, var_level=1469.0)\nmodel = StateSpaceModel(spec, θ, size(y, 2))\n\n# Run the filter\nkalman_filter!(model, y)\n\n# Access filter results\natt = filtered_states(model)           # E[α_t|y_{1:t}] (m × n)\nPtt = filtered_states_cov(model)       # Var[α_t|y_{1:t}] (m × m × n)\nat = predicted_states(model)           # E[α_t|y_{1:t-1}] (m × n)\nPt = predicted_states_cov(model)       # Var[α_t|y_{1:t-1}] (m × m × n)\nvt = prediction_errors(model)          # y_t - E[y_t|y_{1:t-1}] (p × n)\nFt = prediction_errors_cov(model)      # Var[y_t|y_{1:t-1}] (p × p × n)\n\nprintln(\"Final filtered state: \", att[:, end])\nprintln(\"Log-likelihood: \", loglikelihood(model))","category":"section"},{"location":"tutorials/getting_started.html#Filter-Output-Structure","page":"Getting Started","title":"Filter Output Structure","text":"filtered_states(model): State estimate at time t given observations up to t (i.e., a_tt)\npredicted_states(model): State estimate at time t given observations up to t-1 (i.e., a_tt-1)\nprediction_errors(model): Innovation (prediction error) at time t\nAll covariance accessors provide the corresponding variance/covariance matrices","category":"section"},{"location":"tutorials/getting_started.html#Kalman-Smoother","page":"Getting Started","title":"Kalman Smoother","text":"The Kalman smoother computes smoothed state estimates using all available observations:\n\nθ = (var_obs=15099.0, var_level=1469.0)\nmodel = StateSpaceModel(spec, θ, size(y, 2))\n\n# Run filter first\nkalman_filter!(model, y)\n\n# Then run smoother\nkalman_smoother!(model)\n\n# Access results\nalpha = smoothed_states(model)      # E[α_t|y_{1:n}] (m × n)\nV = smoothed_states_cov(model)      # Var[α_t|y_{1:n}] (m × m × n)\n\nprintln(\"Smoothed state at t=50: \", alpha[:, 50])\n\nNote: The smoother results are cached, so smoothed_states(model) will compute the smoother on first call and return cached results thereafter.","category":"section"},{"location":"tutorials/getting_started.html#Parameter-Estimation","page":"Getting Started","title":"Parameter Estimation","text":"Siphon.jl provides a unified fit! API for parameter estimation:\n\n# Create a model container\nmodel = StateSpaceModel(spec, size(y, 2))\n\n# Maximum likelihood estimation\nfit!(MLE(), model, y)\n\n# Access results\nprintln(\"Log-likelihood: \", loglikelihood(model))\nprintln(\"Parameters: \", parameters(model))\nprintln(\"Converged: \", isconverged(model))","category":"section"},{"location":"tutorials/getting_started.html#EM-Algorithm","page":"Getting Started","title":"EM Algorithm","text":"For models with only variance parameters, the EM algorithm can be faster:\n\n# Create model and estimate via EM\nmodel = StateSpaceModel(spec, size(y, 2))\nfit!(EM(), model, y; maxiter=200, verbose=true)\n\nprintln(\"Converged: \", isconverged(model))\nprintln(\"Iterations: \", niterations(model))\nprintln(\"Parameters: \", parameters(model))","category":"section"},{"location":"tutorials/getting_started.html#Alternative:-Direct-Optimization","page":"Getting Started","title":"Alternative: Direct Optimization","text":"For more control over the optimization process:\n\n# Use optimize_ssm directly\nresult = optimize_ssm(spec, y)\n\nθ_mle = result.θ           # Estimated parameters (NamedTuple)\nloglik = result.loglik     # Maximized log-likelihood\n\nprintln(\"Estimated var_obs: \", θ_mle.var_obs)\nprintln(\"Estimated var_level: \", θ_mle.var_level)","category":"section"},{"location":"tutorials/getting_started.html#Filter-and-Smooth-with-Estimated-Parameters","page":"Getting Started","title":"Filter and Smooth with Estimated Parameters","text":"After fitting, you can access filter and smoother results directly:\n\n# Fit the model\nmodel = StateSpaceModel(spec, size(y, 2))\nfit!(MLE(), model, y)\n\n# Filter results are automatically computed during fitting\nprintln(\"Filtered state std: \", std(filtered_states(model)[1, :]))\n\n# Smoothed states computed on demand\nprintln(\"Smoothed state std: \", std(smoothed_states(model)[1, :]))\n\nThe smoothed estimates are generally more accurate than filtered estimates because they use information from the entire sample.","category":"section"},{"location":"tutorials/getting_started.html#Accessing-System-Matrices","page":"Getting Started","title":"Accessing System Matrices","text":"After fitting (or with known parameters), you can access the system matrices:\n\nmodel = StateSpaceModel(spec, size(y, 2))\nfit!(MLE(), model, y)\n\n# Get all matrices at once\nmats = system_matrices(model)\nprintln(\"Z = \", mats.Z)\nprintln(\"H = \", mats.H)\nprintln(\"T = \", mats.T)\nprintln(\"Q = \", mats.Q)\n\n# Or individual matrices\nZ = obs_matrix(model)\nH = obs_cov(model)\nT = transition_matrix(model)\nQ = state_cov(model)","category":"section"},{"location":"tutorials/getting_started.html#Other-Pre-Built-Templates","page":"Getting Started","title":"Other Pre-Built Templates","text":"Siphon.jl provides several other templates:","category":"section"},{"location":"tutorials/getting_started.html#Local-Linear-Trend","page":"Getting Started","title":"Local Linear Trend","text":"# Level + slope model\nspec = local_linear_trend()\n# Parameters: (:var_obs, :var_level, :var_slope)","category":"section"},{"location":"tutorials/getting_started.html#AR(1)-Process","page":"Getting Started","title":"AR(1) Process","text":"# AR(1) with measurement noise\nspec = ar1(ρ_init=0.9)\n# Parameters: (:ρ, :var_obs, :var_state)","category":"section"},{"location":"tutorials/getting_started.html#ARMA(p,-q)","page":"Getting Started","title":"ARMA(p, q)","text":"# ARMA(2, 1) model\nspec = arma(2, 1)\n# Parameters: (:ar_1, :ar_2, :ma_1, :var)","category":"section"},{"location":"tutorials/getting_started.html#Dynamic-Nelson-Siegel","page":"Getting Started","title":"Dynamic Nelson-Siegel","text":"# DNS yield curve model\nmaturities = [3, 6, 12, 24, 36, 60, 84, 120]\nspec = dns_model(maturities)","category":"section"},{"location":"tutorials/getting_started.html#Exact-Diffuse-Initialization","page":"Getting Started","title":"Exact Diffuse Initialization","text":"For models with unknown initial states (like the local level model), standard practice uses a large initial covariance (e.g., P1 = 1e7 * I) as an \"approximate diffuse\" prior. Siphon.jl also supports exact diffuse initialization following Durbin & Koopman (2012), which provides a theoretically correct treatment.","category":"section"},{"location":"tutorials/getting_started.html#The-Problem-with-Approximate-Diffuse","page":"Getting Started","title":"The Problem with Approximate Diffuse","text":"With approximate diffuse, the first observation contributes to the likelihood even though the initial state is essentially unknown. This can bias parameter estimates, especially for short series.","category":"section"},{"location":"tutorials/getting_started.html#Exact-Diffuse-Solution","page":"Getting Started","title":"Exact Diffuse Solution","text":"Exact diffuse splits the initial covariance into:\n\nP1_star: Finite (known) part of initial covariance\nP1_inf: Diffuse (infinite) part indicating unknown components\n\n# Local level model with exact diffuse initialization\np = KFParms(\n    [1.0;;],      # Z\n    [15099.0;;],  # H (observation variance)\n    [1.0;;],      # T\n    [1.0;;],      # R\n    [1469.1;;]    # Q (state variance)\n)\n\na1 = [0.0]           # Initial state mean\nP1_star = [0.0;;]    # No finite uncertainty initially\nP1_inf = [1.0;;]     # Full diffuse on the level\n\n# Use 5-argument form to trigger exact diffuse\nll_exact = kalman_loglik(p, y, a1, P1_star, P1_inf)\n\n# Compare with approximate diffuse (4-argument form)\nP1_approx = [1e7;;]\nll_approx = kalman_loglik(p, y, a1, P1_approx)\n\nprintln(\"Exact diffuse log-likelihood: \", ll_exact)      # -632.55\nprintln(\"Approximate diffuse log-likelihood: \", ll_approx) # -641.59\n\nFor the Nile data, exact diffuse gives a log-likelihood about 9 units higher than approximate diffuse. This difference reflects the correct treatment of the unknown initial state.","category":"section"},{"location":"tutorials/getting_started.html#Validation-Against-R's-KFAS","page":"Getting Started","title":"Validation Against R's KFAS","text":"Siphon.jl's exact diffuse implementation matches R's KFAS package. For the Nile local level model:\n\nQuantity Siphon KFAS\nLog-likelihood (exact diffuse) -632.5456 -632.5456\nLog-likelihood (approx, P1=1e7) -641.5856 -641.5856\nMLE H (obs variance) 15098.52 15098.53\nMLE Q (state variance) 1469.18 1469.18","category":"section"},{"location":"tutorials/getting_started.html#MLE-with-Exact-Diffuse","page":"Getting Started","title":"MLE with Exact Diffuse","text":"using Optimization, OptimizationOptimJL\n\n# Define negative log-likelihood with exact diffuse\nfunction negloglik_diffuse(θ, y)\n    H, Q = exp(θ[1]), exp(θ[2])  # Ensure positive via log transform\n    p = KFParms([1.0;;], [H;;], [1.0;;], [1.0;;], [Q;;])\n    a1 = [0.0]\n    P1_star = [0.0;;]\n    P1_inf = [1.0;;]\n    return -kalman_loglik(p, y, a1, P1_star, P1_inf)\nend\n\n# Optimize with autodiff\nθ0 = [log(1000.0), log(1000.0)]  # Initial values (log scale)\noptf = OptimizationFunction(negloglik_diffuse, Optimization.AutoForwardDiff())\nprob = OptimizationProblem(optf, θ0, y)\nsol = solve(prob, LBFGS())\n\nH_mle, Q_mle = exp(sol.u[1]), exp(sol.u[2])\nprintln(\"H = \", H_mle, \", Q = \", Q_mle)  # H ≈ 15099, Q ≈ 1469","category":"section"},{"location":"tutorials/getting_started.html#Full-Filter-with-Exact-Diffuse","page":"Getting Started","title":"Full Filter with Exact Diffuse","text":"# Get full filter output with exact diffuse\nresult = kalman_filter(p, y, a1, P1_star, P1_inf)\n\n# Check the diffuse period (number of observations needed to initialize)\nd = diffuse_period(result)\nprintln(\"Diffuse period: \", d, \" observations\")\n\n# Access diffuse-specific quantities\nPinf_store, Pstar_store = diffuse_covariances(result)\nflags = diffuse_flags(result)\n\n# After diffuse period, filtered states are fully initialized\nprintln(\"First post-diffuse filtered state: \", result.att[:, d+1])","category":"section"},{"location":"tutorials/getting_started.html#In-Place-Version","page":"Getting Started","title":"In-Place Version","text":"For large-scale applications, use DiffuseKalmanWorkspace:\n\n# Create diffuse workspace\nws = DiffuseKalmanWorkspace(Z, H, T, R, Q, a1, P1_star, P1_inf, n)\n\n# Run filter (workspace type determines diffuse algorithm)\nll = kalman_filter!(ws, y)\n\n# Access results\nd = diffuse_period(ws)\natt = filtered_states(ws)","category":"section"},{"location":"tutorials/getting_started.html#When-to-Use-Exact-Diffuse","page":"Getting Started","title":"When to Use Exact Diffuse","text":"Use exact diffuse initialization when:\n\nThe initial state is truly unknown (random walk, integrated processes)\nYou want theoretically correct likelihood values\nYou're comparing with software that uses exact diffuse (e.g., R's KFAS)\n\nApproximate diffuse is sufficient when:\n\nThe series is long (first observation has negligible impact)\nSpeed is critical (exact diffuse has some overhead)\nInitial state has a proper prior","category":"section"},{"location":"tutorials/getting_started.html#Working-with-Missing-Data","page":"Getting Started","title":"Working with Missing Data","text":"Siphon.jl handles missing observations automatically using NaN:\n\n# Create data with missing values\ny_missing = copy(y)\ny_missing[1, 20:25] .= NaN  # Mark as missing\n\n# Use the unified API\nθ = (var_obs=15099.0, var_level=1469.0)\nmodel = StateSpaceModel(spec, θ, size(y_missing, 2))\n\nkalman_filter!(model, y_missing)\nkalman_smoother!(model)\n\n# The smoother will interpolate through missing periods\nprintln(\"Smoothed states through missing period: \", smoothed_states(model)[1, 18:27])","category":"section"},{"location":"tutorials/getting_started.html#Next-Steps","page":"Getting Started","title":"Next Steps","text":"Learn how to specify Custom Models using the DSL\nUnderstand Parameter Transformations for constrained optimization\nExplore Dynamic Factor Models for multivariate analysis\nSee the Core Functions and DSL & Templates for complete API documentation","category":"section"},{"location":"index.html#Siphon.jl","page":"Home","title":"Siphon.jl","text":"A Julia package for Linear State Space Models with Kalman filtering and smoothing.","category":"section"},{"location":"index.html#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(\"Siphon\")","category":"section"},{"location":"index.html#Overview","page":"Home","title":"Overview","text":"Siphon.jl provides a comprehensive toolkit for working with linear Gaussian state space models. It combines high-performance Kalman filtering algorithms with an ergonomic domain-specific language (DSL) for model specification.","category":"section"},{"location":"index.html#Key-Features","page":"Home","title":"Key Features","text":"High-Performance Kalman Filter: AD-compatible implementations supporting both scalar and matrix observations\nKalman Smoother: Full backward smoothing with disturbance smoothing\nErgonomic DSL: Specify models using intuitive matrix notation with automatic parameter handling\nPre-built Templates: Common models like local level, local linear trend, AR(1), ARMA, and dynamic factor models\nOptimization Integration: Seamless integration with Optimization.jl for maximum likelihood estimation\nBayesian Support: Prior specification and LogDensityProblems.jl interface for MCMC sampling\nStaticArrays Support: Automatic conversion to StaticArrays for small models\nMissing Data Handling: Native support for missing observations","category":"section"},{"location":"index.html#State-Space-Model-Formulation","page":"Home","title":"State Space Model Formulation","text":"Siphon.jl implements the standard linear Gaussian state space model:\n\nbeginaligned\ny_t = Z alpha_t + varepsilon_t quad varepsilon_t sim N(0 H) \nalpha_t+1 = T alpha_t + R eta_t quad eta_t sim N(0 Q)\nendaligned\n\nwhere:\n\ny_t is the p times 1 observation vector\nalpha_t is the m times 1 state vector\nZ is the p times m observation matrix\nH is the p times p observation error covariance\nT is the m times m transition matrix\nR is the m times r selection matrix\nQ is the r times r state innovation covariance","category":"section"},{"location":"index.html#Quick-Start","page":"Home","title":"Quick Start","text":"using Siphon\n\n# Create a local level model specification\nspec = local_level(var_obs_init=225.0, var_level_init=100.0)\n\n# Your data (p × T matrix, where p = number of series, T = time periods)\ny = randn(1, 100)\n\n# Create model and estimate parameters\nmodel = StateSpaceModel(spec, size(y, 2))\nfit!(MLE(), model, y)           # Maximum likelihood\n# or: fit!(EM(), model, y)      # EM algorithm\n\n# Access results\nprintln(\"Log-likelihood: \", loglikelihood(model))\nprintln(\"Parameters: \", model.theta_values)\n\n# Alternative: use optimize_ssm directly\nresult = optimize_ssm(spec, y)\nprintln(\"Estimated parameters: \", result.θ)","category":"section"},{"location":"index.html#Documentation-Structure","page":"Home","title":"Documentation Structure","text":"Getting Started: Basic tutorial covering Kalman filtering, smoothing, and parameter estimation\nCustom Models: Advanced tutorial on specifying custom state space models\nDynamic Factor Models: Tutorial on dynamic factor model specification\nCore Functions: Kalman filter and smoother API\nDSL & Templates: Model specification API","category":"section"},{"location":"index.html#Related-Packages","page":"Home","title":"Related Packages","text":"StateSpaceModels.jl: Alternative state space modeling package\nOptimization.jl: Unified optimization interface used by Siphon.jl\nTransformVariables.jl: Parameter transformations used internally\nLogDensityProblems.jl: Interface for Bayesian inference","category":"section"},{"location":"index.html#License","page":"Home","title":"License","text":"Siphon.jl is released under the MIT License.","category":"section"}]
}
