---
title: "Dynamic vs Static Factor Extraction"
subtitle: "Comparing Kalman Filter/Smoother with Principal Component Analysis"
format:
  html:
    code-fold: false
    toc: true
    toc-depth: 3
    number-sections: true
julia:
  exeflags: ["--project=."]
bibliography: references.bib
---

# Introduction

This tutorial compares two fundamental approaches to factor extraction from panel data:

1. **Dynamic Factor Extraction** via Kalman Filter/Smoother (KFS) with EM estimation (Siphon.jl)
2. **Static Factor Extraction** via Principal Component Analysis (PCA) (Factotum.jl)

The comparison follows the methodology of @poncela2021factor, who provide a comprehensive analysis of when each approach is preferred.

## The Factor Model

Consider a panel of $N$ observable time series $\{y_{it}\}_{i=1}^N$ over $T$ periods. The factor model decomposes observations into common and idiosyncratic components:

$$
y_{it} = \lambda_i' f_t + \varepsilon_{it}
$$

where:

- $f_t$ is an $r \times 1$ vector of latent common factors
- $\lambda_i$ is the $r \times 1$ vector of factor loadings for series $i$
- $\varepsilon_{it}$ is the idiosyncratic error

In matrix form:
$$
Y_t = \Lambda f_t + \varepsilon_t
$$

# Dynamic vs Static Representation

## Static Factors

The **static representation** treats factors as contemporaneous:
$$
Y_t = \Lambda F_t + \varepsilon_t
$$

PCA extracts static factors by finding the directions of maximum variance. The first $r$ principal components provide consistent estimates of the factor space as $N, T \to \infty$.

## Dynamic Factors

The **dynamic representation** explicitly models factor dynamics:
$$
\begin{aligned}
Y_t &= \Lambda_0 f_t + \Lambda_1 f_{t-1} + \cdots + \Lambda_s f_{t-s} + \varepsilon_t \\
f_t &= \Phi_1 f_{t-1} + \cdots + \Phi_q f_{t-q} + \eta_t
\end{aligned}
$$

This can be written in state-space form:
$$
\begin{aligned}
Y_t &= Z \alpha_t + \varepsilon_t \quad &\text{(observation equation)} \\
\alpha_{t+1} &= T \alpha_t + R \eta_t \quad &\text{(state equation)}
\end{aligned}
$$

where $\alpha_t$ stacks current and lagged factors.

## Relationship Between Static and Dynamic Factors

**Key insight**: If the dynamic factor $f_t$ has dimension $r$ and we include $s$ lags in the loadings, then:

$$
\dim(F_t) = r \times (s + 1)
$$

The static factor $F_t$ stacks the dynamic factor and its lags:
$$
F_t = \begin{pmatrix} f_t \\ f_{t-1} \\ \vdots \\ f_{t-s} \end{pmatrix}
$$

**Example**: With $r = 2$ dynamic factors and $s = 1$ lag:

- Dynamic factor dimension: 2
- Static factor dimension: $2 \times 2 = 4$

When $s = 0$ (no lagged loadings), the representations coincide.

# Estimation Approaches

## PCA (Static Approach)

PCA solves:
$$
\min_{\Lambda, F} \sum_{t=1}^T \|Y_t - \Lambda F_t\|^2
$$
subject to normalization constraints.

**Algorithm**:

1. Compute sample covariance matrix $\hat{\Sigma}_Y = \frac{1}{T} \sum_t Y_t Y_t'$
2. Extract eigenvectors corresponding to largest $r$ eigenvalues
3. Factors: $\hat{F}_t = \hat{\Lambda}' Y_t$ (or via SVD)

**Properties**:

- Closed-form solution (fast)
- Consistent as $N, T \to \infty$ under weak dependence
- Does not exploit serial correlation in factors
- No probabilistic interpretation (no standard errors)

## Kalman Filter/Smoother (Dynamic Approach)

KFS estimates factors via the state-space representation, typically with EM for parameter estimation.

**E-step**: Given parameters, run Kalman filter and smoother to obtain:

- $\hat{f}_{t|T} = E[f_t | Y_1, \ldots, Y_T]$ (smoothed factors)
- $P_{t|T} = \text{Var}(f_t | Y_1, \ldots, Y_T)$ (smoothed covariances)

**M-step**: Update parameters $(\Lambda, \Phi, \Sigma_\eta, \Sigma_\varepsilon)$ using sufficient statistics.

**Properties**:

- Exploits factor dynamics for efficient estimation
- Provides MSE estimates (uncertainty quantification)
- Handles missing data naturally
- Requires correct specification of dynamics
- Computationally more intensive

# Rotation Indeterminacy

Both methods identify factors only up to rotation. If $\hat{F}$ estimates $F$, then $\hat{F} R$ is equally valid for any orthogonal $R$.

## Procrustes Alignment

To compare estimated factors $\hat{F}$ with true factors $F$, we solve:
$$
\min_{R: R'R = I} \|F - R \hat{F}\|_F
$$

**Solution**: If $\hat{F} F' = U S V'$ (SVD), then $R = V U'$.

## Canonical Correlations (Rotation-Invariant)

For $r > 1$ factors, Procrustes alignment followed by per-factor correlations is misleading. Instead, use **canonical correlations**:

1. Compute covariance matrices:
   - $\Sigma_{\hat{F}\hat{F}} = \hat{F} \hat{F}' / T$
   - $\Sigma_{FF} = F F' / T$
   - $\Sigma_{\hat{F}F} = \hat{F} F' / T$

2. Canonical correlations $\rho_1 \geq \rho_2 \geq \cdots \geq \rho_r$ are singular values of:
$$
\Sigma_{\hat{F}\hat{F}}^{-1/2} \Sigma_{\hat{F}F} \Sigma_{FF}^{-1/2}
$$

Canonical correlations are **rotation-invariant** and measure how well the estimated factor space aligns with the true factor space.

## Factor Space $R^2$

Another rotation-invariant metric:
$$
R^2 = \frac{1}{r} \text{tr}(P_{\hat{F}} P_F)
$$

where $P_F = F'(FF')^{-1}F$ is the projection matrix onto the factor space.

# Monte Carlo Comparison

We compare KFS and PCA following the simulation design in @poncela2021factor:

- $N \in \{5, 10, 50, 150\}$ cross-sectional units
- $T = 200$ time periods
- $r \in \{1, 2\}$ factors
- Factor dynamics: $f_t = 0.7 f_{t-1} + \eta_t$, $\eta_t \sim N(0, I)$
- Loadings: $\lambda_i \sim N(0, I)$
- Idiosyncratic errors: $\varepsilon_{it} \sim N(0, 0.25)$

```{julia}
#| echo: false
#| output: false
using Pkg
Pkg.activate(joinpath(@__DIR__, "..", "..", ".."))
Pkg.instantiate()
```

```{julia}
using Siphon
using Factotum
using LinearAlgebra
using Statistics
using Random: MersenneTwister, rand
using Printf

# Simulation function
function simulate_dfm(N, T, r; phi=0.7, sigma_e=0.5, seed=nothing)
    rng = isnothing(seed) ? MersenneTwister() : MersenneTwister(seed)

    # Factor loadings ~ N(0, 1)
    Lambda = randn(rng, N, r)

    # AR(1) factors
    f = zeros(r, T)
    sigma_f = sqrt(1 - phi^2)  # Stationary variance = 1
    f[:, 1] = randn(rng, r)
    for t in 2:T
        f[:, t] = phi * f[:, t-1] + sigma_f * randn(rng, r)
    end

    # Observations
    y = Lambda * f + sigma_e * randn(rng, N, T)

    return (y=y, f_true=f, Lambda_true=Lambda)
end

# Canonical correlations
function canonical_correlations(F_est, F_true)
    r, T = size(F_est)
    F_est_c = F_est .- mean(F_est, dims=2)
    F_true_c = F_true .- mean(F_true, dims=2)

    Σ_ee = F_est_c * F_est_c' / T
    Σ_tt = F_true_c * F_true_c' / T
    Σ_et = F_est_c * F_true_c' / T

    U_e, S_e, _ = svd(Σ_ee)
    U_t, S_t, _ = svd(Σ_tt)

    tol = 1e-10
    S_e_inv_sqrt = Diagonal([s > tol ? 1/sqrt(s) : 0.0 for s in S_e])
    S_t_inv_sqrt = Diagonal([s > tol ? 1/sqrt(s) : 0.0 for s in S_t])

    M = (U_e * S_e_inv_sqrt * U_e') * Σ_et * (U_t * S_t_inv_sqrt * U_t')
    cc = clamp.(svdvals(M), 0.0, 1.0)
    sort!(cc, rev=true)

    return cc
end;
```

## Single Factor ($r = 1$)

```{julia}
#| output: true

function run_mc_single(; N_vals=[5, 50, 150], T=200, n_reps=20, seed=42)
    rng = MersenneTwister(seed)
    results = []

    for N in N_vals
        for rep in 1:n_reps
            data = simulate_dfm(N, T, 1; phi=0.7, seed=rand(rng, UInt))

            # KFS
            model = DynamicFactorModel(N, 1, T; factor_lags=1, error_lags=0)
            fit!(EM(), model, data.y; maxiter=200, tol=1e-6, verbose=false)
            f_kfs = Siphon.factors(model)
            cc_kfs = canonical_correlations(f_kfs, data.f_true)

            # PCA
            fm = FactorModel(permutedims(data.y), 1; demean=true)
            f_pca = permutedims(Factotum.factors(fm))
            cc_pca = canonical_correlations(f_pca, data.f_true)

            push!(results, (N=N, method="KFS", cc=cc_kfs[1]))
            push!(results, (N=N, method="PCA", cc=cc_pca[1]))
        end
    end

    return results
end

results_r1 = run_mc_single(n_reps=20)

println("Single Factor (r = 1) Results")
println("=" ^ 50)
println()
@printf("| %6s | %6s | %18s |\n", "N", "Method", "Canonical Corr")
println("|" * "-"^8 * "|" * "-"^8 * "|" * "-"^20 * "|")

for N in [5, 50, 150]
    for method in ["KFS", "PCA"]
        subset = filter(r -> r.N == N && r.method == method, results_r1)
        cc_mean = mean(r.cc for r in subset)
        cc_std = std(r.cc for r in subset)
        @printf("| %6d | %6s | %.4f (%.4f) |\n", N, method, cc_mean, cc_std)
    end
    println("|" * "-"^8 * "|" * "-"^8 * "|" * "-"^20 * "|")
end
```

## Two Factors ($r = 2$)

```{julia}
#| output: true

function run_mc_two(; N_vals=[10, 50, 150], T=200, n_reps=20, seed=42)
    rng = MersenneTwister(seed)
    results = []

    for N in N_vals
        for rep in 1:n_reps
            data = simulate_dfm(N, T, 2; phi=0.7, seed=rand(rng, UInt))

            # KFS
            model = DynamicFactorModel(N, 2, T; factor_lags=1, error_lags=0)
            fit!(EM(), model, data.y; maxiter=200, tol=1e-6, verbose=false)
            f_kfs = Siphon.factors(model)
            cc_kfs = canonical_correlations(f_kfs, data.f_true)

            # PCA
            fm = FactorModel(permutedims(data.y), 2; demean=true)
            f_pca = permutedims(Factotum.factors(fm))
            cc_pca = canonical_correlations(f_pca, data.f_true)

            push!(results, (N=N, method="KFS", avg_cc=mean(cc_kfs), min_cc=minimum(cc_kfs)))
            push!(results, (N=N, method="PCA", avg_cc=mean(cc_pca), min_cc=minimum(cc_pca)))
        end
    end

    return results
end

results_r2 = run_mc_two(n_reps=20)

println("Two Factors (r = 2) Results")
println("=" ^ 60)
println()
@printf("| %6s | %6s | %15s | %15s |\n", "N", "Method", "Avg CC", "Min CC")
println("|" * "-"^8 * "|" * "-"^8 * "|" * "-"^17 * "|" * "-"^17 * "|")

for N in [10, 50, 150]
    for method in ["KFS", "PCA"]
        subset = filter(r -> r.N == N && r.method == method, results_r2)
        avg_mean = mean(r.avg_cc for r in subset)
        avg_std = std(r.avg_cc for r in subset)
        min_mean = mean(r.min_cc for r in subset)
        min_std = std(r.min_cc for r in subset)
        @printf("| %6d | %6s | %.4f (%.3f) | %.4f (%.3f) |\n",
                N, method, avg_mean, avg_std, min_mean, min_std)
    end
    println("|" * "-"^8 * "|" * "-"^8 * "|" * "-"^17 * "|" * "-"^17 * "|")
end
```

# Key Findings

## When to Use Each Method

### Prefer KFS (Dynamic) When:

1. **Small $N$**: With few cross-sectional units, exploiting factor dynamics improves efficiency
2. **Missing data**: KFS handles missing observations naturally via the Kalman filter
3. **Uncertainty quantification**: KFS provides MSE estimates for factors
4. **Dynamic structure is known**: When you're confident about factor dynamics specification
5. **Mixed frequencies**: KFS accommodates different observation frequencies

### Prefer PCA (Static) When:

1. **Large $N$**: As $N \to \infty$, PCA achieves efficiency (asymptotic theory)
2. **Speed**: PCA is orders of magnitude faster (closed-form solution)
3. **Robustness**: PCA doesn't require correct specification of dynamics
4. **Factor selection**: Information criteria (IC, BIC) are well-developed for PCA
5. **Exploratory analysis**: Quick first look at factor structure

## Asymptotic Equivalence

@poncela2021factor show that as $N \to \infty$:

- PCA estimates become efficient
- The advantage of KFS diminishes
- Both methods recover the same factor space

This is reflected in our Monte Carlo: the gap between KFS and PCA shrinks as $N$ increases.

## Computational Comparison

| Aspect | KFS (Siphon.jl) | PCA (Factotum.jl) |
|--------|-----------------|-------------------|
| Complexity | $O(T \cdot m^3)$ per EM iteration | $O(\min(N,T) \cdot NT)$ |
| Typical time | Seconds to minutes | Milliseconds |
| Memory | Stores state covariances | Stores loadings/factors |
| Missing data | Native support | Requires imputation |

# Practical Recommendations

1. **Start with PCA** for exploratory analysis and factor selection
2. **Use information criteria** (IC1, IC2, BIC3) to determine number of factors
3. **Switch to KFS** when:
   - You need uncertainty quantification
   - Data has missing values
   - $N$ is small relative to $T$
   - Factor dynamics are economically meaningful
4. **Compare methods** as a robustness check
5. **Report canonical correlations** between KFS and PCA factors

# Code Example: Full Comparison

```{julia}
#| output: true
#| code-fold: true

# Complete example with real-world-sized data
N, T, r = 50, 200, 3
data = simulate_dfm(N, T, r; phi=0.8, sigma_e=0.3, seed=123)

println("Data dimensions: N=$N, T=$T, r=$r")
println()

# PCA
t_pca = @elapsed begin
    fm = FactorModel(permutedims(data.y), r; demean=true)
    f_pca = permutedims(Factotum.factors(fm))
end

# KFS
t_kfs = @elapsed begin
    model = DynamicFactorModel(N, r, T; factor_lags=1, error_lags=0)
    fit!(EM(), model, data.y; maxiter=200, tol=1e-6, verbose=false)
    f_kfs = Siphon.factors(model)
end

# Compare
cc_pca = canonical_correlations(f_pca, data.f_true)
cc_kfs = canonical_correlations(f_kfs, data.f_true)
cc_methods = canonical_correlations(f_kfs, f_pca)

println("Computation time:")
@printf("  PCA: %.4f seconds\n", t_pca)
@printf("  KFS: %.4f seconds\n", t_kfs)
println()

println("Canonical correlations with true factors:")
for i in 1:r
    @printf("  Factor %d: PCA=%.4f, KFS=%.4f\n", i, cc_pca[i], cc_kfs[i])
end
println()

println("Canonical correlations between methods:")
for i in 1:r
    @printf("  Dimension %d: %.4f\n", i, cc_methods[i])
end
```

# References

::: {#refs}
:::
